<!DOCTYPE html>
<html lang="en">
  <head>
  <base href="/">
    <title>[12876] ilc jobs overwhelming OSG CE gatekeeper node on TTU-Antaeus</title>    <meta charset="utf-8" />
    <meta name="verify-v1" content="na5IcAJsZVOfEkboRxuIiZ1zpZgnZiWra+nKcS7nA/o=" />
    <meta name="google-site-verification" content="DLrk3ft4s8b-S2TloLCL2LD_t6wcTjgSluf5pmiu2kA" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="description" content="" />
    <link href="https://ticket.opensciencegrid.org/rss" rel="alternate" type="application/rss+xml" title="GOC Ticket Update feed" />

    <style type="text/css">
      body {
        padding-top: 50px;
        padding-bottom: 40px;
      }
      .sidebar-nav {
        padding: 9px 0;
      }
     #search {
            width: 300px;
     }

    </style>

<script src="https://code.jquery.com/jquery-3.0.0.js"></script>
<script src="https://code.jquery.com/jquery-migrate-3.0.1.js"></script>

   <link href="//netdna.bootstrapcdn.com/bootstrap/2.3.2/css/bootstrap.min.css" rel="stylesheet"/>
    <script src="//netdna.bootstrapcdn.com/bootstrap/2.3.2/js/bootstrap.min.js"></script>

    <link href="//netdna.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet"/>
    <link href="//ajax.googleapis.com/ajax/libs/jqueryui/1.10.4/themes/smoothness/jquery-ui.min.css" rel="stylesheet"/>
 <script src="//ajax.googleapis.com/ajax/libs/jqueryui/1.12.1/jquery-ui.min.js"></script>


    <link href="//cdnjs.cloudflare.com/ajax/libs/select2/4.0.0-rc.2/css/select2.min.css" rel="stylesheet" />
    <script src="//cdnjs.cloudflare.com/ajax/libs/select2/4.0.0-rc.2/js/select2.min.js"></script>

    <link href="css/ticket.css" rel="stylesheet" />
    <script src="lib/jquery.cookie.js"></script>

    <link href="images/tag_orange.png" rel="icon" type="image/png"/>
    <script src='https://www.google.com/recaptcha/api.js'></script>
  </head>

  <body>
    <div class="navbar navbar-inverse navbar-fixed-top">
      <div class="navbar-inner">
        <div class="container-fluid">
            <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </a>

            <a class="brand" style="padding: 6px 0px 0px 6px;" href="http://opensciencegrid.org"><img src="https://ticket.opensciencegrid.org/images/osglogo.40x30.png"/></a>
            <ul class="nav">
                <li class="dropdown"><a href="#" class="dropdown-toggle" data-toggle="dropdown">GOC Ticket <b class="caret"></b></a>
                    <ul class="dropdown-menu">
                    <li><a href="https://my.opensciencegrid.org">MyOSG</a></li>
                    <li><a href="https://oim.opensciencegrid.org">OIM</a></li>
                    <li class="active"><a href="index">Ticket</a></li>
	<li class="divider"></li>
	<li><a href="http://repo.grid.iu.edu">Repo</a></li>
	<li class="divider"></li>
	<li><a href="http://blogs.grid.iu.edu">Blog</a></li>
                    <li><a href="http://display.grid.iu.edu">Display</a></li>
                    <li><a href="http://osggoc.blogspot.com/">News</a></li>
                    </ul>
                </li>
            </ul>
            <ul class="nav pull-right">
                <li><a href="https://ticket.opensciencegrid.org/sso/">Login</a></li>            </ul>

            <div class="nav-collapse">
                <ul class="nav">
			 <li id="menu_submit"><a href="https://ticket.opensciencegrid.org/submit">Submit</a></li><li id="menu_view" class="dropdown"><a href="\#" class="dropdown-toggle" data-toggle="dropdown">View <b class="caret"></b></a><ul class="dropdown-menu"><li id="submenu_listopen"><a href="https://ticket.opensciencegrid.org/list/open">Open Tickets</a></li><li id="submenu_listrecentclose"><a href="https://ticket.opensciencegrid.org/list/recentclose">Recently Closed Tickets</a></li><li class="divider"></li><li id="submenu_alltickets"><a href="https://ticket.opensciencegrid.org/search?q=&amp;sort=id">All Tickets</a></li></ul></li>                </ul>

                <form class="navbar-search pull-right" action="viewer">
                    <input id="search" type="text" name="id" class="search-query span2" placeholder="Search Ticket" value=""/>
                </form>
            </div>
        </div>
      </div>
    </div>

<script type='text/javascript' src='lib/jquery.timeago.js'></script>
<script type='text/javascript' src='lib/byte2size.js'></script>
<style>
#updates .toolbar {
position: relative;
margin-top: 0px;
top: -10px;
font-weight: normal;
}
#updates a.anchor {
position: relative;
top: -50px;
}
#updates .selected pre {
animation:selected 2s;
animation-iteration-count: 2;
animation-direction: alternate;
-webkit-animation:selected 2s; 
-webkit-animation-iteration-count: 2;
-webkit-animation-direction: alternate;
box-shadow: inset 1px 1px 20px #9ad;
border: 1px solid #9ab;
margin: 5px 0px;
padding-left: 10px;
}
@keyframes selected {
    from  {
        box-shadow: inset 1px 1px 20px #9ad;
        border: 1px solid #9ab;
    }
    to {
        box-shadow: inset 1px 1px 20px #05c;
        border: 1px solid #05c;
    }
}
@-webkit-keyframes selected {
    from  {
        box-shadow: inset 1px 1px 20px #9ad;
        border: 1px solid #9ad;
    }
    to {
        box-shadow: inset 1px 1px 20px #05c;
        border: 1px solid #05c;
    }
}
#updates pre {
background-color: inherit;
line-height: 15px;
padding: 5px;
}
#updates .header {
color: #999;
}
#updates .update_history pre {
background-color: #eee;
color: #666;
font-size: 85%;
}
#updates .clickable {
cursor: pointer;
}
#updates .clickable:hover {
color: #D98719;
}
#updates .meta_information pre {
background-color: #fed;
}
#similar_tickets {
max-height: 300px;
overflow-y: auto;
pointer-events: none;
padding: 5px;
background-color: #f4f4f4;
}
.btn-toolbar {
margin-bottom: 0;
height: 30px;
}
#peers {
position: fixed;
bottom: 0px;
right: 0px;
z-index: 100;
list-style: none;
padding: 5px 0px 0px 5px;
margin: 0px;
background-color: white;
box-shadow: 0px 0px 10px white;
}
#peers li {
background-color: #ccc;
color: #000;
display: inline-block;
padding: 5px 10px;
margin-right: 5px;
position: relative;
}
/*
#peers li:hover {
background-color: #999;
cursor: pointer;
}
*/
#peers span.ip {
padding-left: 5px;
color: #666;
}
#peers .new {
bottom: -30px;
}
/*
#peers .me {
background-color: red;
}
*/
</style>

<div class="container-fluid">
<ul id="peers"></ul>
<div class="alert alert-danger"><a class="close" href="#" data-dismiss="alert">&times;</a>By the end of May 2018, the ticketing system at https://ticket.opensciencegrid.org will be retired and support will be provided at https://support.opensciencegrid.org. Throughout this transition the support email (help@opensciencegrid.org) will be available as a point of contact.<br><br>                                                   
                                                                                                                                                                                   
Please see the service migration page for details: https://opensciencegrid.github.io/technology/policy/service-migrations-spring-2018/#ticket</div><div id="presence" class="pull-right"></div><div class="ticketgui"><script type="text/javascript" src="https://ticket.opensciencegrid.org/lib/checktab.js"></script>

<script>
var expanded = false;
function expand_description() {
    var desc = $(".description");
    if(!expanded) {
        expanded = true;
        //expand to minheight
        var min = 250;
        if(desc.height() < min) {
            desc.animate({height: min}, 200);
        }
    }
}

$(document).ready(function() {
    $("input[name='nad']").datepicker({
        dateFormat: 'yy-mm-dd'
    });
});

</script>



<style>
.form-horizontal .control-label {
padding-top: inherit;
font-size:90%;
color:#666;
}
label {
margin-bottom: 0px;
}
.controls {
line-height: 18px;
}
</style>
<form class="form-horizontal" action="https://ticket.opensciencegrid.org/viewer/updatebasic?id=12876" method="post">
<div class="page-header">
    <h3><span class="muted">12876</span> / ilc jobs overwhelming OSG CE gatekeeper node on TTU-Antaeus</h3>
</div>

<div class="row-fluid">
<div class="span5">
    <legend>Contact</legend>
    <div class="control-group">
        <label class="control-label">Full Name</label>
        <div class="controls">Alan Sill</div>
    </div>
    <div class="control-group">
        <label class="control-label">Email</label>
        <div class="controls">
            <i class="icon-lock"></i>        </div>
    </div>
    <div class="control-group">
        <label class="control-label">Phone</label>
        <div class="controls">
            <i class="icon-lock"></i>        </div>
    </div>
    <div class="control-group">
        <label class="control-label">CC</label>
        <div class="controls">
            <i class="icon-lock"></i>        </div>
    </div>

    <legend>Details</legend>
    <div class="control-group"><label class="control-label">Associated VO</label><div class="controls">ILC</div></div><div class="control-group"><label class="control-label">Submitted Via</label><div class="controls">GOC Ticket/submit</div></div><div class="control-group"><label class="control-label">Submitter</label><div class="controls">Alan Sill</div></div><div class="control-group"><label class="control-label">Support Center</label><div class="controls">ILC</div></div>
    <div class="control-group">
        <label class="control-label">Ticket Type</label>
        <div class="controls"></div>
    </div>
    <div class="control-group">
        <label class="control-label">Priority</label>
        <div class="controls">Normal</div>
    </div>
    <div class="control-group">
        <label class="control-label">Status</label>
        <div class="controls">
Closed</div>
    </div>
    <div class="control-group">
        <label class="control-label">Next Action</label>
        <div class="controls">ENG Action</div>
    </div>
    <div class="control-group">
        <label class="control-label">Next Action Deadline</label>
        <div class="controls flag_red">2012-11-28</div>
    </div>

</div><!--span-->
<div class="span7">
    <legend>Assignees</legend>
    <div class="assignee" style="width: 60%">GIP Support <span class="muted"> / OSG Support Centers</span></div><div class="assignee" style="width: 60%">FNAL (Prod)&nbsp;<a target="_blank" href="https://fermi.service-now.com/nav_to.do?uri=incident.do?sys_id=INC000000327794">INC000000327794</a> <span class="muted"> / Ticket Exchange</span></div><div class="assignee" style="width: 60%">GGUS (Prod)&nbsp;<a target="_blank" href="https://ggus.eu/ws/ticket_info.php?ticket=88985">88985</a> <span class="muted"> / Ticket Exchange</span></div>    <br>

    <legend>Assignees</legend>
    TODO
    <br>

    <style>
legend.noborder {
border-bottom: none;
}
</style>

<div id="attachment-list"/>
<script>
$(function () {
    var first = true;
    $.getJSON("https://ticket.opensciencegrid.org/attachment/list?id=12876", function (files) {
        //console.dir(files);
        var html = "<table class=\"table table-condensed\">";
        $(files).each(function() {
            if(first) {
                first = false;
                html += "<legend class=\"noborder\">Attachmenets</legend>";
            }
            html += "<tr class=\"attachment\">";
            html += "<td><img src=\""+this.thumbnail_url+"\"/></td>";
            html += "<td><a href=\""+this.url+"\" target=\"_blank\">"+this.name+"</a></td>";
            html += "<td>"+bytesToSize(this.size, 1)+"</td>";
            html += "</tr>";
        });
        html += "</table>";
        $("#attachment-list").html(html);
    });
});

function download(url) {
    window.open(url, "_blank");
}
</script>


</div><!--span-->
</div><!--row-fluid-->


</form>

</div>
<div id="updates" style="clear: both;">
    <legend>Past Updates
    <div class="btn-toolbar pull-right toolbar">
        <div class="btn-group">
                <a class="btn btn-small" href="/12876?sort=up&"><i class="icon-arrow-up"></i> Sort</a>

        
        <a class="btn btn-small" href="/12876?expandall=true&">Expand Descriptions</a>        <a class="btn btn-small" target="_blank" href="mailto:osg@tick.globalnoc.iu.edu?subject=Open%20Science%20Grid%3A%20ilc%20jobs%20overwhelming%20OSG%20CE%20gatekeeper%20node%20on%20TTU-Antaeus%20ISSUE%3D12876%20PROJ%3D71"><i class="icon-envelope"></i> Update w/Email</a>
        </div>
    </div><!--btn-toolbar-->
    </legend>

    <div class='update_description'><i onclick="document.location='12876#1354219171'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2012-11-29T19:59:31+00:00">Nov 29, 2012 07:59 PM UTC</time> by <b>OSG-GOC</b><a class="anchor" name="1354219171">&nbsp;</a></div><pre>This is now being handled by the European side of ILC support center (ilc-vo-support@....) via GGUS so it is no longer necessary to have the Fermilab
side of the support ticket. all relevant ILC staff at Fermilab and DESY will see the ticket via GGUS.

-- by timm at Thu Nov 29 19&#58;58&#58;35 UTC 2012

GOCTX Source&#58; <a href='https&#58;//fermi.service-now.com/nav_to.do?uri=incident.do?sys_id=INC000000327794' target='_blank' rel='nofollow'>https&#58;//fermi.service-now.com/nav_to.do?uri=incident.do?sys_id=INC000000327794</a></pre></div><div class='update_description'><i onclick="document.location='12876#1354176653'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2012-11-29T08:10:53+00:00">Nov 29, 2012 08:10 AM UTC</time> by <b>OSG-GOC</b><a class="anchor" name="1354176653">&nbsp;</a></div><pre>Changed concerned VO to&#58; ilc

-- by Alessandro Paolini at Thu Nov 29 08&#58;10&#58;39 UTC 2012

GOCTX Source&#58; <a href='https&#58;//ggus.eu/ws/ticket_info.php?ticket=88985' target='_blank' rel='nofollow'>https&#58;//ggus.eu/ws/ticket_info.php?ticket=88985</a></pre></div><div class='update_description'><i onclick="document.location='12876#1354137968'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2012-11-28T21:26:08+00:00">Nov 28, 2012 09:26 PM UTC</time> by <b>Rob Quick</b><a class="anchor" name="1354137968">&nbsp;</a></div><pre>Based on Alan&#39;s comment&#58;

&#34;the ILC VO should follow the advice given earlier in this thread to alter its decision-making mechanisms to avoid flooding resources based on its present decision algorithm.  If this requires updates to gLite, those should be identified and forwarded to the relevant EGI contact people. Once this is done, the problem should be improved somewhat.&#34;

I&#39;m going to forward this ticket to GGUS for review.</pre></div><div class='update_description'><i onclick="document.location='12876#1353946929'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2012-11-26T16:22:09+00:00">Nov 26, 2012 04:22 PM UTC</time> by <b>Burt Holzman</b><a class="anchor" name="1353946929">&nbsp;</a></div><pre>Hi Alan,

I&#39;ll add a configuration option to adjust this policy variable in GIP --
but it may be a while before I have time to work on it and cut an new
release.

However, I disagree that anything in SGE easily maps one-to-one with the
Glue concepts of queues (basically LSF/PBS).  If you come up with a
prescription that works at TTU and Colorado, send it on and I&#39;ll have a
look.

- B

On 11/21/12 16&#58;22, Open Science Grid FootPrints wrote&#58;
<font color='#7F7E6F'>&#62; [Duplicate message snipped]</font></pre></div><div class='update_description'><i onclick="document.location='12876#1353536548'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2012-11-21T22:22:28+00:00">Nov 21, 2012 10:22 PM UTC</time><a class="anchor" name="1353536548">&nbsp;</a></div><pre>Elizabeth,

Sorry, you caught me in transit to CERN, where I am now and will be for the next couple of weeks for a test beam.

I think we need two actions to resolve this ticket&#58;

(1) the ILC VO should follow the advice given earlier in this thread to alter its decision-making mechanisms to avoid flooding resources based on its present decision algorithm.  If this requires updates to gLite, those should be identified and forwarded to the relevant EGI contact people. Once this is done, the problem should be improved somewhat.

(2) the bug that sets the advertised queue maximum values to 100000 more than the actual queue slots should be fixed in the OSG configuration steps.  This workaround is clearly causing more problems than it solves, so those problems (if they still exist) should be dealt with in another way.  This is my interpretation of the earlier comments; please advise if this is incorrect.

If at least suggestion (1) is followed, with appropriate details from Burt and Brian, that would be helpful.  Somewhat less importantly, I do think that (2) should be looked at seriously.  For the moment, we have the workaround in place documented earlier in this ticket thread to set the maximum queue limits manually to their proper, un-altered values, though I think this is an unattractive solution as the originally advertised values would have been better without this extra 100000 value, in my opinion.

Beyond this, as regards the comments forwarded from Doug, SGE does have a pending queue state that can be polled rather straightforwardly using stat, so I don&#39;t really understand this problem.

I don&#39;t have time to look at this for at least the next two weeks due to activities at the test beam, but we might be able to get someone else at TTU to look at this after the Thanksgiving holidays, for example Amy Wang or Per Andersen.  I&#39;ve put them into the CC&#58; list for this ticket, in the hope that they can help.  But for SGE issues, I suggest that we leave these off and open a new ticket to explore them.  So my suggestions (1) and (2) above are the minimum ones required to close this ticket, I believe.

Have a good holiday break!

Alan

by /DC=org/DC=doegrids/OU=People/CN=Alan Sill 503049</pre></div><div class='update_description'><i onclick="document.location='12876#1353534402'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2012-11-21T21:46:42+00:00">Nov 21, 2012 09:46 PM UTC</time> by <b>echism</b><a class="anchor" name="1353534402">&nbsp;</a></div><pre>Alan,

Please share your thoughts on this. I think we&#39;re at a halt here and I&#39;m not sure what to do with this ticket.

Thank you,
Elizabeth</pre></div><div class='update_description'><i onclick="document.location='12876#1352922778'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2012-11-14T19:52:58+00:00">Nov 14, 2012 07:52 PM UTC</time> by <b>echism</b><a class="anchor" name="1352922778">&nbsp;</a></div><pre>Alan,

Did you get those last few updates?

Thank you,
Elizabeth</pre></div><div class='update_description'><i onclick="document.location='12876#1352498772'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2012-11-09T22:06:12+00:00">Nov 9, 2012 10:06 PM UTC</time> by <b>Burt Holzman</b><a class="anchor" name="1352498772">&nbsp;</a></div><pre>Hi,

I dug deeper into SGE and also talked with Doug Johnson, who administers
a CMS Tier 3 with SGE. There&#39;s no easy mapping between the SGE concept
of a queue and what the GIP expects, unfortunately. Given the challenge
of the task and the effort we have available, we&#39;re going to have to
accept this as a limit of what we can publish about SGE batch systems.

- B

On 11/09/12 15&#58;45, Open Science Grid FootPrints wrote&#58;
<font color='#7F7E6F'>&#62; [Duplicate message snipped]</font></pre></div><div class='update_description'><i onclick="document.location='12876#1352497518'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2012-11-09T21:45:18+00:00">Nov 9, 2012 09:45 PM UTC</time> by <b>echism</b><a class="anchor" name="1352497518">&nbsp;</a></div><pre>Is there anything new on this? The last I heard was Burt advising ILC VO users to avoid this method of job submission and saying he would ask SGE experts for their assistance.

Thank you,
Elizabeth</pre></div><div class='update_description'><i onclick="document.location='12876#1351881178'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2012-11-02T18:32:58+00:00">Nov 2, 2012 06:32 PM UTC</time> by <b>Burt Holzman</b><a class="anchor" name="1351881178">&nbsp;</a></div><pre>I also looked at this from the GIP side.  As far as I can tell, we&#39;ve never reported the number of actual waiting jobs in SGE (which is why the EstimatedResponseTimes are locked at 3600).  I&#39;ll work with SGE experts to see if we can come up with a reasonable metric, but it&#39;s not something that SGE natively maps, so it&#39;s not trivial.  However&#58; I believe that the act of submitting jobs until BDII tells you the site is full is not good practice and will not go well for VOs that try it.

- B</pre></div><div class='update_description'><i onclick="document.location='12876#1351729184'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2012-11-01T00:19:44+00:00">Nov 1, 2012 12:19 AM UTC</time><a class="anchor" name="1351729184">&nbsp;</a></div><pre>Hi,

Here are two requirements from Stephane&#39;s job below that must be true&#58;

other.GlueCEStateWaitingJobs * 4 &#60; other.GlueCEStateRunningJobs
other.GlueCEPolicyMaxTotalJobs == 0 || other.GlueCEStateTotalJobs &#60; other.GlueCEPolicyMaxTotalJobs

Right now, running is listed as 432 and waiting is 0.  So, you may want to investigate why the waiting jobs aren&#39;t getting counted for the GIP?  Right now, it appears more jobs are submitted until many waiting jobs show up in BDII.

FWIW - the other pilot systems track the number of waiting pilots per site explicitly, as opposed to relying on BDII to be correct.  My understanding is that LHCb uses DIRAC and doesn&#39;t rely on BDII correctness - maybe you can ask them how they approach this issue?  It would help with running jobs on the OSG.

Hope this helps!

Brian

by /DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=People/CN=Brian Bockelman</pre></div><div class='update_description'><i onclick="document.location='12876#1351725958'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2012-10-31T23:25:58+00:00">Oct 31, 2012 11:25 PM UTC</time> by <b>OSG-GOC</b><a class="anchor" name="1351725958">&nbsp;</a></div><pre>reply from&#58; stephane.guillaume.poss@....

Hi,
Yes, fuzzy ranking is enabled.
Here is the logging info of a job running properly at another site (itâ€™s quite long and possibly ugly formatting)&#58;
===================== glite-job-logging-info Success =====================

LOGGING INFORMATION&#58;

Printing info for the Job &#58; <a href='https&#58;//wms303.cern.ch&#58;9000/14eL_Z9odMkvuvcLEdP-9w' target='_blank' rel='nofollow'>https&#58;//wms303.cern.ch&#58;9000/14eL_Z9odMkvuvcLEdP-9w</a>
---
Event&#58; RegJob
- Arrived                    =    Wed Oct 31 21&#58;48&#58;45 2012 CET
- Host                       =    wms303.cern.ch
- Jobtype                    =    SIMPLE
- Level                      =    SYSTEM
- Ns                         =    <a href='https&#58;//wms303.cern.ch&#58;7443/glite_wms_wmproxy_server' target='_blank' rel='nofollow'>https&#58;//wms303.cern.ch&#58;7443/glite_wms_wmproxy_server</a>
- Nsubjobs                   =    0
- Parent                     =    <a href='https&#58;//wms303.cern.ch&#58;9000/oH8T5cOnzfugEnUzHdc8dg' target='_blank' rel='nofollow'>https&#58;//wms303.cern.ch&#58;9000/oH8T5cOnzfugEnUzHdc8dg</a>
- Priority                   =    synchronous
- Seqcode                    =    UI=000000&#58;NS=0000000001&#58;WM=000000&#58;BH=0000000000&#58;JSS=000000&#58;LM=000000&#58;LRMS=000000&#58;APP=000000&#58;LBS=000000
- Source                     =    NetworkServer
- Src instance               =    <a href='https&#58;//wms303.cern.ch&#58;7443/glite_wms_wmproxy_server' target='_blank' rel='nofollow'>https&#58;//wms303.cern.ch&#58;7443/glite_wms_wmproxy_server</a>
- Timestamp                  =    Wed Oct 31 21&#58;48&#58;45 2012 CET
- User                       =    /DC=ch/DC=cern/OU=Organic Units/OU=Users/CN=sposs/CN=641989/CN=Stephane Guillaume Poss
- Jdl            =

[
RetryCount = 0;
edg_jobid = &#34;<a href='https&#58;//wms303.cern.ch&#58;9000/14eL_Z9odMkvuvcLEdP-9w&#34' target='_blank' rel='nofollow'>https&#58;//wms303.cern.ch&#58;9000/14eL_Z9odMkvuvcLEdP-9w&#34</a>;;
WmsClient =
[
RetryCount = 0;
LBEndPoints = { &#34;<a href='https&#58;//lb101.cern.ch&#58;9000&#34' target='_blank' rel='nofollow'>https&#58;//lb101.cern.ch&#58;9000&#34</a>; };
WMProxyEndPoints = { &#34;<a href='https&#58;//wmsshared.cern.ch&#58;7443/glite_wms_wmproxy_server&#34' target='_blank' rel='nofollow'>https&#58;//wmsshared.cern.ch&#58;7443/glite_wms_wmproxy_server&#34</a>; };
<div id='show_933420894' class=''><button class="btn">Show More</button></div><div class='detail hidden' id='detail_933420894'>EnableServiceDiscovery = false;
JdlDefaultAttributes =
[
PerusalFileEnable = false;
SignificantAttributes = { &#34;Requirements&#34;,&#34;Rank&#34;,&#34;FuzzyRank&#34; };
requirements = ( other.GlueCEStateStatus == &#34;Production&#34; || other.GlueCEStateStatus == &#34;Special&#34; );
AllowZippedISB = true
];
ListenerStorage = &#34;/opt/dirac/versions/v13r0p1_1345549839/work/WorkloadManagement/TaskQueueDirector/TQ_180224_02Sq2p/Storage&#34;;
MyProxyServer = &#34;f06876eb75.cern.ch&#34;;
OutputStorage = &#34;/opt/dirac/versions/v13r0p1_1345549839/work/WorkloadManagement/TaskQueueDirector/TQ_180224_02Sq2p/pilotOutput&#34;;
ShallowRetryCount = 0;
ErrorStorage = &#34;/opt/dirac/versions/v13r0p1_1345549839/work/WorkloadManagement/TaskQueueDirector/TQ_180224_02Sq2p/pilotError&#34;
];
Arguments = &#34;-G ilc_prod -M 5 -d -S ILC-Production -C dips&#58;//volcd01.cern.ch&#58;9135/Configuration/Server,dips&#58;//volcd02.cern.ch&#58;9135/Configuration/Server -e ILC -r v13r1p7 -l ILCDIRAC -V ILCDIRAC -T 345600 -o /Resources/Computing/CEDefaults/SubmitPool=gLite -g 2012-02-20 -i 26&#34;;
Environment = { &#34;ParameterValue=0&#34; };
NodeName = &#34;Node_0&#34;;
CertificateSubject = &#34;/DC=ch/DC=cern/OU=Organic Units/OU=Users/CN=sposs/CN=641989/CN=Stephane Guillaume Poss&#34;;
MyProxyServer = &#34;f06876eb75.cern.ch&#34;;
index = isString(cap[6]) && regexp(Lookup,cap[6]) ? 6 &#58; i5;
FuzzyRank = true;
JobType = &#34;normal&#34;;
Executable = &#34;dirac-pilot.py&#34;;
VirtualOrganisation = &#34;ilc&#34;;
SignificantAttributes = { &#34;Requirements&#34;,&#34;Rank&#34;,&#34;FuzzyRank&#34; };
InputSandbox = { &#34;<a href='gsiftp&#58;//wms303.cern.ch&#58;2811/var/SandboxDir/oH/https_3a_2f_2fwms303.cern.ch_3a9000_2foH8T5cOnzfugEnUzHdc8dg/input/dirac-install.py&#34' target='_blank' rel='nofollow'>gsiftp&#58;//wms303.cern.ch&#58;2811/var/SandboxDir/oH/https_3a_2f_2fwms303.cern.ch_3a9000_2foH8T5cOnzfugEnUzHdc8dg/input/dirac-install.py&#34</a>;,&#34;<a href='gsiftp&#58;//wms303.cern.ch&#58;2811/var/SandboxDir/oH/https_3a_2f_2fwms303.cern.ch_3a9000_2foH8T5cOnzfugEnUzHdc8dg/input/dirac-pilot.py&#34' target='_blank' rel='nofollow'>gsiftp&#58;//wms303.cern.ch&#58;2811/var/SandboxDir/oH/https_3a_2f_2fwms303.cern.ch_3a9000_2foH8T5cOnzfugEnUzHdc8dg/input/dirac-pilot.py&#34</a>; };
StdOutput = &#34;StdOut&#34;;
Lookup = &#34;CPUScalingReferenceSI00=*&#34;;
ShallowRetryCount = 0;
QueueTimeRef = real(other.GlueCEPolicyMaxCPUTime * 60);
VOMS_FQAN = &#34;/ilc/Role=production/Capability=NULL&#34;;
InputSandboxDestFileName = { &#34;dirac-install.py&#34;,&#34;dirac-pilot.py&#34; };
OutputSandboxPath = &#34;/var/SandboxDir/14/https_3a_2f_2fwms303.cern.ch_3a9000_2f14eL_5fZ9odMkvuvcLEdP-9w/output&#34;;
pilotRequirements = Rank &#62;  -2 && QueueWorkRef &#62; CPUWorkRef && ( other.GlueCEInfoHostName == &#34;hepgrid6.ph.liv.ac.uk&#34; || other.GlueCEInfoHostName == &#34;hepgrid10.ph.liv.ac.uk&#34; || other.GlueCEInfoHostName == &#34;hepgrid5.ph.liv.ac.uk&#34; || other.GlueCEInfoHostName == &#34;ceprod03.grid.hep.ph.ic.ac.uk&#34; || other.GlueCEInfoHostName == &#34;ceprod04.grid.hep.ph.ic.ac.uk&#34; || other.GlueCEInfoHostName == &#34;ceprod05.grid.hep.ph.ic.ac.uk&#34; || other.GlueCEInfoHostName == &#34;ceprod06.grid.hep.ph.ic.ac.uk&#34; || other.GlueCEInfoHostName == &#34;ceprod07.grid.hep.ph.ic.ac.uk&#34; || other.GlueCEInfoHostName == &#34;ceprod08.grid.hep.ph.ic.ac.uk&#34; || other.GlueCEInfoHostName == &#34;wipp-ce.weizmann.ac.il&#34; || other.GlueCEInfoHostName == &#34;wipp-crm.weizmann.ac.il&#34; || other.GlueCEInfoHostName == &#34;epgr05.ph.bham.ac.uk&#34; || other.GlueCEInfoHostName == &#34;epgr07.ph.bham.ac.uk&#34; || other.GlueCEInfoHostName == &#34;epgr04.ph.bham.ac.uk&#34; || other.GlueCEInfoHostName == &#34;heplnx206.pp.rl.ac.uk&#34; || other.GlueCEInfoHostName == &#34;heplnx207.pp.rl.ac.uk&#34; || other.GlueCEInfoHostName == &#34;heplnx208.pp.rl.ac.uk&#34; || other.GlueCEInfoHostName == &#34;epgr02.ph.bham.ac.uk&#34; || other.GlueCEInfoHostName == &#34;ce02.igfae.usc.es&#34; || other.GlueCEInfoHostName == &#34;ce2.ppgrid1.rhul.ac.uk&#34; || other.GlueCEInfoHostName == &#34;ce3.ppgrid1.rhul.ac.uk&#34; || other.GlueCEInfoHostName == &#34;ce04.esc.qmul.ac.uk&#34; || other.GlueCEInfoHostName == &#34;ce05.esc.qmul.ac.uk&#34; || other.GlueCEInfoHostName == &#34;ce06.esc.qmul.ac.uk&#34; || other.GlueCEInfoHostName == &#34;osg.pnl.gov&#34; || other.GlueCEInfoHostName == &#34;sbgce2.in2p3.fr&#34; || other.GlueCEInfoHostName == &#34;fnpcosg1.fnal.gov&#34; || other.GlueCEInfoHostName == &#34;fermigridosg1.fnal.gov&#34; || other.GlueCEInfoHostName == &#34;gw-6.ccc.ucl.ac.uk&#34; || other.GlueCEInfoHostName == &#34;ce01.tier2.hep.manchester.ac.uk&#34; || other.GlueCEInfoHostName == &#34;ce02.tier2.hep.manchester.ac.uk&#34; );
cap = isList(other.GlueCECapability) ? other.GlueCECapability &#58; { &#34;dummy&#34; };
i = isUndefined(index) ? 0 &#58; index;
requirements = ( ( pilotRequirements && true ) && ( ( other.GlueCEStateStatus == &#34;Production&#34; || other.GlueCEStateStatus == &#34;Special&#34; ) ) ) && ( ( ( ShortDeadlineJob is true ) ? RegExp(&#34;.*sdj$&#34;,other.GlueCEUniqueID) &#58;  !RegExp(&#34;.*sdj$&#34;,other.GlueCEUniqueID) ) && ( other.GlueCEPolicyMaxTotalJobs == 0 || other.GlueCEStateTotalJobs &#60; other.GlueCEPolicyMaxTotalJobs ) && ( EnableWmsFeedback is true ? RegExp(&#34;cream&#34;,other.GlueCEImplementationName,&#34;i&#34;) &#58; true ) );
QueueWorkRef = QueuePowerRef * QueueTimeRef;
CPUPowerRef = 250;
CPUTimeRef = 345600;
rank = ( other.GlueCEStateWaitingJobs == 0 ? ( other.GlueCEStateFreeCPUs * 10 / other.GlueCEInfoTotalCPUs + other.GlueCEInfoTotalCPUs / 500 ) &#58;  -other.GlueCEStateWaitingJobs * 4 / ( other.GlueCEStateRunningJobs + 1 ) - 1 );
Type = &#34;job&#34;;
CPUWorkRef = real(CPUTimeRef * CPUPowerRef);
OutputSandboxDestURI = { &#34;<a href='gsiftp&#58;//wms303.cern.ch&#58;2811/var/SandboxDir/14/https_3a_2f_2fwms303.cern.ch_3a9000_2f14eL_5fZ9odMkvuvcLEdP-9w/output/StdOut&#34' target='_blank' rel='nofollow'>gsiftp&#58;//wms303.cern.ch&#58;2811/var/SandboxDir/14/https_3a_2f_2fwms303.cern.ch_3a9000_2f14eL_5fZ9odMkvuvcLEdP-9w/output/StdOut&#34</a>;,&#34;<a href='gsiftp&#58;//wms303.cern.ch&#58;2811/var/SandboxDir/14/https_3a_2f_2fwms303.cern.ch_3a9000_2f14eL_5fZ9odMkvuvcLEdP-9w/output/StdErr&#34' target='_blank' rel='nofollow'>gsiftp&#58;//wms303.cern.ch&#58;2811/var/SandboxDir/14/https_3a_2f_2fwms303.cern.ch_3a9000_2f14eL_5fZ9odMkvuvcLEdP-9w/output/StdErr&#34</a>;,&#34;<a href='gsiftp&#58;//wms303.cern.ch&#58;2811/var/SandboxDir/14/https_3a_2f_2fwms303.cern.ch_3a9000_2f14eL_5fZ9odMkvuvcLEdP-9w/output/std.out&#34' target='_blank' rel='nofollow'>gsiftp&#58;//wms303.cern.ch&#58;2811/var/SandboxDir/14/https_3a_2f_2fwms303.cern.ch_3a9000_2f14eL_5fZ9odMkvuvcLEdP-9w/output/std.out&#34</a>;,&#34;<a href='gsiftp&#58;//wms303.cern.ch&#58;2811/var/SandboxDir/14/https_3a_2f_2fwms303.cern.ch_3a9000_2f14eL_5fZ9odMkvuvcLEdP-9w/output/std.err&#34' target='_blank' rel='nofollow'>gsiftp&#58;//wms303.cern.ch&#58;2811/var/SandboxDir/14/https_3a_2f_2fwms303.cern.ch_3a9000_2f14eL_5fZ9odMkvuvcLEdP-9w/output/std.err&#34</a>; };
StdError = &#34;StdErr&#34;;
i0 = regexp(Lookup,cap[0]) ? 0 &#58; undefined;
AllowsGenericPilot = Member(&#34;VO-lhcb-pilot&#34;,other.GlueHostApplicationSoftwareRunTimeEnvironment);
i1 = isString(cap[1]) && regexp(Lookup,cap[1]) ? 1 &#58; i0;
DefaultRank =  -other.GlueCEStateEstimatedResponseTime;
i2 = isString(cap[2]) && regexp(Lookup,cap[2]) ? 2 &#58; i1;
i3 = isString(cap[3]) && regexp(Lookup,cap[3]) ? 3 &#58; i2;
i4 = isString(cap[4]) && regexp(Lookup,cap[4]) ? 4 &#58; i3;
WMPInputSandboxBaseURI = &#34;<a href='gsiftp&#58;//wms303.cern.ch&#58;2811/var/SandboxDir/14/https_3a_2f_2fwms303.cern.ch_3a9000_2f14eL_5fZ9odMkvuvcLEdP-9w&#34' target='_blank' rel='nofollow'>gsiftp&#58;//wms303.cern.ch&#58;2811/var/SandboxDir/14/https_3a_2f_2fwms303.cern.ch_3a9000_2f14eL_5fZ9odMkvuvcLEdP-9w&#34</a>;;
i5 = isString(cap[5]) && regexp(Lookup,cap[5]) ? 5 &#58; i4;
X509UserProxy = &#34;/var/SandboxDir/14/https_3a_2f_2fwms303.cern.ch_3a9000_2f14eL_5fZ9odMkvuvcLEdP-9w/user.proxy&#34;;
QueuePowerRef = real( !isUndefined(index) ? int(substr(cap[i],size(Lookup) - 1)) &#58; other.GlueHostBenchmarkSI00);
InputSandboxPath = &#34;/var/SandboxDir/14/https_3a_2f_2fwms303.cern.ch_3a9000_2f14eL_5fZ9odMkvuvcLEdP-9w/input&#34;;
OutputSandbox = { &#34;StdOut&#34;,&#34;StdErr&#34;,&#34;std.out&#34;,&#34;std.err&#34; }
]
---
Event&#58; EnQueued
- Arrived                    =    Wed Oct 31 21&#58;48&#58;47 2012 CET
- Host                       =    wms303.cern.ch
- Level                      =    SYSTEM
- Priority                   =    synchronous
- Queue                      =    /var/workload_manager/jobdir
- Result                     =    OK
- Seqcode                    =    UI=000000&#58;NS=0000000003&#58;WM=000003&#58;BH=0000000000&#58;JSS=000000&#58;LM=000000&#58;LRMS=000000&#58;APP=000000&#58;LBS=000000
- Source                     =    NetworkServer
- Src instance               =    5141
- Timestamp                  =    Wed Oct 31 21&#58;48&#58;47 2012 CET
- User                       =    /DC=ch/DC=cern/OU=Organic Units/OU=Users/CN=sposs/CN=641989/CN=Stephane Guillaume Poss/CN=proxy/CN=proxy/CN=proxy/CN=proxy
- Job            =

[
RetryCount = 0;
edg_jobid = &#34;<a href='https&#58;//wms303.cern.ch&#58;9000/14eL_Z9odMkvuvcLEdP-9w&#34' target='_blank' rel='nofollow'>https&#58;//wms303.cern.ch&#58;9000/14eL_Z9odMkvuvcLEdP-9w&#34</a>;;
lrms_type = &#34;condor&#34;;
WmsClient =
[
RetryCount = 0;
LBEndPoints = { &#34;<a href='https&#58;//lb101.cern.ch&#58;9000&#34' target='_blank' rel='nofollow'>https&#58;//lb101.cern.ch&#58;9000&#34</a>; };
WMProxyEndPoints = { &#34;<a href='https&#58;//wmsshared.cern.ch&#58;7443/glite_wms_wmproxy_server&#34' target='_blank' rel='nofollow'>https&#58;//wmsshared.cern.ch&#58;7443/glite_wms_wmproxy_server&#34</a>; };
EnableServiceDiscovery = false;
JdlDefaultAttributes =
[
PerusalFileEnable = false;
SignificantAttributes = { &#34;Requirements&#34;,&#34;Rank&#34;,&#34;FuzzyRank&#34; };
requirements = ( other.GlueCEStateStatus == &#34;Production&#34; || other.GlueCEStateStatus == &#34;Special&#34; );
AllowZippedISB = true
];
ListenerStorage = &#34;/opt/dirac/versions/v13r0p1_1345549839/work/WorkloadManagement/TaskQueueDirector/TQ_180224_02Sq2p/Storage&#34;;
MyProxyServer = &#34;f06876eb75.cern.ch&#34;;
OutputStorage = &#34;/opt/dirac/versions/v13r0p1_1345549839/work/WorkloadManagement/TaskQueueDirector/TQ_180224_02Sq2p/pilotOutput&#34;;
ShallowRetryCount = 0;
ErrorStorage = &#34;/opt/dirac/versions/v13r0p1_1345549839/work/WorkloadManagement/TaskQueueDirector/TQ_180224_02Sq2p/pilotError&#34;
];
CEInfoHostName = &#34;fnpcosg1.fnal.gov&#34;;
Arguments = &#34;-G ilc_prod -M 5 -d -S ILC-Production -C dips&#58;//volcd01.cern.ch&#58;9135/Configuration/Server,dips&#58;//volcd02.cern.ch&#58;9135/Configuration/Server -e ILC -r v13r1p7 -l ILCDIRAC -V ILCDIRAC -T 345600 -o /Resources/Computing/CEDefaults/SubmitPool=gLite -g 2012-02-20 -i 26&#34;;
Environment = { &#34;ParameterValue=0&#34; };
NodeName = &#34;Node_0&#34;;
CertificateSubject = &#34;/DC=ch/DC=cern/OU=Organic Units/OU=Users/CN=sposs/CN=641989/CN=Stephane Guillaume Poss&#34;;
MyProxyServer = &#34;f06876eb75.cern.ch&#34;;
ce_id = &#34;fnpcosg1.fnal.gov&#58;2119/jobmanager-condor-group_ilc&#34;;
QueueName = &#34;group_ilc&#34;;
index = isString(cap[6]) && regexp(Lookup,cap[6]) ? 6 &#58; i5;
FuzzyRank = true;
JobType = &#34;normal&#34;;
Executable = &#34;dirac-pilot.py&#34;;
VirtualOrganisation = &#34;ilc&#34;;
SignificantAttributes = { &#34;Requirements&#34;,&#34;Rank&#34;,&#34;FuzzyRank&#34; };
InputSandbox = { &#34;<a href='gsiftp&#58;//wms303.cern.ch&#58;2811/var/SandboxDir/oH/https_3a_2f_2fwms303.cern.ch_3a9000_2foH8T5cOnzfugEnUzHdc8dg/input/dirac-install.py&#34' target='_blank' rel='nofollow'>gsiftp&#58;//wms303.cern.ch&#58;2811/var/SandboxDir/oH/https_3a_2f_2fwms303.cern.ch_3a9000_2foH8T5cOnzfugEnUzHdc8dg/input/dirac-install.py&#34</a>;,&#34;<a href='gsiftp&#58;//wms303.cern.ch&#58;2811/var/SandboxDir/oH/https_3a_2f_2fwms303.cern.ch_3a9000_2foH8T5cOnzfugEnUzHdc8dg/input/dirac-pilot.py&#34' target='_blank' rel='nofollow'>gsiftp&#58;//wms303.cern.ch&#58;2811/var/SandboxDir/oH/https_3a_2f_2fwms303.cern.ch_3a9000_2foH8T5cOnzfugEnUzHdc8dg/input/dirac-pilot.py&#34</a>;,&#34;<a href='gsiftp&#58;//wms303.cern.ch&#58;2811/var/SandboxDir/14/https_3a_2f_2fwms303.cern.ch_3a9000_2f14eL_5fZ9odMkvuvcLEdP-9w/input/.BrokerInfo&#34' target='_blank' rel='nofollow'>gsiftp&#58;//wms303.cern.ch&#58;2811/var/SandboxDir/14/https_3a_2f_2fwms303.cern.ch_3a9000_2f14eL_5fZ9odMkvuvcLEdP-9w/input/.BrokerInfo&#34</a>; };
StdOutput = &#34;StdOut&#34;;
Lookup = &#34;CPUScalingReferenceSI00=*&#34;;
ShallowRetryCount = 0;
QueueTimeRef = real(other.GlueCEPolicyMaxCPUTime * 60);
VOMS_FQAN = &#34;/ilc/Role=production/Capability=NULL&#34;;
InputSandboxDestFileName = { &#34;dirac-install.py&#34;,&#34;dirac-pilot.py&#34; };
OutputSandboxPath = &#34;/var/SandboxDir/14/https_3a_2f_2fwms303.cern.ch_3a9000_2f14eL_5fZ9odMkvuvcLEdP-9w/output&#34;;
pilotRequirements = Rank &#62;  -2 && QueueWorkRef &#62; CPUWorkRef && ( other.GlueCEInfoHostName == &#34;hepgrid6.ph.liv.ac.uk&#34; || other.GlueCEInfoHostName == &#34;hepgrid10.ph.liv.ac.uk&#34; || other.GlueCEInfoHostName == &#34;hepgrid5.ph.liv.ac.uk&#34; || other.GlueCEInfoHostName == &#34;ceprod03.grid.hep.ph.ic.ac.uk&#34; || other.GlueCEInfoHostName == &#34;ceprod04.grid.hep.ph.ic.ac.uk&#34; || other.GlueCEInfoHostName == &#34;ceprod05.grid.hep.ph.ic.ac.uk&#34; || other.GlueCEInfoHostName == &#34;ceprod06.grid.hep.ph.ic.ac.uk&#34; || other.GlueCEInfoHostName == &#34;ceprod07.grid.hep.ph.ic.ac.uk&#34; || other.GlueCEInfoHostName == &#34;ceprod08.grid.hep.ph.ic.ac.uk&#34; || other.GlueCEInfoHostName == &#34;wipp-ce.weizmann.ac.il&#34; || other.GlueCEInfoHostName == &#34;wipp-crm.weizmann.ac.il&#34; || other.GlueCEInfoHostName == &#34;epgr05.ph.bham.ac.uk&#34; || other.GlueCEInfoHostName == &#34;epgr07.ph.bham.ac.uk&#34; || other.GlueCEInfoHostName == &#34;epgr04.ph.bham.ac.uk&#34; || other.GlueCEInfoHostName == &#34;heplnx206.pp.rl.ac.uk&#34; || other.GlueCEInfoHostName == &#34;heplnx207.pp.rl.ac.uk&#34; || other.GlueCEInfoHostName == &#34;heplnx208.pp.rl.ac.uk&#34; || other.GlueCEInfoHostName == &#34;epgr02.ph.bham.ac.uk&#34; || other.GlueCEInfoHostName == &#34;ce02.igfae.usc.es&#34; || other.GlueCEInfoHostName == &#34;ce2.ppgrid1.rhul.ac.uk&#34; || other.GlueCEInfoHostName == &#34;ce3.ppgrid1.rhul.ac.uk&#34; || other.GlueCEInfoHostName == &#34;ce04.esc.qmul.ac.uk&#34; || other.GlueCEInfoHostName == &#34;ce05.esc.qmul.ac.uk&#34; || other.GlueCEInfoHostName == &#34;ce06.esc.qmul.ac.uk&#34; || other.GlueCEInfoHostName == &#34;osg.pnl.gov&#34; || other.GlueCEInfoHostName == &#34;sbgce2.in2p3.fr&#34; || other.GlueCEInfoHostName == &#34;fnpcosg1.fnal.gov&#34; || other.GlueCEInfoHostName == &#34;fermigridosg1.fnal.gov&#34; || other.GlueCEInfoHostName == &#34;gw-6.ccc.ucl.ac.uk&#34; || other.GlueCEInfoHostName == &#34;ce01.tier2.hep.manchester.ac.uk&#34; || other.GlueCEInfoHostName == &#34;ce02.tier2.hep.manchester.ac.uk&#34; );
cap = isList(other.GlueCECapability) ? other.GlueCECapability &#58; { &#34;dummy&#34; };
i = isUndefined(index) ? 0 &#58; index;
requirements = ( ( pilotRequirements && true ) && ( ( other.GlueCEStateStatus == &#34;Production&#34; || other.GlueCEStateStatus == &#34;Special&#34; ) ) ) && ( ( ( ShortDeadlineJob is true ) ? RegExp(&#34;.*sdj$&#34;,other.GlueCEUniqueID) &#58;  !RegExp(&#34;.*sdj$&#34;,other.GlueCEUniqueID) ) && ( other.GlueCEPolicyMaxTotalJobs == 0 || other.GlueCEStateTotalJobs &#60; other.GlueCEPolicyMaxTotalJobs ) && ( EnableWmsFeedback is true ? RegExp(&#34;cream&#34;,other.GlueCEImplementationName,&#34;i&#34;) &#58; true ) );
QueueWorkRef = QueuePowerRef * QueueTimeRef;
CPUPowerRef = 250;
CPUTimeRef = 345600;
rank = ( other.GlueCEStateWaitingJobs == 0 ? ( other.GlueCEStateFreeCPUs * 10 / other.GlueCEInfoTotalCPUs + other.GlueCEInfoTotalCPUs / 500 ) &#58;  -other.GlueCEStateWaitingJobs * 4 / ( other.GlueCEStateRunningJobs + 1 ) - 1 );
Type = &#34;job&#34;;
CPUWorkRef = real(CPUTimeRef * CPUPowerRef);
OutputSandboxDestURI = { &#34;<a href='gsiftp&#58;//wms303.cern.ch&#58;2811/var/SandboxDir/14/https_3a_2f_2fwms303.cern.ch_3a9000_2f14eL_5fZ9odMkvuvcLEdP-9w/output/StdOut&#34' target='_blank' rel='nofollow'>gsiftp&#58;//wms303.cern.ch&#58;2811/var/SandboxDir/14/https_3a_2f_2fwms303.cern.ch_3a9000_2f14eL_5fZ9odMkvuvcLEdP-9w/output/StdOut&#34</a>;,&#34;<a href='gsiftp&#58;//wms303.cern.ch&#58;2811/var/SandboxDir/14/https_3a_2f_2fwms303.cern.ch_3a9000_2f14eL_5fZ9odMkvuvcLEdP-9w/output/StdErr&#34' target='_blank' rel='nofollow'>gsiftp&#58;//wms303.cern.ch&#58;2811/var/SandboxDir/14/https_3a_2f_2fwms303.cern.ch_3a9000_2f14eL_5fZ9odMkvuvcLEdP-9w/output/StdErr&#34</a>;,&#34;<a href='gsiftp&#58;//wms303.cern.ch&#58;2811/var/SandboxDir/14/https_3a_2f_2fwms303.cern.ch_3a9000_2f14eL_5fZ9odMkvuvcLEdP-9w/output/std.out&#34' target='_blank' rel='nofollow'>gsiftp&#58;//wms303.cern.ch&#58;2811/var/SandboxDir/14/https_3a_2f_2fwms303.cern.ch_3a9000_2f14eL_5fZ9odMkvuvcLEdP-9w/output/std.out&#34</a>;,&#34;<a href='gsiftp&#58;//wms303.cern.ch&#58;2811/var/SandboxDir/14/https_3a_2f_2fwms303.cern.ch_3a9000_2f14eL_5fZ9odMkvuvcLEdP-9w/output/std.err&#34' target='_blank' rel='nofollow'>gsiftp&#58;//wms303.cern.ch&#58;2811/var/SandboxDir/14/https_3a_2f_2fwms303.cern.ch_3a9000_2f14eL_5fZ9odMkvuvcLEdP-9w/output/std.err&#34</a>; };
StdError = &#34;StdErr&#34;;
i0 = regexp(Lookup,cap[0]) ? 0 &#58; undefined;
AllowsGenericPilot = Member(&#34;VO-lhcb-pilot&#34;,other.GlueHostApplicationSoftwareRunTimeEnvironment);
i1 = isString(cap[1]) && regexp(Lookup,cap[1]) ? 1 &#58; i0;
DefaultRank =  -other.GlueCEStateEstimatedResponseTime;
i2 = isString(cap[2]) && regexp(Lookup,cap[2]) ? 2 &#58; i1;
i3 = isString(cap[3]) && regexp(Lookup,cap[3]) ? 3 &#58; i2;
i4 = isString(cap[4]) && regexp(Lookup,cap[4]) ? 4 &#58; i3;
WMPInputSandboxBaseURI = &#34;<a href='gsiftp&#58;//wms303.cern.ch&#58;2811/var/SandboxDir/14/https_3a_2f_2fwms303.cern.ch_3a9000_2f14eL_5fZ9odMkvuvcLEdP-9w&#34' target='_blank' rel='nofollow'>gsiftp&#58;//wms303.cern.ch&#58;2811/var/SandboxDir/14/https_3a_2f_2fwms303.cern.ch_3a9000_2f14eL_5fZ9odMkvuvcLEdP-9w&#34</a>;;
i5 = isString(cap[5]) && regexp(Lookup,cap[5]) ? 5 &#58; i4;
X509UserProxy = &#34;/var/SandboxDir/14/https_3a_2f_2fwms303.cern.ch_3a9000_2f14eL_5fZ9odMkvuvcLEdP-9w/user.proxy&#34;;
GlobusResourceContactString = &#34;fnpcosg1.fnal.gov&#58;2119/jobmanager-condor&#34;;
QueuePowerRef = real( !isUndefined(index) ? int(substr(cap[i],size(Lookup) - 1)) &#58; other.GlueHostBenchmarkSI00);
InputSandboxPath = &#34;/var/SandboxDir/14/https_3a_2f_2fwms303.cern.ch_3a9000_2f14eL_5fZ9odMkvuvcLEdP-9w/input&#34;;
OutputSandbox = { &#34;StdOut&#34;,&#34;StdErr&#34;,&#34;std.out&#34;,&#34;std.err&#34; }
]
---
Event&#58; Match
- Arrived                    =    Wed Oct 31 21&#58;48&#58;47 2012 CET
- Dest id                    =    fnpcosg1.fnal.gov&#58;2119/jobmanager-condor-group_ilc
- Host                       =    wms303.cern.ch
- Level                      =    SYSTEM
- Priority                   =    synchronous
- Seqcode                    =    UI=000000&#58;NS=0000000004&#58;WM=000003&#58;BH=0000000000&#58;JSS=000000&#58;LM=000000&#58;LRMS=000000&#58;APP=000000&#58;LBS=000000
- Source                     =    WorkloadManager
- Src instance               =    5141
- Timestamp                  =    Wed Oct 31 21&#58;48&#58;47 2012 CET
- User                       =    /DC=ch/DC=cern/OU=Organic Units/OU=Users/CN=sposs/CN=641989/CN=Stephane Guillaume Poss/CN=proxy/CN=proxy/CN=proxy/CN=proxy
---
Event&#58; UserTag
- Arrived                    =    Wed Oct 31 21&#58;48&#58;47 2012 CET
- Host                       =    wms303.cern.ch
- Level                      =    SYSTEM
- Name                       =    CEInfoHostName
- Priority                   =    synchronous
- Seqcode                    =    UI=000000&#58;NS=0000000004&#58;WM=000004&#58;BH=0000000000&#58;JSS=000000&#58;LM=000000&#58;LRMS=000000&#58;APP=000000&#58;LBS=000000
- Source                     =    WorkloadManager
- Src instance               =    5141
- Timestamp                  =    Wed Oct 31 21&#58;48&#58;47 2012 CET
- User                       =    /DC=ch/DC=cern/OU=Organic Units/OU=Users/CN=sposs/CN=641989/CN=Stephane Guillaume Poss/CN=proxy/CN=proxy/CN=proxy/CN=proxy
- Value                      =    fnpcosg1.fnal.gov
---
Event&#58; EnQueued
- Arrived                    =    Wed Oct 31 21&#58;48&#58;47 2012 CET
- Host                       =    wms303.cern.ch
- Level                      =    SYSTEM
- Priority                   =    synchronous
- Queue                      =    /var/jobcontrol/jobdir
- Result                     =    START
- Seqcode                    =    UI=000000&#58;NS=0000000004&#58;WM=000005&#58;BH=0000000000&#58;JSS=000000&#58;LM=000000&#58;LRMS=000000&#58;APP=000000&#58;LBS=000000
- Source                     =    WorkloadManager
- Src instance               =    5141
- Timestamp                  =    Wed Oct 31 21&#58;48&#58;47 2012 CET
- User                       =    /DC=ch/DC=cern/OU=Organic Units/OU=Users/CN=sposs/CN=641989/CN=Stephane Guillaume Poss/CN=proxy/CN=proxy/CN=proxy/CN=proxy
---
Event&#58; EnQueued
- Arrived                    =    Wed Oct 31 21&#58;48&#58;47 2012 CET
- Host                       =    wms303.cern.ch
- Level                      =    SYSTEM
- Priority                   =    synchronous
- Queue                      =    /var/jobcontrol/jobdir
- Result                     =    OK
- Seqcode                    =    UI=000000&#58;NS=0000000004&#58;WM=000006&#58;BH=0000000000&#58;JSS=000000&#58;LM=000000&#58;LRMS=000000&#58;APP=000000&#58;LBS=000000
- Source                     =    WorkloadManager
- Src instance               =    5141
- Timestamp                  =    Wed Oct 31 21&#58;48&#58;47 2012 CET
- User                       =    /DC=ch/DC=cern/OU=Organic Units/OU=Users/CN=sposs/CN=641989/CN=Stephane Guillaume Poss/CN=proxy/CN=proxy/CN=proxy/CN=proxy
- Job            =

[
Arguments =
[
JobAd =
[
UserSubjectName = &#34;/DC=ch/DC=cern/OU=Organic Units/OU=Users/CN=sposs/CN=641989/CN=Stephane Guillaume Poss&#34;;
LB_sequence_code = &#34;UI=000000&#58;NS=0000000004&#58;WM=000006&#58;BH=0000000000&#58;JSS=000000&#58;LM=000000&#58;LRMS=000000&#58;APP=000000&#58;LBS=000000&#34;;
stream_output = false;
edg_jobid = &#34;<a href='https&#58;//wms303.cern.ch&#58;9000/14eL_Z9odMkvuvcLEdP-9w&#34' target='_blank' rel='nofollow'>https&#58;//wms303.cern.ch&#58;9000/14eL_Z9odMkvuvcLEdP-9w&#34</a>;;
Notification = &#34;never&#34;;
CEInfoHostName = &#34;fnpcosg1.fnal.gov&#34;;
ce_id = &#34;fnpcosg1.fnal.gov&#58;2119/jobmanager-condor-group_ilc&#34;;
Transfer_Executable = true;
Output = &#34;/var/jobcontrol/condorio/14/https_3a_2f_2fwms303.cern.ch_3a9000_2f14eL_5fZ9odMkvuvcLEdP-9w/StandardOutput&#34;;
Copy_to_Spool = false;
grid_resource = &#34;gt2 fnpcosg1.fnal.gov&#58;2119/jobmanager-condor&#34;;
Executable = &#34;/var/jobcontrol/submit/14/JobWrapper.https_3a_2f_2fwms303.cern.ch_3a9000_2f14eL_5fZ9odMkvuvcLEdP-9w.sh&#34;;
Error_ = &#34;/var/jobcontrol/condorio/14/https_3a_2f_2fwms303.cern.ch_3a9000_2f14eL_5fZ9odMkvuvcLEdP-9w/StandardError&#34;;
stream_error = false;
GlobusRSL = &#34;(queue=group_ilc)(jobtype=single)(environment=(EDG_WL_JOBID &#39;<a href='https&#58;//wms303.cern.ch&#58;9000/14eL_Z9odMkvuvcLEdP-9w&#39' target='_blank' rel='nofollow'>https&#58;//wms303.cern.ch&#58;9000/14eL_Z9odMkvuvcLEdP-9w&#39</a>;))&#34;;
Type = &#34;job&#34;;
Universe = &#34;grid&#34;;
Log = &#34;/var/logmonitor/CondorG.log/CondorG.log&#34;;
grid_type = &#34;globus&#34;;
X509UserProxy = &#34;/var/SandboxDir/14/https_3a_2f_2fwms303.cern.ch_3a9000_2f14eL_5fZ9odMkvuvcLEdP-9w/user.proxy&#34;;
GlobusScheduler = &#34;fnpcosg1.fnal.gov&#58;2119/jobmanager-condor&#34;
]
];
Command = &#34;Submit&#34;;
Source = 2;
Protocol = &#34;1.0.0&#34;
]
---
Event&#58; DeQueued
- Arrived                    =    Wed Oct 31 21&#58;48&#58;48 2012 CET
- Host                       =    wms303.cern.ch
- Level                      =    SYSTEM
- Local jobid                =    unavailable
- Priority                   =    synchronous
- Queue                      =    /var/jobcontrol/jobdir
- Seqcode                    =    UI=000000&#58;NS=0000000004&#58;WM=000006&#58;BH=0000000000&#58;JSS=000001&#58;LM=000000&#58;LRMS=000000&#58;APP=000000&#58;LBS=000000
- Source                     =    JobController
- Src instance               =    unique
- Timestamp                  =    Wed Oct 31 21&#58;48&#58;48 2012 CET
- User                       =    /DC=ch/DC=cern/OU=Organic Units/OU=Users/CN=sposs/CN=641989/CN=Stephane Guillaume Poss/CN=proxy/CN=proxy/CN=proxy/CN=proxy
---
Event&#58; Transfer
- Arrived                    =    Wed Oct 31 21&#58;48&#58;48 2012 CET
- Dest host                  =    localhost
- Dest instance              =    /var/logmonitor/CondorG.log/CondorG.1351705181.log
- Dest jobid                 =    unavailable
- Destination                =    LogMonitor
- Host                       =    wms303.cern.ch
- Level                      =    SYSTEM
- Priority                   =    synchronous
- Reason                     =    unavailable
- Result                     =    START
- Seqcode                    =    UI=000000&#58;NS=0000000004&#58;WM=000006&#58;BH=0000000000&#58;JSS=000002&#58;LM=000000&#58;LRMS=000000&#58;APP=000000&#58;LBS=000000
- Source                     =    JobController
- Src instance               =    unique
- Timestamp                  =    Wed Oct 31 21&#58;48&#58;48 2012 CET
- User                       =    /DC=ch/DC=cern/OU=Organic Units/OU=Users/CN=sposs/CN=641989/CN=Stephane Guillaume Poss/CN=proxy/CN=proxy/CN=proxy/CN=proxy
- Job            =   unavailable
---
Event&#58; Transfer
- Arrived                    =    Wed Oct 31 21&#58;48&#58;48 2012 CET
- Dest host                  =    localhost
- Dest instance              =    /var/logmonitor/CondorG.log/CondorG.1351705181.log
- Dest jobid                 =    901503
- Destination                =    LogMonitor
- Host                       =    wms303.cern.ch
- Level                      =    SYSTEM
- Priority                   =    synchronous
- Reason                     =    unavailable
- Result                     =    OK
- Seqcode                    =    UI=000000&#58;NS=0000000004&#58;WM=000006&#58;BH=0000000000&#58;JSS=000003&#58;LM=000000&#58;LRMS=000000&#58;APP=000000&#58;LBS=000000
- Source                     =    JobController
- Src instance               =    unique
- Timestamp                  =    Wed Oct 31 21&#58;48&#58;48 2012 CET
- User                       =    /DC=ch/DC=cern/OU=Organic Units/OU=Users/CN=sposs/CN=641989/CN=Stephane Guillaume Poss/CN=proxy/CN=proxy/CN=proxy/CN=proxy
- Job            =   (unavailable)
---
Event&#58; Accepted
- Arrived                    =    Wed Oct 31 21&#58;48&#58;54 2012 CET
- From                       =    JobController
- From host                  =    localhost
- From instance              =    unavailable
- Host                       =    wms303.cern.ch
- Level                      =    SYSTEM
- Local jobid                =    901503
- Priority                   =    synchronous
- Seqcode                    =    UI=000000&#58;NS=0000000004&#58;WM=000006&#58;BH=0000000000&#58;JSS=000003&#58;LM=000001&#58;LRMS=000000&#58;APP=000000&#58;LBS=000000
- Source                     =    LogMonitor
- Src instance               =    unique
- Timestamp                  =    Wed Oct 31 21&#58;48&#58;54 2012 CET
---
Event&#58; Transfer
- Arrived                    =    Wed Oct 31 21&#58;49&#58;07 2012 CET
- Dest host                  =    fnpcosg1.fnal.gov&#58;2119/jobmanager-condor
- Dest instance              =    /var/logmonitor/CondorG.log/CondorG.1351705181.log
- Dest jobid                 =    unavailable
- Destination                =    LRMS
- Host                       =    wms303.cern.ch
- Level                      =    SYSTEM
- Priority                   =    synchronous
- Reason                     =    Job successfully submitted to Globus
- Result                     =    OK
- Seqcode                    =    UI=000000&#58;NS=0000000004&#58;WM=000006&#58;BH=0000000000&#58;JSS=000003&#58;LM=000003&#58;LRMS=000000&#58;APP=000000&#58;LBS=000000
- Source                     =    LogMonitor
- Src instance               =    unique
- Timestamp                  =    Wed Oct 31 21&#58;49&#58;07 2012 CET
- Job            =   (queue=group_ilc)(jobtype=single)(environment=(EDG_WL_JOBID &#39;<a href='https&#58;//wms303.cern.ch&#58;9000/14eL_Z9odMkvuvcLEdP-9w&#39' target='_blank' rel='nofollow'>https&#58;//wms303.cern.ch&#58;9000/14eL_Z9odMkvuvcLEdP-9w&#39</a>;))
---
Event&#58; Running
- Arrived                    =    Wed Oct 31 21&#58;51&#58;33 2012 CET
- Host                       =    wms303.cern.ch
- Level                      =    SYSTEM
- Node                       =    gt2 fnpcosg1.fnal.gov&#58;2119/jobmanager-condor
- Priority                   =    synchronous
- Seqcode                    =    UI=000000&#58;NS=0000000004&#58;WM=000006&#58;BH=0000000000&#58;JSS=000003&#58;LM=000005&#58;LRMS=000000&#58;APP=000000&#58;LBS=000000
- Source                     =    LogMonitor
- Src instance               =    unique
- Timestamp                  =    Wed Oct 31 21&#58;51&#58;33 2012 CET
---
Event&#58; Done
- Arrived                    =    Wed Oct 31 22&#58;54&#58;53 2012 CET
- Exit code                  =    0
- Host                       =    wms303.cern.ch
- Level                      =    SYSTEM
- Priority                   =    synchronous
- Reason                     =    Job terminated successfully
- Seqcode                    =    UI=000000&#58;NS=0000000004&#58;WM=000006&#58;BH=0000000000&#58;JSS=000003&#58;LM=000007&#58;LRMS=000000&#58;APP=000000&#58;LBS=000000
- Source                     =    LogMonitor
- Src instance               =    unique
- Status code                =    OK
- Timestamp                  =    Wed Oct 31 22&#58;54&#58;53 2012 CET
==========================================================================

*** Log file created ***
Possible Errors and Debug messages have been printed in the following

From&#58; Fermilab Service Desk [mailto&#58;fermi@....]
Sent&#58; 01 November 2012 00&#58;11
To&#58; Steven Timm; ilc-vo-support@....
Subject&#58; [ilc-vo-support] Incident INC000000327794 updated -- ilc jobs overwhelming OSG CE gatekeeper node on TTU-Antaeus

Incident INC000000327794&#60;<a href='https&#58;//fermi.service-now.com/nav_to.do?uri=incident.do%3Fsys_id=623d00d9f0d1b080075744d687cc2b60%26sysparm_stack=incident_list.do%3Fsysparm_query=active=true&#62' target='_blank' rel='nofollow'>https&#58;//fermi.service-now.com/nav_to.do?uri=incident.do%3Fsys_id=623d00d9f0d1b080075744d687cc2b60%26sysparm_stack=incident_list.do%3Fsysparm_query=active=true&#62</a>; comments have been updated.

Last update&#58;
2012-10-31 23&#58;10&#58;13 - cd-srv-goc-ops (Additional comments)
Hi Stephane,

Can you provide the logging of an example job you are running? And/or the JDL you are using? I can provide a some guidance on the best way forward.

Are you enabling fuzzy ranking?

Brian

by /DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=People/CN=Brian Bockelman
-- by goc at Wed Oct 31 23&#58;09&#58;06 UTC 2012

________________________________
Incident State&#58; Work in Progress
Caller&#58; cd-srv-goc-ops
Assignment Group&#58; International Linear Collider VO
Assigned To&#58; Steven Timm
Summary&#58;
[by Alan Sill]

Our cluster is receiving a very high rate of globus-job-manager jobs from the ilc VO, leading to a load over 1000 on the gatekeeper.

I have tried shutting off grid services (we are still using OSG 1.2 on this resource) and terminating all ilc processes, then removing ilc from the list of supported VOs in the $DT_LOCATION/monitoring/osg-supported-vo-list.txt file, then re-running the osg-configure -c step, and also removing all gram_scratch files from the ilc user home area, b u these jobs seem to re-appear after we turn VDT services back on.

Can you please&#58;

1) Contact the ILC VO, especially the users below, to ask them to look into their workflow,

2) Please advise how we can remove ILC from the locally supported VOs until this is done/

I may have to block the ILC at the GUMS level to prevent our CE from being overloaded in the meantime.

Thanks,
Alan

PID&#58; 25783 -- PRIMA INFO ts=2012-10-23T14&#58;13&#58;38-06&#58;00 event=org.osg.prima.authz.end status=0 decision=PERMIT DN=&#34;/DC=ch/DC=cern/OU=Organic Units/OU=Users/CN=sposs/CN=641989/CN=Stephane Guillaume Poss&#34; FQAN=&#34;/ilc/Role=production/Capability=NULL&#34; FQAN_Issuer=&#34;/C=DE/O=GermanGrid/OU=DESY/CN=host/grid-voms.desy.de&#34; Service_URL=&#34;<a href='https&#58;//gums.hpcc.ttu.edu&#58;8443/gums/services/GUMSXACMLAuthorizationServicePort&#34' target='_blank' rel='nofollow'>https&#58;//gums.hpcc.ttu.edu&#58;8443/gums/services/GUMSXACMLAuthorizationServicePort&#34</a>; local_user=ilc â€¦
PID&#58; 26163 -- PRIMA INFO ts=2012-10-23T14&#58;13&#58;38-06&#58;00 event=org.osg.prima.authz.start DN=&#34;/C=JP/O=KEK/OU=CRC/CN=TIAN Junping&#34; FQAN=&#34;/ilc/Role=NULL/Capability=NULL&#34; FQAN_Issuer=&#34;/C=DE/O=GermanGrid/OU=DESY/CN=host/grid-voms.desy.de&#34; Service_URL=&#34;<a href='https&#58;//gums.hpcc.ttu.edu&#58;8443/gums/services/GUMSXACMLAuthorizationServicePort&#34' target='_blank' rel='nofollow'>https&#58;//gums.hpcc.ttu.edu&#58;8443/gums/services/GUMSXACMLAuthorizationServicePort&#34</a>;

</div><script type='text/javascript'>
        $('#show_933420894').click(function() {
            $('#detail_933420894').slideDown("normal");
            $('#show_933420894').hide();
            $('#hide_933420894').show();
        });
        $('#hide_933420894').click(function() {
            $('#detail_933420894').slideUp();
            $('#hide_933420894').hide();
            $('#show_933420894').show();
        });
        </script></pre></div><div class='update_description'><i onclick="document.location='12876#1351724946'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2012-10-31T23:09:06+00:00">Oct 31, 2012 11:09 PM UTC</time><a class="anchor" name="1351724946">&nbsp;</a></div><pre>Hi Stephane,

Can you provide the logging of an example job you are running?  And/or the JDL you are using?  I can provide a some guidance on the best way forward.

Are you enabling fuzzy ranking?

Brian

by /DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=People/CN=Brian Bockelman</pre></div><div class='update_description'><i onclick="document.location='12876#1351724650'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2012-10-31T23:04:10+00:00">Oct 31, 2012 11:04 PM UTC</time> by <b>OSG-GOC</b><a class="anchor" name="1351724650">&nbsp;</a></div><pre>reply from&#58; stephane.guillaume.poss@....

Hi,
There are several issues here&#58; the use of GLUE CE things, and the issues with our software at TTU. For the former, we are not doing anything&#58; gLite is taking care of the scheduling, so if the info published in the bdii is wrong (or default to very high values), then it will be doing very wrong things. But as far as Iâ€™m concerned, I see this as a bug in the gLite middleware if â€˜defaultâ€™ values are not treated properly, not the end userâ€™s responsibilityâ€¦
Now for the latter problem&#58; the issues we had in TTU was that one of our python library, the xml.elementtree (I think) is not the right version because we do some xml parsing with this and it produces an exception. This is not the siteâ€™s fault as we come with our python things and very likely for the platform running in TTU the version is â€œoldâ€ and/or buggy. Itâ€™s very likely that other jobs not using this xml parsing will work like a charm (they used to at least).

Cheers, and thanks for the follow up.

S Poss.

From&#58; Fermilab Service Desk [mailto&#58;fermi@....]
Sent&#58; 31 October 2012 22&#58;47
To&#58; Steven Timm; ilc-vo-support@....
Subject&#58; [ilc-vo-support] Incident INC000000327794 updated -- ilc jobs overwhelming OSG CE gatekeeper node on TTU-Antaeus

Incident INC000000327794&#60;<a href='https&#58;//fermi.service-now.com/nav_to.do?uri=incident.do%3Fsys_id=623d00d9f0d1b080075744d687cc2b60%26sysparm_stack=incident_list.do%3Fsysparm_query=active=true&#62' target='_blank' rel='nofollow'>https&#58;//fermi.service-now.com/nav_to.do?uri=incident.do%3Fsys_id=623d00d9f0d1b080075744d687cc2b60%26sysparm_stack=incident_list.do%3Fsysparm_query=active=true&#62</a>; comments have been updated.

Last update&#58;
2012-10-31 21&#58;45&#58;22 - cd-srv-goc-ops (Additional comments)
Brian&#58; interesting; but if I check this for our cluster, I see that GlueCEStateWorstResponseTime and GlueCEStateEstimatedResponseTime return the same value (3600) for all of our advertised queues, even though that cluster is right now pretty full.

Is there something else we should check?

Also, it would be nice to get a response on this point from the ILC folks on this ticket, as well as one to our query about what caused their jobs to fail in the first place. We&#39;re happy to fix anything that we have set amiss!

Alan

by /DC=org/DC=doegrids/OU=People/CN=Alan Sill 503049
-- by goc at Wed Oct 31 21&#58;44&#58;43 UTC 2012

________________________________
Incident State&#58; Work in Progress
Caller&#58; cd-srv-goc-ops
Assignment Group&#58; International Linear Collider VO
Assigned To&#58; Steven Timm
<div id='show_1985435202' class=''><button class="btn">Show More</button></div><div class='detail hidden' id='detail_1985435202'>Summary&#58;
[by Alan Sill]

Our cluster is receiving a very high rate of globus-job-manager jobs from the ilc VO, leading to a load over 1000 on the gatekeeper.

I have tried shutting off grid services (we are still using OSG 1.2 on this resource) and terminating all ilc processes, then removing ilc from the list of supported VOs in the $DT_LOCATION/monitoring/osg-supported-vo-list.txt file, then re-running the osg-configure -c step, and also removing all gram_scratch files from the ilc user home area, b u these jobs seem to re-appear after we turn VDT services back on.

Can you please&#58;

1) Contact the ILC VO, especially the users below, to ask them to look into their workflow,

2) Please advise how we can remove ILC from the locally supported VOs until this is done/

I may have to block the ILC at the GUMS level to prevent our CE from being overloaded in the meantime.

Thanks,
Alan

PID&#58; 25783 -- PRIMA INFO ts=2012-10-23T14&#58;13&#58;38-06&#58;00 event=org.osg.prima.authz.end status=0 decision=PERMIT DN=&#34;/DC=ch/DC=cern/OU=Organic Units/OU=Users/CN=sposs/CN=641989/CN=Stephane Guillaume Poss&#34; FQAN=&#34;/ilc/Role=production/Capability=NULL&#34; FQAN_Issuer=&#34;/C=DE/O=GermanGrid/OU=DESY/CN=host/grid-voms.desy.de&#34; Service_URL=&#34;<a href='https&#58;//gums.hpcc.ttu.edu&#58;8443/gums/services/GUMSXACMLAuthorizationServicePort&#34' target='_blank' rel='nofollow'>https&#58;//gums.hpcc.ttu.edu&#58;8443/gums/services/GUMSXACMLAuthorizationServicePort&#34</a>; local_user=ilc â€¦
PID&#58; 26163 -- PRIMA INFO ts=2012-10-23T14&#58;13&#58;38-06&#58;00 event=org.osg.prima.authz.start DN=&#34;/C=JP/O=KEK/OU=CRC/CN=TIAN Junping&#34; FQAN=&#34;/ilc/Role=NULL/Capability=NULL&#34; FQAN_Issuer=&#34;/C=DE/O=GermanGrid/OU=DESY/CN=host/grid-voms.desy.de&#34; Service_URL=&#34;<a href='https&#58;//gums.hpcc.ttu.edu&#58;8443/gums/services/GUMSXACMLAuthorizationServicePort&#34' target='_blank' rel='nofollow'>https&#58;//gums.hpcc.ttu.edu&#58;8443/gums/services/GUMSXACMLAuthorizationServicePort&#34</a>;

</div><script type='text/javascript'>
        $('#show_1985435202').click(function() {
            $('#detail_1985435202').slideDown("normal");
            $('#show_1985435202').hide();
            $('#hide_1985435202').show();
        });
        $('#hide_1985435202').click(function() {
            $('#detail_1985435202').slideUp();
            $('#hide_1985435202').hide();
            $('#show_1985435202').show();
        });
        </script></pre></div><div class='update_description'><i onclick="document.location='12876#1351720156'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2012-10-31T21:49:16+00:00">Oct 31, 2012 09:49 PM UTC</time><a class="anchor" name="1351720156">&nbsp;</a></div><pre>Hi,

I think this is the relevant ticket&#58;

<a href='https&#58;//savannah.cern.ch/bugs/?44599' target='_blank' rel='nofollow'>https&#58;//savannah.cern.ch/bugs/?44599</a>

It seems the OSG default (999999) is in fact lower than the EMI default (999999999)!

I think we&#39;re looking at the wrong thing here!  The GlueCEStateEstimatedResponseTime is probably what you want to pay attention to.  If there are many jobs in queue, that number should get increasingly large.

*THAT SAID* - CMS investigation has shown that using GlueCEStateEstimatedResponseTime (the EMI WMS default) is provably worse than randomly distributing jobs.

Brian

by /DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=People/CN=Brian Bockelman</pre></div><div class='update_description'><i onclick="document.location='12876#1351719883'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2012-10-31T21:44:43+00:00">Oct 31, 2012 09:44 PM UTC</time><a class="anchor" name="1351719883">&nbsp;</a></div><pre>Brian&#58; interesting; but if I check this for our cluster, I see that GlueCEStateWorstResponseTime and GlueCEStateEstimatedResponseTime return the same value (3600) for all of our advertised queues, even though that cluster is right now pretty full.

Is there something else we should check?

Also, it would be nice to get a response on this point from the ILC folks on this ticket, as well as one to our query about what caused their jobs to fail in the first place.  We&#39;re happy to fix anything that we have set amiss!

Alan

by /DC=org/DC=doegrids/OU=People/CN=Alan Sill 503049</pre></div><div class='update_description'><i onclick="document.location='12876#1351719612'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2012-10-31T21:40:12+00:00">Oct 31, 2012 09:40 PM UTC</time><a class="anchor" name="1351719612">&nbsp;</a></div><pre>Yeah - at some point, GLUE folks were advising folks to have &#34;999999&#34; to mean &#34;value not set&#34;.  It was only half done, and apparently not respected everywhere.

Do we understand why Auger and ILC run like this while other gliteWMS users dont?  It&#39;s an incorrect use of GLUE.

Something I&#39;d rather try - use delete-attributes.conf to just nuke this line altogether.

by /DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=People/CN=Brian Bockelman</pre></div><div class='update_description'><i onclick="document.location='12876#1351719298'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2012-10-31T21:34:58+00:00">Oct 31, 2012 09:34 PM UTC</time> by <b>OSG-GOC</b><a class="anchor" name="1351719298">&nbsp;</a></div><pre>reply from&#58; timm@....

The &#34;Auger&#34; VO from EGI uses the same gliteWMS and has run into
the same problem.  If I recall my GIP history, the 1000000 is not
actually a &#34;bug&#34;, it was a feature that was actually set high on purpose
but I do not remember why?  Is there any reason it has to stay high?
If we find the one place in the code that has the 1000000 in it, can
we just change it to a lower value?

Steve Timm

On Wed, 31 Oct 2012, Fermilab Service Desk wrote&#58;

<font color='#7F7E6F'>&#62; Incident INC000000327794 comments have been updated.</font>
<font color='#7F7E6F'>&#62; Â </font>
<font color='#7F7E6F'>&#62; Last update&#58;</font>
<font color='#7F7E6F'>&#62; 2012-10-31 21&#58;24&#58;11 - cd-srv-goc-ops (Additional comments)</font>
<font color='#7F7E6F'>&#62; Hi Alan,Â </font>
<font color='#7F7E6F'>&#62; While we can fix the bug, the ILC folks are using GLUE incorrectly.Â </font>
<font color='#7F7E6F'>&#62; The &#34;Max&#34; items indicate the number of jobs the batch system can possibly</font>
<font color='#7F7E6F'>&#62; have, not the suggested running or queued jobs for the CE. CMS has in the</font>
<font color='#7F7E6F'>&#62; past sorted by&#58;Â </font>
<font color='#7F7E6F'>&#62; GlueCEStateEstimatedResponseTimeÂ </font>
<font color='#7F7E6F'>&#62; which EGI WMS ranks on by default. Not sure if they still do that.Â </font>
<font color='#7F7E6F'>&#62; BrianÂ </font>
<font color='#7F7E6F'>&#62; by /DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=People/CN=Brian Bockelman</font>
<font color='#7F7E6F'>&#62; -- by goc at Wed Oct 31 21&#58;23&#58;53 UTC 2012Â </font>
<font color='#7F7E6F'>&#62; Â </font>
<font color='#7F7E6F'>&#62;</font>
<font color='#7F7E6F'>&#62; ____________________________________________________________________________</font>
<font color='#7F7E6F'>&#62; Incident State&#58; Work in Progress</font>
<font color='#7F7E6F'>&#62; Caller&#58; cd-srv-goc-ops</font>
<font color='#7F7E6F'>&#62; Assignment Group&#58; International Linear Collider VO</font>
<font color='#7F7E6F'>&#62; Assigned To&#58; Steven Timm</font>
<font color='#7F7E6F'>&#62; Summary&#58;</font>
<div id='show_459541680' class=''><button class="btn">Show More</button></div><div class='detail hidden' id='detail_459541680'><font color='#7F7E6F'>&#62; [by Alan Sill]</font>
<font color='#7F7E6F'>&#62; Â </font>
<font color='#7F7E6F'>&#62; Our cluster is receiving a very high rate of globus-job-manager jobs from</font>
<font color='#7F7E6F'>&#62; the ilc VO, leading to a load over 1000 on the gatekeeper.</font>
<font color='#7F7E6F'>&#62; Â </font>
<font color='#7F7E6F'>&#62; I have tried shutting off grid services (we are still using OSG 1.2 on this</font>
<font color='#7F7E6F'>&#62; resource) and terminating all ilc processes, then removing ilc from the list</font>
<font color='#7F7E6F'>&#62; of supported VOs in the $DT_LOCATION/monitoring/osg-supported-vo-list.txt</font>
<font color='#7F7E6F'>&#62; file, then re-running the osg-configure -c step, and also removing all</font>
<font color='#7F7E6F'>&#62; gram_scratch files from the ilc user home area, b u these jobs seem to</font>
<font color='#7F7E6F'>&#62; re-appear after we turn VDT services back on.</font>
<font color='#7F7E6F'>&#62; Â </font>
<font color='#7F7E6F'>&#62; Can you please&#58;</font>
<font color='#7F7E6F'>&#62; Â </font>
<font color='#7F7E6F'>&#62; 1) Contact the ILC VO, especially the users below, to ask them to look into</font>
<font color='#7F7E6F'>&#62; their workflow,</font>
<font color='#7F7E6F'>&#62; Â </font>
<font color='#7F7E6F'>&#62; 2) Please advise how we can remove ILC from the locally supported VOs until</font>
<font color='#7F7E6F'>&#62; this is done/</font>
<font color='#7F7E6F'>&#62; Â </font>
<font color='#7F7E6F'>&#62; I may have to block the ILC at the GUMS level to prevent our CE from being</font>
<font color='#7F7E6F'>&#62; overloaded in the meantime.</font>
<font color='#7F7E6F'>&#62; Â </font>
<font color='#7F7E6F'>&#62; Thanks,</font>
<font color='#7F7E6F'>&#62; Alan</font>
<font color='#7F7E6F'>&#62; Â </font>
<font color='#7F7E6F'>&#62; PID&#58; 25783 -- PRIMA INFO ts=2012-10-23T14&#58;13&#58;38-06&#58;00</font>
<font color='#7F7E6F'>&#62; event=org.osg.prima.authz.end status=0 decision=PERMIT</font>
<font color='#7F7E6F'>&#62; DN=&#34;/DC=ch/DC=cern/OU=Organic Units/OU=Users/CN=sposs/CN=641989/CN=Stephane</font>
<font color='#7F7E6F'>&#62; Guillaume Poss&#34; FQAN=&#34;/ilc/Role=production/Capability=NULL&#34;</font>
<font color='#7F7E6F'>&#62; FQAN_Issuer=&#34;/C=DE/O=GermanGrid/OU=DESY/CN=host/grid-voms.desy.de&#34;Service_URL=&#34;<a href='https&#58;//gums.hpcc.ttu.edu&#58;8443/gums/services/GUMSXACMLAuthoriz' target='_blank' rel='nofollow'>https&#58;//gums.hpcc.ttu.edu&#58;8443/gums/services/GUMSXACMLAuthoriz</a></font>
<font color='#7F7E6F'>&#62; ationServicePort&#34; local_user=ilc â€¦</font>
<font color='#7F7E6F'>&#62; PID&#58; 26163 -- PRIMA INFO ts=2012-10-23T14&#58;13&#58;38-06&#58;00</font>
<font color='#7F7E6F'>&#62; event=org.osg.prima.authz.start DN=&#34;/C=JP/O=KEK/OU=CRC/CN=TIAN Junping&#34;</font>
<font color='#7F7E6F'>&#62; FQAN=&#34;/ilc/Role=NULL/Capability=NULL&#34;</font>
<font color='#7F7E6F'>&#62; FQAN_Issuer=&#34;/C=DE/O=GermanGrid/OU=DESY/CN=host/grid-voms.desy.de&#34;Service_URL=&#34;<a href='https&#58;//gums.hpcc.ttu.edu&#58;8443/gums/services/GUMSXACMLAuthoriz' target='_blank' rel='nofollow'>https&#58;//gums.hpcc.ttu.edu&#58;8443/gums/services/GUMSXACMLAuthoriz</a></font>
<font color='#7F7E6F'>&#62; ationServicePort&#34;</font>
<font color='#7F7E6F'>&#62; Â </font>
<font color='#7F7E6F'>&#62; [META Information]</font>
<font color='#7F7E6F'>&#62; VO on which user is having this issue&#58; ILC(20)</font>
<font color='#7F7E6F'>&#62; Primary Admin for ILC VO is Fermilab Service Desk and has been CC&#39;d.</font>
<font color='#7F7E6F'>&#62; Â </font>
<font color='#7F7E6F'>&#62; [Ticket Origin]</font>
<font color='#7F7E6F'>&#62; <a href='https&#58;//ticket.grid.iu.edu/goc/viewer?id=12876#1351087485' target='_blank' rel='nofollow'>https&#58;//ticket.grid.iu.edu/goc/viewer?id=12876#1351087485</a></font>
<font color='#7F7E6F'>&#62; Â </font>
<font color='#7F7E6F'>&#62;</font>
<font color='#7F7E6F'>&#62; ____________________________________________________________________________</font>
<font color='#7F7E6F'>&#62; Â </font>
<font color='#7F7E6F'>&#62; Ref&#58;MSG0677485</font>
<font color='#7F7E6F'>&#62;</font>
<font color='#7F7E6F'>&#62;</font>

------------------------------------------------------------------
Steven C. Timm, Ph.D  (630) 840-8525
timm@....  <a href='http&#58;//home.fnal.gov/' target='_blank' rel='nofollow'>http&#58;//home.fnal.gov/</a>~timm/
Fermilab Computing Division, Scientific Computing Facilities,
Grid Facilities Department, FermiGrid Services Group, Group Leader.
Lead of FermiCloud project.
-- by timm at Wed Oct 31 21&#58;33&#58;34 UTC 2012

GOCTX Source&#58; <a href='https&#58;//fermi.service-now.com/nav_to.do?uri=incident.do?sys_id=INC000000327794' target='_blank' rel='nofollow'>https&#58;//fermi.service-now.com/nav_to.do?uri=incident.do?sys_id=INC000000327794</a>
</div><script type='text/javascript'>
        $('#show_459541680').click(function() {
            $('#detail_459541680').slideDown("normal");
            $('#show_459541680').hide();
            $('#hide_459541680').show();
        });
        $('#hide_459541680').click(function() {
            $('#detail_459541680').slideUp();
            $('#hide_459541680').hide();
            $('#show_459541680').show();
        });
        </script></pre></div><div class='update_description'><i onclick="document.location='12876#1351718633'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2012-10-31T21:23:53+00:00">Oct 31, 2012 09:23 PM UTC</time><a class="anchor" name="1351718633">&nbsp;</a></div><pre>Hi Alan,

While we can fix the bug, the ILC folks are using GLUE incorrectly.

The &#34;Max&#34; items indicate the number of jobs the batch system can possibly have, not the suggested running or queued jobs for the CE.  CMS has in the past sorted by&#58;

GlueCEStateEstimatedResponseTime

which EGI WMS ranks on by default.  Not sure if they still do that.

Brian

by /DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=People/CN=Brian Bockelman</pre></div><div class='update_description'><i onclick="document.location='12876#1351718001'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2012-10-31T21:13:21+00:00">Oct 31, 2012 09:13 PM UTC</time> by <b>echism</b><a class="anchor" name="1351718001">&nbsp;</a></div><pre>GIP Team,

Do you have any knowledge of this bug?

&#34;There seems to be a bug in the configuration that has added 1000000 to the otherwise sensible numbers for maximum numbers of jobs in our various queues.  See below (blank lines added between queue jobmangers for clarity), and please compare GlueCEPolicyMaxTotalJobs to GlueCEPolicyMaxRunningJobs for each queue.

If time allows, I will try to follow Steve&#39;s suggestion to put in an alter-attributes.conf in our $VDT_LOCATION/gip/etc area to correct some of these parameters.  Once we do so, I assume that we should stop services and re-run configure-osg -c to pick the settings up, then re-enable services -- is this correct?  Or are such changes picked up automatically?

This could help explain why CMS jobs also seem to load our gatekeeper heavily from time to time, assuming they check similar parameters.

Can you also advise how to set GlueCEPolicyMaxWaitingJobs for good balance with the number of running jobs (I assume a similar number to GlueCEPolicyMaxRunningJobs?  Or should this add up with GlueCEPolicyMaxRunningJobs to equal GlueCEPolicyMaxTotalJobs)?  And perhaps some advice on setting all of those various cpu- and running-tiime parameters would be helpful.

Alan

[root@antaeus etc]# ldapsearch -x -LLL -h lcg-bdii.cern.ch -p 2170 -b mds-vo-name=local,o=grid &#39;(&(objectclass=gluece)(glueceuniqueid=*.ttu.*))&#39; | grep &#34;jobmanager&#92;|GlueCEPolicyMax&#34;

dn&#58; GlueCEUniqueID=antaeus.hpcc.ttu.edu&#58;2119/jobmanager-sge-cms,Mds-Vo-name=TT
GlueCEUniqueID&#58; antaeus.hpcc.ttu.edu&#58;2119/jobmanager-sge-cms
GlueCEPolicyMaxTotalJobs&#58; 1000335
GlueCEPolicyMaxSlotsPerJob&#58; 1
GlueCEPolicyMaxWaitingJobs&#58; 999999
GlueCEPolicyMaxCPUTime&#58; 9999999
GlueCEInfoContactString&#58; antaeus.hpcc.ttu.edu/jobmanager-sge-cms
GlueCEPolicyMaxObtainableWallClockTime&#58; 9999999
GlueCEPolicyMaxRunningJobs&#58; 336
GlueCEPolicyMaxWallClockTime&#58; 9999999
GlueCEPolicyMaxObtainableCPUTime&#58; 9999999

dn&#58; GlueCEUniqueID=antaeus.hpcc.ttu.edu&#58;2119/jobmanager-sge-serial,Mds-Vo-name
GlueCEUniqueID&#58; antaeus.hpcc.ttu.edu&#58;2119/jobmanager-sge-serial
GlueCEPolicyMaxTotalJobs&#58; 1000431
GlueCEPolicyMaxSlotsPerJob&#58; 1
GlueCEPolicyMaxWaitingJobs&#58; 999999
GlueCEPolicyMaxCPUTime&#58; 9999999
GlueCEInfoContactString&#58; antaeus.hpcc.ttu.edu/jobmanager-sge-serial
<div id='show_2126519227' class=''><button class="btn">Show More</button></div><div class='detail hidden' id='detail_2126519227'>GlueCEPolicyMaxObtainableWallClockTime&#58; 9999999
GlueCEPolicyMaxRunningJobs&#58; 432
GlueCEPolicyMaxWallClockTime&#58; 9999999
GlueCEPolicyMaxObtainableCPUTime&#58; 9999999

dn&#58; GlueCEUniqueID=antaeus.hpcc.ttu.edu&#58;2119/jobmanager-sge-suragrid,Mds-Vo-na
GlueCEUniqueID&#58; antaeus.hpcc.ttu.edu&#58;2119/jobmanager-sge-suragrid
GlueCEPolicyMaxTotalJobs&#58; 1000079
GlueCEPolicyMaxSlotsPerJob&#58; 1
GlueCEPolicyMaxWaitingJobs&#58; 999999
GlueCEPolicyMaxCPUTime&#58; 9999999
GlueCEInfoContactString&#58; antaeus.hpcc.ttu.edu/jobmanager-sge-suragrid
GlueCEPolicyMaxObtainableWallClockTime&#58; 9999999
GlueCEPolicyMaxRunningJobs&#58; 80
GlueCEPolicyMaxWallClockTime&#58; 9999999
GlueCEPolicyMaxObtainableCPUTime&#58; 9999999&#34;

Thank you,
Elizabeth
</div><script type='text/javascript'>
        $('#show_2126519227').click(function() {
            $('#detail_2126519227').slideDown("normal");
            $('#show_2126519227').hide();
            $('#hide_2126519227').show();
        });
        $('#hide_2126519227').click(function() {
            $('#detail_2126519227').slideUp();
            $('#hide_2126519227').hide();
            $('#show_2126519227').show();
        });
        </script></pre></div><div class='update_description'><i onclick="document.location='12876#1351538893'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2012-10-29T19:28:13+00:00">Oct 29, 2012 07:28 PM UTC</time><a class="anchor" name="1351538893">&nbsp;</a></div><pre>Last Friday we updated our GLUE data along the lines of Steve Timm&#39;s suggestions, with an alter-attributes.conf file.  While this is a bit hand-wired right now (it won&#39;t track changes to the queue setups automatically, for example), it does remove the &#34;real number plus a million&#34; bug that was apparently in the GIP automatic setup results.

Can the ILC folks try a (small!) number of jobs and let us know what else might need to be fixed, please?

Alan

by /DC=org/DC=doegrids/OU=People/CN=Alan Sill 503049</pre></div><div class='update_description'><i onclick="document.location='12876#1351293108'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2012-10-26T23:11:48+00:00">Oct 26, 2012 11:11 PM UTC</time><a class="anchor" name="1351293108">&nbsp;</a></div><pre>To Stephane&#58;

Thanks for the GLUE info.  I think this is a bug for OSG folks to help us trace, see below.

Separately, we would be very interested to know why your software did not install and run on our CE, which is used quite successfully by many other VOs.  If you would be so kind, please advise so that we can look into the problem(s) that you encountered and fix them.

To OSG&#58;

There seems to be a bug in the configuration that has added 1000000 to the otherwise sensible numbers for maximum numbers of jobs in our various queues.  See below (blank lines added between queue jobmangers for clarity), and please compare GlueCEPolicyMaxTotalJobs to GlueCEPolicyMaxRunningJobs for each queue.

If time allows, I will try to follow Steve&#39;s suggestion to put in an alter-attributes.conf in our $VDT_LOCATION/gip/etc area to correct some of these parameters.  Once we do so, I assume that we should stop services and re-run configure-osg -c to pick the settings up, then re-enable services -- is this correct?  Or are such changes picked up automatically?

This could help explain why CMS jobs also seem to load our gatekeeper heavily from time to time, assuming they check similar parameters.

Can you also advise how to set GlueCEPolicyMaxWaitingJobs for good balance with the number of running jobs (I assume a similar number to GlueCEPolicyMaxRunningJobs?  Or should this add up with GlueCEPolicyMaxRunningJobs to equal GlueCEPolicyMaxTotalJobs)?  And perhaps some advice on setting all of those various cpu- and running-tiime parameters would be helpful.

Alan

[root@antaeus etc]# ldapsearch -x -LLL -h lcg-bdii.cern.ch -p 2170 -b mds-vo-name=local,o=grid &#39;(&(objectclass=gluece)(glueceuniqueid=*.ttu.*))&#39; | grep &#34;jobmanager&#92;|GlueCEPolicyMax&#34;

dn&#58; GlueCEUniqueID=antaeus.hpcc.ttu.edu&#58;2119/jobmanager-sge-cms,Mds-Vo-name=TT
GlueCEUniqueID&#58; antaeus.hpcc.ttu.edu&#58;2119/jobmanager-sge-cms
GlueCEPolicyMaxTotalJobs&#58; 1000335
GlueCEPolicyMaxSlotsPerJob&#58; 1
GlueCEPolicyMaxWaitingJobs&#58; 999999
GlueCEPolicyMaxCPUTime&#58; 9999999
GlueCEInfoContactString&#58; antaeus.hpcc.ttu.edu/jobmanager-sge-cms
GlueCEPolicyMaxObtainableWallClockTime&#58; 9999999
GlueCEPolicyMaxRunningJobs&#58; 336
GlueCEPolicyMaxWallClockTime&#58; 9999999
GlueCEPolicyMaxObtainableCPUTime&#58; 9999999

dn&#58; GlueCEUniqueID=antaeus.hpcc.ttu.edu&#58;2119/jobmanager-sge-serial,Mds-Vo-name
GlueCEUniqueID&#58; antaeus.hpcc.ttu.edu&#58;2119/jobmanager-sge-serial
GlueCEPolicyMaxTotalJobs&#58; 1000431
<div id='show_264805116' class=''><button class="btn">Show More</button></div><div class='detail hidden' id='detail_264805116'>GlueCEPolicyMaxSlotsPerJob&#58; 1
GlueCEPolicyMaxWaitingJobs&#58; 999999
GlueCEPolicyMaxCPUTime&#58; 9999999
GlueCEInfoContactString&#58; antaeus.hpcc.ttu.edu/jobmanager-sge-serial
GlueCEPolicyMaxObtainableWallClockTime&#58; 9999999
GlueCEPolicyMaxRunningJobs&#58; 432
GlueCEPolicyMaxWallClockTime&#58; 9999999
GlueCEPolicyMaxObtainableCPUTime&#58; 9999999

dn&#58; GlueCEUniqueID=antaeus.hpcc.ttu.edu&#58;2119/jobmanager-sge-suragrid,Mds-Vo-na
GlueCEUniqueID&#58; antaeus.hpcc.ttu.edu&#58;2119/jobmanager-sge-suragrid
GlueCEPolicyMaxTotalJobs&#58; 1000079
GlueCEPolicyMaxSlotsPerJob&#58; 1
GlueCEPolicyMaxWaitingJobs&#58; 999999
GlueCEPolicyMaxCPUTime&#58; 9999999
GlueCEInfoContactString&#58; antaeus.hpcc.ttu.edu/jobmanager-sge-suragrid
GlueCEPolicyMaxObtainableWallClockTime&#58; 9999999
GlueCEPolicyMaxRunningJobs&#58; 80
GlueCEPolicyMaxWallClockTime&#58; 9999999
GlueCEPolicyMaxObtainableCPUTime&#58; 9999999

by /DC=org/DC=doegrids/OU=People/CN=Alan Sill 503049
</div><script type='text/javascript'>
        $('#show_264805116').click(function() {
            $('#detail_264805116').slideDown("normal");
            $('#show_264805116').hide();
            $('#hide_264805116').show();
        });
        $('#hide_264805116').click(function() {
            $('#detail_264805116').slideUp();
            $('#hide_264805116').hide();
            $('#show_264805116').show();
        });
        </script></pre></div><div class='update_description'><i onclick="document.location='12876#1351278874'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2012-10-26T19:14:34+00:00">Oct 26, 2012 07:14 PM UTC</time> by <b>OSG-GOC</b><a class="anchor" name="1351278874">&nbsp;</a></div><pre>This is what we did at FermiGrid

Added the following to $VDT_LOCATION/gip/etc/alter-attributes.conf

cat alter-attributes.conf

dn&#58; GlueCEUniqueID=fermigridosg1.fnal.gov&#58;2119/jobmanager-condor-default,mds-vo-name=local,o=grid
GlueCEPolicyMaxTotalJobs&#58; 5000
GlueCEPolicyMaxWaitingJobs&#58; 5000

=-------------------

If your jobmanager is something other than than jobmanager-condor-default then you will have to modify that part accordingly.

I have raised the issue of the high values of various values in the OSG GIP to the GIP maintainers and they may fix them eventually but
we are not expecting a fast fix.  Auger VO has seen a similar trouble, as have superb and belle VO&#39;s.

Steve Timm

-- by timm at Fri Oct 26 19&#58;13&#58;50 UTC 2012

GOCTX Source&#58; <a href='https&#58;//fermi.service-now.com/nav_to.do?uri=incident.do?sys_id=INC000000327794' target='_blank' rel='nofollow'>https&#58;//fermi.service-now.com/nav_to.do?uri=incident.do?sys_id=INC000000327794</a></pre></div><div class='update_description'><i onclick="document.location='12876#1351260220'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2012-10-26T14:03:40+00:00">Oct 26, 2012 02:03 PM UTC</time><a class="anchor" name="1351260220">&nbsp;</a></div><pre>I don&#39;t think OSG uses these parameters.  GOC, can you advise a remediation?

by /DC=org/DC=doegrids/OU=People/CN=Alan Sill 503049</pre></div><div class='update_description'><i onclick="document.location='12876#1351235419'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2012-10-26T07:10:19+00:00">Oct 26, 2012 07:10 AM UTC</time> by <b>OSG-GOC</b><a class="anchor" name="1351235419">&nbsp;</a></div><pre>reply from&#58; stephane.guillaume.poss@....

Hi,
The reason you got so many jobs from ILC is because of
GlueCEPolicyMaxTotalJobs&#58; 1000431
GlueCEPolicyMaxWaitingJobs&#58; 999999
GlueCEPolicyMaxCPUTime&#58; 9999999
GlueCEPolicyMaxObtainableCPUTime&#58; 9999999
GlueCEPolicyMaxObtainableWallClockTime&#58; 9999999

in particular the GlueCEPolicyMaxWaitingJobs&#58; 999999

That&#39;s what I thought, what is advertised is not correct. gLite assumes
it can swamp your site with up to 999999 jobs, a bit too much for you I
guess. You might also check if the GlueCEPolicyMaxTotalJobs&#58; 1000431 is
correct, as well as the other strange values...

Thanks,.
S Poss

P.S.&#58; To get this, I ran the following&#58;
ldapsearch -x -LLL -h lcg-bdii.cern.ch -p 2170 -b
mds-vo-name=local,o=grid &#39;(&(objectclass=gluece)(glueceuniqueid=*.ttu.*))&#39;

On 10/25/2012 11&#58;55 PM, Fermilab Service Desk wrote&#58;
<font color='#7F7E6F'>&#62; Incident INC000000327794</font>
<font color='#7F7E6F'>&#62;</font>
<font color='#7F7E6F'>&#62; comments have been updated.</font>
<font color='#7F7E6F'>&#62; *Last update&#58;*</font>
<font color='#7F7E6F'>&#62; 2012-10-25 21&#58;54&#58;10 - cd-srv-goc-ops (Additional comments)</font>
<font color='#7F7E6F'>&#62; As far as we know, the advertised information from our site is correct.</font>
<font color='#7F7E6F'>&#62; The errors you say you encountered may affect other VOs. Please let us</font>
<font color='#7F7E6F'>&#62; know what these were, and we will try to clear them up.</font>
<font color='#7F7E6F'>&#62; by /DC=org/DC=doegrids/OU=People/CN=Alan Sill 503049</font>
<font color='#7F7E6F'>&#62; -- by goc at Thu Oct 25 21&#58;53&#58;07 UTC 2012</font>
<div id='show_187566143' class=''><button class="btn">Show More</button></div><div class='detail hidden' id='detail_187566143'><font color='#7F7E6F'>&#62; ------------------------------------------------------------------------</font>
<font color='#7F7E6F'>&#62; *Incident State&#58;* Work in Progress</font>
<font color='#7F7E6F'>&#62; *Caller&#58;* cd-srv-goc-ops</font>
<font color='#7F7E6F'>&#62; *Assignment Group&#58;* International Linear Collider VO</font>
<font color='#7F7E6F'>&#62; *Assigned To&#58;* Steven Timm</font>
<font color='#7F7E6F'>&#62; *Summary&#58;*</font>
<font color='#7F7E6F'>&#62; [by Alan Sill]</font>
<font color='#7F7E6F'>&#62; Our cluster is receiving a very high rate of globus-job-manager jobs</font>
<font color='#7F7E6F'>&#62; from the ilc VO, leading to a load over 1000 on the gatekeeper.</font>
<font color='#7F7E6F'>&#62; I have tried shutting off grid services (we are still using OSG 1.2 on</font>
<font color='#7F7E6F'>&#62; this resource) and terminating all ilc processes, then removing ilc from</font>
<font color='#7F7E6F'>&#62; the list of supported VOs in the</font>
<font color='#7F7E6F'>&#62; $DT_LOCATION/monitoring/osg-supported-vo-list.txt file, then re-running</font>
<font color='#7F7E6F'>&#62; the osg-configure -c step, and also removing all gram_scratch files from</font>
<font color='#7F7E6F'>&#62; the ilc user home area, b u these jobs seem to re-appear after we turn</font>
<font color='#7F7E6F'>&#62; VDT services back on.</font>
<font color='#7F7E6F'>&#62; Can you please&#58;</font>
<font color='#7F7E6F'>&#62; 1) Contact the ILC VO, especially the users below, to ask them to look</font>
<font color='#7F7E6F'>&#62; into their workflow,</font>
<font color='#7F7E6F'>&#62; 2) Please advise how we can remove ILC from the locally supported VOs</font>
<font color='#7F7E6F'>&#62; until this is done/</font>
<font color='#7F7E6F'>&#62; I may have to block the ILC at the GUMS level to prevent our CE from</font>
<font color='#7F7E6F'>&#62; being overloaded in the meantime.</font>
<font color='#7F7E6F'>&#62; Thanks,</font>
<font color='#7F7E6F'>&#62; Alan</font>
<font color='#7F7E6F'>&#62; PID&#58; 25783 -- PRIMA INFO ts=2012-10-23T14&#58;13&#58;38-06&#58;00</font>
<font color='#7F7E6F'>&#62; event=org.osg.prima.authz.end status=0 decision=PERMIT</font>
<font color='#7F7E6F'>&#62; DN=&#34;/DC=ch/DC=cern/OU=Organic</font>
<font color='#7F7E6F'>&#62; Units/OU=Users/CN=sposs/CN=641989/CN=Stephane Guillaume Poss&#34;</font>
<font color='#7F7E6F'>&#62; FQAN=&#34;/ilc/Role=production/Capability=NULL&#34;</font>
<font color='#7F7E6F'>&#62; FQAN_Issuer=&#34;/C=DE/O=GermanGrid/OU=DESY/CN=host/grid-voms.desy.de&#34;</font>
<font color='#7F7E6F'>&#62; Service_URL=&#34;<a href='https&#58;//gums.hpcc.ttu.edu&#58;8443/gums/services/GUMSXACMLAuthorizationServicePort&#34' target='_blank' rel='nofollow'>https&#58;//gums.hpcc.ttu.edu&#58;8443/gums/services/GUMSXACMLAuthorizationServicePort&#34</a>;</font>
<font color='#7F7E6F'>&#62; local_user=ilc â€¦</font>
<font color='#7F7E6F'>&#62; PID&#58; 26163 -- PRIMA INFO ts=2012-10-23T14&#58;13&#58;38-06&#58;00</font>
<font color='#7F7E6F'>&#62; event=org.osg.prima.authz.start DN=&#34;/C=JP/O=KEK/OU=CRC/CN=TIAN Junping&#34;</font>
<font color='#7F7E6F'>&#62; FQAN=&#34;/ilc/Role=NULL/Capability=NULL&#34;</font>
<font color='#7F7E6F'>&#62; FQAN_Issuer=&#34;/C=DE/O=GermanGrid/OU=DESY/CN=host/grid-voms.desy.de&#34;</font>
<font color='#7F7E6F'>&#62; Service_URL=&#34;<a href='https&#58;//gums.hpcc.ttu.edu&#58;8443/gums/services/GUMSXACMLAuthorizationServicePort&#34' target='_blank' rel='nofollow'>https&#58;//gums.hpcc.ttu.edu&#58;8443/gums/services/GUMSXACMLAuthorizationServicePort&#34</a>;</font>
<font color='#7F7E6F'>&#62; [META Information]</font>
<font color='#7F7E6F'>&#62; VO on which user is having this issue&#58; ILC(20)</font>
<font color='#7F7E6F'>&#62; Primary Admin for ILC VO is Fermilab Service Desk and has been CC&#39;d.</font>
<font color='#7F7E6F'>&#62; [Ticket Origin]</font>
<font color='#7F7E6F'>&#62; <a href='https&#58;//ticket.grid.iu.edu/goc/viewer?id=12876#1351087485' target='_blank' rel='nofollow'>https&#58;//ticket.grid.iu.edu/goc/viewer?id=12876#1351087485</a></font>
<font color='#7F7E6F'>&#62; ------------------------------------------------------------------------</font>
<font color='#7F7E6F'>&#62; Ref&#58;MSG0668949</font>

-- by guest at Fri Oct 26 07&#58;09&#58;05 UTC 2012

GOCTX Source&#58; <a href='https&#58;//fermi.service-now.com/nav_to.do?uri=incident.do?sys_id=INC000000327794' target='_blank' rel='nofollow'>https&#58;//fermi.service-now.com/nav_to.do?uri=incident.do?sys_id=INC000000327794</a>
</div><script type='text/javascript'>
        $('#show_187566143').click(function() {
            $('#detail_187566143').slideDown("normal");
            $('#show_187566143').hide();
            $('#hide_187566143').show();
        });
        $('#hide_187566143').click(function() {
            $('#detail_187566143').slideUp();
            $('#hide_187566143').hide();
            $('#show_187566143').show();
        });
        </script></pre></div><div class='update_description'><i onclick="document.location='12876#1351201987'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2012-10-25T21:53:07+00:00">Oct 25, 2012 09:53 PM UTC</time><a class="anchor" name="1351201987">&nbsp;</a></div><pre>As far as we know, the advertised information from our site is correct.

The errors you say you encountered may affect other VOs. Please let us know what these were, and we will try to clear them up.

by /DC=org/DC=doegrids/OU=People/CN=Alan Sill 503049</pre></div><div class='update_description'><i onclick="document.location='12876#1351201536'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2012-10-25T21:45:36+00:00">Oct 25, 2012 09:45 PM UTC</time> by <b>OSG-GOC</b><a class="anchor" name="1351201536">&nbsp;</a></div><pre>reply from&#58; stephane.guillaume.poss@....

Hi,
What happened is that we had problems getting our software to install
and start at your site, leading to many errors. Those errors led to many
more jobs being submitted. Now the fact that many ended up at your site
can be because what is advertised in the BDII is incorrect. The BDII is
used by gLite to target its jobs...

In any case, because the software does not run at your site, we removed
it from our mask.

Cheers,
S Poss

On 10/25/2012 10&#58;56 PM, Fermilab Service Desk wrote&#58;
<font color='#7F7E6F'>&#62; Incident INC000000327794</font>
<font color='#7F7E6F'>&#62;</font>
<font color='#7F7E6F'>&#62; comments have been updated.</font>
<font color='#7F7E6F'>&#62; *Last update&#58;*</font>
<font color='#7F7E6F'>&#62; 2012-10-25 20&#58;39&#58;11 - cd-srv-goc-ops (Additional comments)</font>
<font color='#7F7E6F'>&#62; It is not our intention to ban ILC jobs permanently. We only noticed</font>
<font color='#7F7E6F'>&#62; that the load on our gatekeeper node had risen to above 1100 (!) from</font>
<font color='#7F7E6F'>&#62; its normal value of a few to 10 or so, and that almost all of this load</font>
<font color='#7F7E6F'>&#62; was due to ILC jobs.</font>
<font color='#7F7E6F'>&#62; It would help if we were able to establish some cause-and-effect. Was</font>
<font color='#7F7E6F'>&#62; there some failure mode for jobs that caused them to fail and resubmit</font>
<font color='#7F7E6F'>&#62; rapidly, for example?</font>
<font color='#7F7E6F'>&#62; We are happy to include ILC as a supported VO if we can avoid having the</font>
<font color='#7F7E6F'>&#62; gatekeeper flooded.</font>
<font color='#7F7E6F'>&#62; by /DC=org/DC=doegrids/OU=People/CN=Alan Sill 503049</font>
<font color='#7F7E6F'>&#62; -- by goc at Thu Oct 25 20&#58;39&#58;00 UTC 2012</font>
<font color='#7F7E6F'>&#62; ------------------------------------------------------------------------</font>
<font color='#7F7E6F'>&#62; *Incident State&#58;* Work in Progress</font>
<font color='#7F7E6F'>&#62; *Caller&#58;* cd-srv-goc-ops</font>
<div id='show_544532593' class=''><button class="btn">Show More</button></div><div class='detail hidden' id='detail_544532593'><font color='#7F7E6F'>&#62; *Assignment Group&#58;* International Linear Collider VO</font>
<font color='#7F7E6F'>&#62; *Assigned To&#58;* Steven Timm</font>
<font color='#7F7E6F'>&#62; *Summary&#58;*</font>
<font color='#7F7E6F'>&#62; [by Alan Sill]</font>
<font color='#7F7E6F'>&#62; Our cluster is receiving a very high rate of globus-job-manager jobs</font>
<font color='#7F7E6F'>&#62; from the ilc VO, leading to a load over 1000 on the gatekeeper.</font>
<font color='#7F7E6F'>&#62; I have tried shutting off grid services (we are still using OSG 1.2 on</font>
<font color='#7F7E6F'>&#62; this resource) and terminating all ilc processes, then removing ilc from</font>
<font color='#7F7E6F'>&#62; the list of supported VOs in the</font>
<font color='#7F7E6F'>&#62; $DT_LOCATION/monitoring/osg-supported-vo-list.txt file, then re-running</font>
<font color='#7F7E6F'>&#62; the osg-configure -c step, and also removing all gram_scratch files from</font>
<font color='#7F7E6F'>&#62; the ilc user home area, b u these jobs seem to re-appear after we turn</font>
<font color='#7F7E6F'>&#62; VDT services back on.</font>
<font color='#7F7E6F'>&#62; Can you please&#58;</font>
<font color='#7F7E6F'>&#62; 1) Contact the ILC VO, especially the users below, to ask them to look</font>
<font color='#7F7E6F'>&#62; into their workflow,</font>
<font color='#7F7E6F'>&#62; 2) Please advise how we can remove ILC from the locally supported VOs</font>
<font color='#7F7E6F'>&#62; until this is done/</font>
<font color='#7F7E6F'>&#62; I may have to block the ILC at the GUMS level to prevent our CE from</font>
<font color='#7F7E6F'>&#62; being overloaded in the meantime.</font>
<font color='#7F7E6F'>&#62; Thanks,</font>
<font color='#7F7E6F'>&#62; Alan</font>
<font color='#7F7E6F'>&#62; PID&#58; 25783 -- PRIMA INFO ts=2012-10-23T14&#58;13&#58;38-06&#58;00</font>
<font color='#7F7E6F'>&#62; event=org.osg.prima.authz.end status=0 decision=PERMIT</font>
<font color='#7F7E6F'>&#62; DN=&#34;/DC=ch/DC=cern/OU=Organic</font>
<font color='#7F7E6F'>&#62; Units/OU=Users/CN=sposs/CN=641989/CN=Stephane Guillaume Poss&#34;</font>
<font color='#7F7E6F'>&#62; FQAN=&#34;/ilc/Role=production/Capability=NULL&#34;</font>
<font color='#7F7E6F'>&#62; FQAN_Issuer=&#34;/C=DE/O=GermanGrid/OU=DESY/CN=host/grid-voms.desy.de&#34;</font>
<font color='#7F7E6F'>&#62; Service_URL=&#34;<a href='https&#58;//gums.hpcc.ttu.edu&#58;8443/gums/services/GUMSXACMLAuthorizationServicePort&#34' target='_blank' rel='nofollow'>https&#58;//gums.hpcc.ttu.edu&#58;8443/gums/services/GUMSXACMLAuthorizationServicePort&#34</a>;</font>
<font color='#7F7E6F'>&#62; local_user=ilc â€¦</font>
<font color='#7F7E6F'>&#62; PID&#58; 26163 -- PRIMA INFO ts=2012-10-23T14&#58;13&#58;38-06&#58;00</font>
<font color='#7F7E6F'>&#62; event=org.osg.prima.authz.start DN=&#34;/C=JP/O=KEK/OU=CRC/CN=TIAN Junping&#34;</font>
<font color='#7F7E6F'>&#62; FQAN=&#34;/ilc/Role=NULL/Capability=NULL&#34;</font>
<font color='#7F7E6F'>&#62; FQAN_Issuer=&#34;/C=DE/O=GermanGrid/OU=DESY/CN=host/grid-voms.desy.de&#34;</font>
<font color='#7F7E6F'>&#62; Service_URL=&#34;<a href='https&#58;//gums.hpcc.ttu.edu&#58;8443/gums/services/GUMSXACMLAuthorizationServicePort&#34' target='_blank' rel='nofollow'>https&#58;//gums.hpcc.ttu.edu&#58;8443/gums/services/GUMSXACMLAuthorizationServicePort&#34</a>;</font>
<font color='#7F7E6F'>&#62; [META Information]</font>
<font color='#7F7E6F'>&#62; VO on which user is having this issue&#58; ILC(20)</font>
<font color='#7F7E6F'>&#62; Primary Admin for ILC VO is Fermilab Service Desk and has been CC&#39;d.</font>
<font color='#7F7E6F'>&#62; [Ticket Origin]</font>
<font color='#7F7E6F'>&#62; <a href='https&#58;//ticket.grid.iu.edu/goc/viewer?id=12876#1351087485' target='_blank' rel='nofollow'>https&#58;//ticket.grid.iu.edu/goc/viewer?id=12876#1351087485</a></font>
<font color='#7F7E6F'>&#62; ------------------------------------------------------------------------</font>
<font color='#7F7E6F'>&#62; Ref&#58;MSG0668803</font>

-- by guest at Thu Oct 25 21&#58;41&#58;31 UTC 2012

GOCTX Source&#58; <a href='https&#58;//fermi.service-now.com/nav_to.do?uri=incident.do?sys_id=INC000000327794' target='_blank' rel='nofollow'>https&#58;//fermi.service-now.com/nav_to.do?uri=incident.do?sys_id=INC000000327794</a>
</div><script type='text/javascript'>
        $('#show_544532593').click(function() {
            $('#detail_544532593').slideDown("normal");
            $('#show_544532593').hide();
            $('#hide_544532593').show();
        });
        $('#hide_544532593').click(function() {
            $('#detail_544532593').slideUp();
            $('#hide_544532593').hide();
            $('#show_544532593').show();
        });
        </script></pre></div><div class='update_description'><i onclick="document.location='12876#1351197540'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2012-10-25T20:39:00+00:00">Oct 25, 2012 08:39 PM UTC</time><a class="anchor" name="1351197540">&nbsp;</a></div><pre>It is not our intention to ban ILC jobs permanently.  We only noticed that the load on our gatekeeper node had risen to above 1100 (!) from its normal value of a few to 10 or so, and that almost all of this load was due to ILC jobs.

It would help if we were able to establish some cause-and-effect.  Was there some failure mode for jobs that caused them to fail and resubmit rapidly, for example?

We are happy to include ILC as a supported VO if we can avoid having the gatekeeper flooded.

by /DC=org/DC=doegrids/OU=People/CN=Alan Sill 503049</pre></div><div class='update_description'><i onclick="document.location='12876#1351184398'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2012-10-25T16:59:58+00:00">Oct 25, 2012 04:59 PM UTC</time> by <b>OSG-GOC</b><a class="anchor" name="1351184398">&nbsp;</a></div><pre>reply from&#58; stephane.guillaume.poss@....

Hi,
We are not targeting the sites directly, it&#39;s always through gLite. I
suspect the glue info is still different between what OSG reports and
what gLite sees.

In the mean time, we have removed TTU-ANTAEUS from our site mask (since
yesterday in fact).

Cheers,
S Poss

On 10/25/2012 06&#58;53 PM, Fermilab Service Desk wrote&#58;
<font color='#7F7E6F'>&#62; Incident INC000000327794</font>
<font color='#7F7E6F'>&#62;</font>
<font color='#7F7E6F'>&#62; comments have been updated.</font>
<font color='#7F7E6F'>&#62; *Last update&#58;*</font>
<font color='#7F7E6F'>&#62; 2012-10-25 11&#58;53&#58;04 - Steven Timm (Additional comments)</font>
<font color='#7F7E6F'>&#62; Dear ilc-vo-support@....</font>
<font color='#7F7E6F'>&#62; There has been a very high load at TTU-ANTAEUS site from ILC jobs, with</font>
<font color='#7F7E6F'>&#62; ILC users trying to run many more jobs than they</font>
<font color='#7F7E6F'>&#62; are allowed to run, and overwhelming the gatekeeper with a load of 1000.</font>
<font color='#7F7E6F'>&#62; the users were TIAN Junping and Stephane Guillaume Poss.</font>
<font color='#7F7E6F'>&#62; We need to get to the bottom of what is making the ILC system think that</font>
<font color='#7F7E6F'>&#62; OSG sites can take many more jobs than what they really can.</font>
<font color='#7F7E6F'>&#62; Steve Timm</font>
<font color='#7F7E6F'>&#62; ------------------------------------------------------------------------</font>
<font color='#7F7E6F'>&#62; *Incident State&#58;* Assigned</font>
<font color='#7F7E6F'>&#62; *Caller&#58;* cd-srv-goc-ops</font>
<font color='#7F7E6F'>&#62; *Assignment Group&#58;* International Linear Collider VO</font>
<font color='#7F7E6F'>&#62; *Assigned To&#58;* Steven Timm</font>
<font color='#7F7E6F'>&#62; *Summary&#58;*</font>
<font color='#7F7E6F'>&#62; [by Alan Sill]</font>
<font color='#7F7E6F'>&#62; Our cluster is receiving a very high rate of globus-job-manager jobs</font>
<div id='show_951374521' class=''><button class="btn">Show More</button></div><div class='detail hidden' id='detail_951374521'><font color='#7F7E6F'>&#62; from the ilc VO, leading to a load over 1000 on the gatekeeper.</font>
<font color='#7F7E6F'>&#62; I have tried shutting off grid services (we are still using OSG 1.2 on</font>
<font color='#7F7E6F'>&#62; this resource) and terminating all ilc processes, then removing ilc from</font>
<font color='#7F7E6F'>&#62; the list of supported VOs in the</font>
<font color='#7F7E6F'>&#62; $DT_LOCATION/monitoring/osg-supported-vo-list.txt file, then re-running</font>
<font color='#7F7E6F'>&#62; the osg-configure -c step, and also removing all gram_scratch files from</font>
<font color='#7F7E6F'>&#62; the ilc user home area, b u these jobs seem to re-appear after we turn</font>
<font color='#7F7E6F'>&#62; VDT services back on.</font>
<font color='#7F7E6F'>&#62; Can you please&#58;</font>
<font color='#7F7E6F'>&#62; 1) Contact the ILC VO, especially the users below, to ask them to look</font>
<font color='#7F7E6F'>&#62; into their workflow,</font>
<font color='#7F7E6F'>&#62; 2) Please advise how we can remove ILC from the locally supported VOs</font>
<font color='#7F7E6F'>&#62; until this is done/</font>
<font color='#7F7E6F'>&#62; I may have to block the ILC at the GUMS level to prevent our CE from</font>
<font color='#7F7E6F'>&#62; being overloaded in the meantime.</font>
<font color='#7F7E6F'>&#62; Thanks,</font>
<font color='#7F7E6F'>&#62; Alan</font>
<font color='#7F7E6F'>&#62; PID&#58; 25783 -- PRIMA INFO ts=2012-10-23T14&#58;13&#58;38-06&#58;00</font>
<font color='#7F7E6F'>&#62; event=org.osg.prima.authz.end status=0 decision=PERMIT</font>
<font color='#7F7E6F'>&#62; DN=&#34;/DC=ch/DC=cern/OU=Organic</font>
<font color='#7F7E6F'>&#62; Units/OU=Users/CN=sposs/CN=641989/CN=Stephane Guillaume Poss&#34;</font>
<font color='#7F7E6F'>&#62; FQAN=&#34;/ilc/Role=production/Capability=NULL&#34;</font>
<font color='#7F7E6F'>&#62; FQAN_Issuer=&#34;/C=DE/O=GermanGrid/OU=DESY/CN=host/grid-voms.desy.de&#34;</font>
<font color='#7F7E6F'>&#62; Service_URL=&#34;<a href='https&#58;//gums.hpcc.ttu.edu&#58;8443/gums/services/GUMSXACMLAuthorizationServicePort&#34' target='_blank' rel='nofollow'>https&#58;//gums.hpcc.ttu.edu&#58;8443/gums/services/GUMSXACMLAuthorizationServicePort&#34</a>;</font>
<font color='#7F7E6F'>&#62; local_user=ilc â€¦</font>
<font color='#7F7E6F'>&#62; PID&#58; 26163 -- PRIMA INFO ts=2012-10-23T14&#58;13&#58;38-06&#58;00</font>
<font color='#7F7E6F'>&#62; event=org.osg.prima.authz.start DN=&#34;/C=JP/O=KEK/OU=CRC/CN=TIAN Junping&#34;</font>
<font color='#7F7E6F'>&#62; FQAN=&#34;/ilc/Role=NULL/Capability=NULL&#34;</font>
<font color='#7F7E6F'>&#62; FQAN_Issuer=&#34;/C=DE/O=GermanGrid/OU=DESY/CN=host/grid-voms.desy.de&#34;</font>
<font color='#7F7E6F'>&#62; Service_URL=&#34;<a href='https&#58;//gums.hpcc.ttu.edu&#58;8443/gums/services/GUMSXACMLAuthorizationServicePort&#34' target='_blank' rel='nofollow'>https&#58;//gums.hpcc.ttu.edu&#58;8443/gums/services/GUMSXACMLAuthorizationServicePort&#34</a>;</font>
<font color='#7F7E6F'>&#62; [META Information]</font>
<font color='#7F7E6F'>&#62; VO on which user is having this issue&#58; ILC(20)</font>
<font color='#7F7E6F'>&#62; Primary Admin for ILC VO is Fermilab Service Desk and has been CC&#39;d.</font>
<font color='#7F7E6F'>&#62; [Ticket Origin]</font>
<font color='#7F7E6F'>&#62; <a href='https&#58;//ticket.grid.iu.edu/goc/viewer?id=12876#1351087485' target='_blank' rel='nofollow'>https&#58;//ticket.grid.iu.edu/goc/viewer?id=12876#1351087485</a></font>
<font color='#7F7E6F'>&#62; ------------------------------------------------------------------------</font>
<font color='#7F7E6F'>&#62; Ref&#58;MSG0668350</font>

-- by guest at Thu Oct 25 16&#58;58&#58;36 UTC 2012

GOCTX Source&#58; <a href='https&#58;//fermi.service-now.com/nav_to.do?uri=incident.do?sys_id=INC000000327794' target='_blank' rel='nofollow'>https&#58;//fermi.service-now.com/nav_to.do?uri=incident.do?sys_id=INC000000327794</a>
</div><script type='text/javascript'>
        $('#show_951374521').click(function() {
            $('#detail_951374521').slideDown("normal");
            $('#show_951374521').hide();
            $('#hide_951374521').show();
        });
        $('#hide_951374521').click(function() {
            $('#detail_951374521').slideUp();
            $('#hide_951374521').hide();
            $('#show_951374521').show();
        });
        </script></pre></div><div class='update_description'><i onclick="document.location='12876#1351184047'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2012-10-25T16:54:07+00:00">Oct 25, 2012 04:54 PM UTC</time> by <b>OSG-GOC</b><a class="anchor" name="1351184047">&nbsp;</a></div><pre>Dear ilc-vo-support@....

There has been a very high load at TTU-ANTAEUS site from ILC jobs, with ILC users trying to run  many more jobs than they
are allowed to run, and overwhelming the gatekeeper with a load of 1000.  the users were TIAN Junping and Stephane Guillaume Poss.

We need to get to the bottom of what is making the ILC system think that OSG sites can take many more jobs than what they really can.

Steve Timm

-- by timm at Thu Oct 25 16&#58;53&#58;04 UTC 2012

GOCTX Source&#58; <a href='https&#58;//fermi.service-now.com/nav_to.do?uri=incident.do?sys_id=INC000000327794' target='_blank' rel='nofollow'>https&#58;//fermi.service-now.com/nav_to.do?uri=incident.do?sys_id=INC000000327794</a></pre></div><div class='update_description'><i onclick="document.location='12876#1351183868'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2012-10-25T16:51:08+00:00">Oct 25, 2012 04:51 PM UTC</time> by <b>OSG-GOC</b><a class="anchor" name="1351183868">&nbsp;</a></div><pre>Alan--the supported VO list is generated by the gums_host_cron process and is based on the combination of the users you
have in GUMS and the unix UID&#39;s on your gatekeeper.  The list is regenerated every time that gums_host_cron runs, so
removing ilc from the supported VO list has no effect.

Our process when we get one of thse tickets is to forward it to the co-admins of the ILC VO at DESY,
ilc-vo-support@.... and also file a GGUS ticket.
There is some weird feature in the gLiteWMS which sometimes thinks a site can take many more jobs than it really can,
and that is what is happening here.

Steve Timm

-- by timm at Thu Oct 25 16&#58;50&#58;28 UTC 2012

GOCTX Source&#58; <a href='https&#58;//fermi.service-now.com/nav_to.do?uri=incident.do?sys_id=INC000000327794' target='_blank' rel='nofollow'>https&#58;//fermi.service-now.com/nav_to.do?uri=incident.do?sys_id=INC000000327794</a></pre></div><div class='update_description'><i onclick="document.location='12876#1351089205'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2012-10-24T14:33:25+00:00">Oct 24, 2012 02:33 PM UTC</time><a class="anchor" name="1351089205">&nbsp;</a></div><pre>Thanks for the GUMS banning document link.  This looks more or less as I would have thought and adds useful practical hints.

In terms of dropping support for an entire VO, is removing it from the osg-supported-vo-list.txt in OSG 1.2 sufficient, if followed by repeating the configure-osg -c step and restarting VDT services?  How about in OSG 1.3?

Experimentally, I found that I had to clear out the gram_scratch areas of the ilc user account, and delete running and pending jobs from the queue.  It then seemed to take a few hours for remaining ilc processes to be purged.

We don&#39;t like having to ban a VO, but as they do not use pool accounts, it was not possible to isolate the problem to a given user.  Thus dropping the ilc VO for now seemed like the most sensible alternative.  At present, there are no ilc jobs running on the cluster or gatekeeper node.  (Again, the problem that we saw was a very heavily loaded gatekeeper.)

by /DC=org/DC=doegrids/OU=People/CN=Alan Sill 503049</pre></div><div class='update_description'><i onclick="document.location='12876#1351088102'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2012-10-24T14:15:02+00:00">Oct 24, 2012 02:15 PM UTC</time> by <b>echism</b><a class="anchor" name="1351088102">&nbsp;</a></div><pre>Alan,

Here&#39;s the document on banning users.

<a href='https&#58;//twiki.grid.iu.edu/bin/view/Documentation/Release3/BanningUsersAtSite' target='_blank' rel='nofollow'>https&#58;//twiki.grid.iu.edu/bin/view/Documentation/Release3/BanningUsersAtSite</a>

If this is a problem, please contact me for further options.

Thank you,
Elizabeth</pre></div><div class='update_description'><i onclick="document.location='12876#1351087485'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2012-10-24T14:04:45+00:00">Oct 24, 2012 02:04 PM UTC</time> by <b>OSG-GOC</b><a class="anchor" name="1351087485">&nbsp;</a></div><pre>[by Alan Sill]

Our cluster is receiving a very high rate of globus-job-manager jobs from the ilc VO, leading to a load over 1000 on the gatekeeper.

I have tried shutting off grid services (we are still using OSG 1.2 on this resource) and terminating all ilc processes, then removing ilc from the list of supported VOs in the $DT_LOCATION/monitoring/osg-supported-vo-list.txt file, then re-running the osg-configure -c step, and also removing all gram_scratch files from the ilc user home area, b u these jobs seem to re-appear after we turn VDT services back on.

Can you please&#58;

1) Contact the ILC VO, especially the users below, to ask them to look into their workflow,

2) Please advise how we can remove ILC from the locally supported VOs until this is done/

I may have to block the ILC at the GUMS level to prevent our CE from being overloaded in the meantime.

Thanks,
Alan

PID&#58; 25783 -- PRIMA INFO ts=2012-10-23T14&#58;13&#58;38-06&#58;00 event=org.osg.prima.authz.end status=0 decision=PERMIT DN=&#34;/DC=ch/DC=cern/OU=Organic Units/OU=Users/CN=sposs/CN=641989/CN=Stephane Guillaume Poss&#34; FQAN=&#34;/ilc/Role=production/Capability=NULL&#34; FQAN_Issuer=&#34;/C=DE/O=GermanGrid/OU=DESY/CN=host/grid-voms.desy.de&#34; Service_URL=&#34;<a href='https&#58;//gums.hpcc.ttu.edu&#58;8443/gums/services/GUMSXACMLAuthorizationServicePort&#34' target='_blank' rel='nofollow'>https&#58;//gums.hpcc.ttu.edu&#58;8443/gums/services/GUMSXACMLAuthorizationServicePort&#34</a>; local_user=ilc â€¦
PID&#58; 26163 -- PRIMA INFO ts=2012-10-23T14&#58;13&#58;38-06&#58;00 event=org.osg.prima.authz.start DN=&#34;/C=JP/O=KEK/OU=CRC/CN=TIAN Junping&#34; FQAN=&#34;/ilc/Role=NULL/Capability=NULL&#34; FQAN_Issuer=&#34;/C=DE/O=GermanGrid/OU=DESY/CN=host/grid-voms.desy.de&#34; Service_URL=&#34;<a href='https&#58;//gums.hpcc.ttu.edu&#58;8443/gums/services/GUMSXACMLAuthorizationServicePort&#34' target='_blank' rel='nofollow'>https&#58;//gums.hpcc.ttu.edu&#58;8443/gums/services/GUMSXACMLAuthorizationServicePort&#34</a>;</pre></div><legend>Similar Recent Tickets <small>modified within the last 30 days</small></legend><div id="similar_tickets"><p class="muted">No similar tickets found.</p></div>
</div>
<script type="text/javascript">
function reset_anchor() {
    $("#updates .selected").removeClass("selected");
    var urls = document.location.toString().split('#'); 
    var anchor = urls[1];
    if(anchor) {
        $("a[name='"+anchor+"']").parents(".update_description").addClass("selected");
    }
}
function submitspam(ticket_id) {
    myret = confirm("Would you like to close this ticket as a security ticket, and submit the ticket content to akismet?");
    if(myret == true) {
        $.ajax("viewer/processspam?id="+ticket_id).done(function() {
            window.location.reload();
        });
    }
}

$(function() {
    reset_anchor();
    var ADDITIONAL_COOKIE_NAME = 'gocticket';
    var options = { path: '/', expires: 365};

    if(window.opener && window.opener.name == "gocticket_list") {
        v = $.cookie("closewindow");
        if(!v) {
            $("#closewindow").attr("checked", "checked"); //on by default
        } else {
            if(v == "checked") {
                $("#closewindow").attr("checked", "checked");
            }
        }
        $("#closewindow").click(function() {
            $.cookie("closewindow", $(this).attr('checked'), options);
        });
    } else {
        $("#closewindow_area").hide();
    }
    function updateTimeago() {
        $("time").timeago();
        setTimeout(updateTimeago, 30*1000);
    }
    updateTimeago();
    $(".description").focus(expand_description);
});
</script>
<hr/>
<footer>
<p>GOC Ticket Version 2.2 | <a href="https://ticket.opensciencegrid.org/goc/submit?app_issue_check=on&amp;app_issue_type=goc&amp;app_goc_url=https%3A%2F%2Fticket.opensciencegrid.org%3A443%2F12876">Report Bugs</a>
 | <a href="https://github.com/opensciencegrid/operations/blob/master/docs/privacy.md">Privacy Policy</a>
</p>

<p> <img align="top" src="/images/tag_orange.png"/> Copyright 2018 The Trustees of Indiana University - Developed for Open Science Grid</p>
</footer>


</div><!--container-fluid-->
<script src="https://ticket1.grid.iu.edu:8443/socket.io/socket.io.js"></script>
<script>
var chat = io.connect('https://ticket1.grid.iu.edu:8443');
chat.on('connect', function() {
    chat.emit('authenticate', {nodekey:'', ticketid: 12876});
});
chat.on('peers', function(peers) {
    $("#peers").html("");
    for(var pid in peers) {
        var peer = peers[pid];
        addPeer(pid, peer);
    }
});
chat.on('peer_disconnect', function(pid) {
    $("#peer_"+pid).hide("slow");
});
chat.on('peer_connected', function(peers) {
    //expect only 1 peer connecting, but..
    for(var pid in peers) {
        var peer = peers[pid];
        addPeer(pid, peer);
    }
});
chat.on('submit', function() {
    if(confirm("This ticket was updated. Do you want to refresh?")) {
        history.go(0);
    }
});

function addPeer(pid, peer) {
    var ipinfo = "";
    if(peer.ip != undefined) {
        ipinfo = "<span class=\"ip\">"+peer.ip+"</span>";
    }
    if(chat.io.engine.id == pid) {
        //don't display myself
        return;
    }
    var html = "<li class=\"new\" id=\"peer_"+pid+"\" class=\"peer\">"+peer.name+ipinfo+"</li>";
    $("#peers").prepend(html);
    $("#peers .new").animate({bottom: 0}, 1000, function() {$(this).removeClass("new")});
}

$(function() {
    $("#ticket_form").submit(function() {
        chat.emit('submit');
        return true;
    });
});
</script>
<script>
//used by searchbox
function parseValue(value) {
    var obj = new Object();
    var tokens = value.split("\t");
    obj.str = tokens[0];
    obj.count = tokens[1];
    return obj;
}

$(function() {
    //bootstrap-2.0.4 stuff
    $(".alert-message").alert();
    $(".dropdown-toggle").dropdown();
    $("span[rel='tooltip']").tooltip();
    $("a[rel=popover]").popover();

    //activate menu that user is currently on
    $("#menu_navigator").addClass("active"); 
    $("#submenu_").addClass("active"); 

    //translate zend validation error message to bootstrap
    $(".errors").addClass("alert").addClass("alert-error");

    //enable autocomplete for search box
    $("#search").autocomplete({
        source: function( request, response ) {
            $.ajax({
                url: "search/autocomplete",
                dataType: "text",
                data: {
                    //featureClass: "P",
                    //style: "full",
                    //maxRows: 12,
                    //name_startsWith: request.term
                    q: request.term
                },
                success: function( data ) {
                    response( $.map( data.split("\n"), function( item ) {
                        if(item == "") return null;
                        return {
                            value: item
                        }
                    }));
                }
            });
        },
        select: function(event, ui) {
            document.location = "search?q="+ui.item.value;
        }
    });
    
});
</script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
try {
var pageTracker = _gat._getTracker("UA-69012-13");
pageTracker._trackPageview();
} catch(err) {}
</script>

</body>
