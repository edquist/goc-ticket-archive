<!DOCTYPE html>
<html lang="en">
  <head>
  <base href="">
    <title>[35526] jobs with executable blastpgp thrashing on my site</title>    <meta charset="utf-8" />
    <meta name="verify-v1" content="na5IcAJsZVOfEkboRxuIiZ1zpZgnZiWra+nKcS7nA/o=" />
    <meta name="google-site-verification" content="DLrk3ft4s8b-S2TloLCL2LD_t6wcTjgSluf5pmiu2kA" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="description" content="" />
    <meta name="robots" content="noindex, nofollow" />

    <style type="text/css">
      body {
        padding-top: 50px;
        padding-bottom: 40px;
      }
      .sidebar-nav {
        padding: 9px 0;
      }
     #search {
            width: 300px;
     }

    </style>

<script src="https://code.jquery.com/jquery-3.0.0.js"></script>
<script src="https://code.jquery.com/jquery-migrate-3.0.1.js"></script>

   <link href="https://netdna.bootstrapcdn.com/bootstrap/2.3.2/css/bootstrap.min.css" rel="stylesheet"/>
    <script src="https://netdna.bootstrapcdn.com/bootstrap/2.3.2/js/bootstrap.min.js"></script>

    <link href="https://netdna.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet"/>
    <link href="https://ajax.googleapis.com/ajax/libs/jqueryui/1.10.4/themes/smoothness/jquery-ui.min.css" rel="stylesheet"/>
 <script src="https://ajax.googleapis.com/ajax/libs/jqueryui/1.12.1/jquery-ui.min.js"></script>


    <link href="https://cdnjs.cloudflare.com/ajax/libs/select2/4.0.0-rc.2/css/select2.min.css" rel="stylesheet" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/select2/4.0.0-rc.2/js/select2.min.js"></script>

    <link href="css/ticket.css" rel="stylesheet" />
    <script src="lib/jquery.cookie.js"></script>

    <link href="images/tag_orange.png" rel="icon" type="image/png"/>
  </head>

  <body>
    <div class="navbar navbar-inverse navbar-fixed-top">
      <div class="navbar-inner">
        <div class="container-fluid">
            <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </a>

            <a class="brand" style="padding: 6px 0px 0px 6px;" href="http://opensciencegrid.org"><img src="images/osglogo.40x30.png"/></a>
            <ul class="nav">
                <li class="dropdown"><a href="https://ticket.opensciencegrid.org/#" class="dropdown-toggle" data-toggle="dropdown">GOC Ticket <b class="caret"></b></a>
                    <ul class="dropdown-menu">
                    <li><a href="https://my.opensciencegrid.org">MyOSG</a></li>
                    <li><a href="https://oim.opensciencegrid.org">OIM</a></li>
                    <li class="active"><a href="https://ticket.opensciencegrid.org/index">Ticket</a></li>
	<li class="divider"></li>
	<li><a href="http://repo.grid.iu.edu">Repo</a></li>
	<li class="divider"></li>
	<li><a href="http://blogs.grid.iu.edu">Blog</a></li>
                    <li><a href="http://display.grid.iu.edu">Display</a></li>
                    <li><a href="http://osggoc.blogspot.com/">News</a></li>
                    </ul>
                </li>
            </ul>
            <ul class="nav pull-right">
                <li><a href="https://ticket.opensciencegrid.org/sso/">Login</a></li>            </ul>

            <div class="nav-collapse">
                <ul class="nav">
			 <li id="menu_submit"><a href="https://ticket.opensciencegrid.org/submit">Submit</a></li><li id="menu_view" class="dropdown"><a href="https://ticket.opensciencegrid.org/\#" class="dropdown-toggle" data-toggle="dropdown">View <b class="caret"></b></a><ul class="dropdown-menu"><li id="submenu_listopen"><a href="https://ticket.opensciencegrid.org/list/open">Open Tickets</a></li><li id="submenu_listrecentclose"><a href="https://ticket.opensciencegrid.org/list/recentclose">Recently Closed Tickets</a></li><li class="divider"></li><li id="submenu_alltickets"><a href="https://ticket.opensciencegrid.org/search?q=&amp;sort=id">All Tickets</a></li></ul></li>                </ul>

                <form class="navbar-search pull-right" action="https://ticket.opensciencegrid.org/viewer">
                    <input id="search" type="text" name="id" class="search-query span2" placeholder="Search Ticket" value=""/>
                </form>
            </div>
        </div>
      </div>
    </div>

<script type='text/javascript' src='lib/jquery.timeago.js'></script>
<script type='text/javascript' src='lib/byte2size.js'></script>
<style>
#updates .toolbar {
position: relative;
margin-top: 0px;
top: -10px;
font-weight: normal;
}
#updates a.anchor {
position: relative;
top: -50px;
}
#updates .selected pre {
animation:selected 2s;
animation-iteration-count: 2;
animation-direction: alternate;
-webkit-animation:selected 2s; 
-webkit-animation-iteration-count: 2;
-webkit-animation-direction: alternate;
box-shadow: inset 1px 1px 20px #9ad;
border: 1px solid #9ab;
margin: 5px 0px;
padding-left: 10px;
}
@keyframes selected {
    from  {
        box-shadow: inset 1px 1px 20px #9ad;
        border: 1px solid #9ab;
    }
    to {
        box-shadow: inset 1px 1px 20px #05c;
        border: 1px solid #05c;
    }
}
@-webkit-keyframes selected {
    from  {
        box-shadow: inset 1px 1px 20px #9ad;
        border: 1px solid #9ad;
    }
    to {
        box-shadow: inset 1px 1px 20px #05c;
        border: 1px solid #05c;
    }
}
#updates pre {
background-color: inherit;
line-height: 15px;
padding: 5px;
}
#updates .header {
color: #999;
}
#updates .update_history pre {
background-color: #eee;
color: #666;
font-size: 85%;
}
#updates .clickable {
cursor: pointer;
}
#updates .clickable:hover {
color: #D98719;
}
#updates .meta_information pre {
background-color: #fed;
}
#similar_tickets {
max-height: 300px;
overflow-y: auto;
pointer-events: none;
padding: 5px;
background-color: #f4f4f4;
}
.btn-toolbar {
margin-bottom: 0;
height: 30px;
}
#peers {
position: fixed;
bottom: 0px;
right: 0px;
z-index: 100;
list-style: none;
padding: 5px 0px 0px 5px;
margin: 0px;
background-color: white;
box-shadow: 0px 0px 10px white;
}
#peers li {
background-color: #ccc;
color: #000;
display: inline-block;
padding: 5px 10px;
margin-right: 5px;
position: relative;
}
/*
#peers li:hover {
background-color: #999;
cursor: pointer;
}
*/
#peers span.ip {
padding-left: 5px;
color: #666;
}
#peers .new {
bottom: -30px;
}
/*
#peers .me {
background-color: red;
}
*/
</style>

<div class="container-fluid">
<ul id="peers"></ul>
<div class="alert alert-danger"><a class="close" href="https://ticket.opensciencegrid.org/#" data-dismiss="alert">&times;</a>By the end of May 2018, the ticketing system at https://ticket.opensciencegrid.org will be retired and support will be provided at https://support.opensciencegrid.org. Throughout this transition the support email (help@opensciencegrid.org) will be available as a point of contact.<br><br>                                                   
                                                                                                                                                                                   
Please see the service migration page for details: https://opensciencegrid.github.io/technology/policy/service-migrations-spring-2018/#ticket</div><div id="presence" class="pull-right"></div><div class="ticketgui"><script type="text/javascript" src="lib/checktab.js"></script>

<script>
var expanded = false;
function expand_description() {
    var desc = $(".description");
    if(!expanded) {
        expanded = true;
        //expand to minheight
        var min = 250;
        if(desc.height() < min) {
            desc.animate({height: min}, 200);
        }
    }
}

$(document).ready(function() {
    $("input[name='nad']").datepicker({
        dateFormat: 'yy-mm-dd'
    });
});

</script>



<style>
.form-horizontal .control-label {
padding-top: inherit;
font-size:90%;
color:#666;
}
label {
margin-bottom: 0px;
}
.controls {
line-height: 18px;
}
</style>
<form class="form-horizontal" action="https://ticket.opensciencegrid.org/viewer/updatebasic?id=35526" method="post">
<div class="page-header">
    <h3><span class="muted">35526</span> / jobs with executable blastpgp thrashing on my site</h3>
</div>

<div class="row-fluid">
<div class="span5">
    <legend>Contact</legend>
    <div class="control-group">
        <label class="control-label">Full Name</label>
        <div class="controls">Richard T. Jones</div>
    </div>
    <div class="control-group">
        <label class="control-label">Email</label>
        <div class="controls">
            <i class="icon-lock"></i>        </div>
    </div>
    <div class="control-group">
        <label class="control-label">Phone</label>
        <div class="controls">
            <i class="icon-lock"></i>        </div>
    </div>
    <div class="control-group">
        <label class="control-label">CC</label>
        <div class="controls">
            <i class="icon-lock"></i>        </div>
    </div>

    <legend>Details</legend>
    <div class="control-group"><label class="control-label">Associated VO</label><div class="controls">HCC</div></div><div class="control-group"><label class="control-label">Submitted Via</label><div class="controls">GOC Ticket/submit</div></div><div class="control-group"><label class="control-label">Submitter</label><div class="controls">Richard T. Jones</div></div><div class="control-group"><label class="control-label">Support Center</label><div class="controls">OSG-GOC</div></div><div class="control-group"><label class="control-label">Ticket Links</label><div class="controls"></div></div>
    <div class="control-group">
        <label class="control-label">Ticket Type</label>
        <div class="controls">Problem/Request</div>
    </div>
    <div class="control-group">
        <label class="control-label">Priority</label>
        <div class="controls">Normal</div>
    </div>
    <div class="control-group">
        <label class="control-label">Status</label>
        <div class="controls">
Closed</div>
    </div>
    <div class="control-group">
        <label class="control-label">Next Action</label>
        <div class="controls">ENG Action</div>
    </div>
    <div class="control-group">
        <label class="control-label">Next Action Deadline</label>
        <div class="controls flag_red">2018-03-13</div>
    </div>

</div><!--span-->
<div class="span7">
    <legend>Assignees</legend>
    <div class="assignee" style="width: 60%">Kyle Gross <span class="muted"> / OSG GOC Support Team</span></div><div class="assignee" style="width: 60%">Sarah Schmiechen <span class="muted"> / OSG GOC Support Team</span></div><div class="assignee" style="width: 60%">Sarah Schmiechen <span class="muted"> / OSG Operations Infrastructure</span></div><div class="assignee" style="width: 60%">OSG-GOC <span class="muted"> / OSG Support Centers</span></div><div class="assignee" style="width: 60%">Software Support (Triage) <span class="muted"> / OSG Software Team</span></div><div class="assignee" style="width: 60%">Brian Lin <span class="muted"> / OSG Software Team</span></div>    <br>

    <legend>Assignees</legend>
    TODO
    <br>

    <style>
legend.noborder {
border-bottom: none;
}
</style>

<div id="attachment-list"/>
<script>
$(function () {
    var first = true;
    $.getJSON("attachment/list/35526", function (files) {
        //console.dir(files);
        var html = "<table class=\"table table-condensed\">";
        $(files).each(function() {
            if(first) {
                first = false;
                html += "<legend class=\"noborder\">Attachmenets</legend>";
            }
            html += "<tr class=\"attachment\">";
            html += "<td><img src=\""+this.thumbnail_url+"\"/></td>";
            html += "<td><a href=\""+this.url+"\" target=\"_blank\">"+this.name+"</a></td>";
            html += "<td>"+bytesToSize(this.size, 1)+"</td>";
            html += "</tr>";
        });
        html += "</table>";
        $("#attachment-list").html(html);
    });
});

function download(url) {
    window.open(url, "_blank");
}
</script>


</div><!--span-->
</div><!--row-fluid-->


</form>

</div>
<div id="updates" style="clear: both;">
    <legend>Past Updates
    <div class="btn-toolbar pull-right toolbar">
    </div><!--btn-toolbar-->
    </legend>

    <div class='update_description'><i onclick="document.location='35526#1521041253'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2018-03-14T15:27:33+00:00">Mar 14, 2018 03:27 PM UTC</time> by <b>Suchandra Thapa</b><a class="anchor" name="1521041253">&nbsp;</a></div><pre>Closing per Richard&#39;s last email</pre></div><div class='update_description'><i onclick="document.location='35526#1520956758'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2018-03-13T15:59:18+00:00">Mar 13, 2018 03:59 PM UTC</time> by <b>GLUEX</b><a class="anchor" name="1520956758">&nbsp;</a></div><pre>Brian,

Ok, thanks a lot for all of your help with this. I think I know what I need
to do if things get any worse. For the time being, I am going to err on the
side of permissive, and run a periodic cron job that scans for these
offending jobs and kills them. This is what I have been doing, and it is
sufficient to keep my system stable. The problem has not been so bad over
the past month or so, so I think I can live with it. This ticket can be
closed, and thanks again!

-Richard Jones

On Tue, Mar 13, 2018 at 11&#58;48 AM, Open Science Grid FootPrints &#60;
osg@....&#62; wrote&#58;

<font color='#7F7E6F'>&#62; [Duplicate message snipped]</font></pre></div><div class='update_description'><i onclick="document.location='35526#1520956082'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2018-03-13T15:48:02+00:00">Mar 13, 2018 03:48 PM UTC</time> by <b>Brian Lin</b><a class="anchor" name="1520956082">&nbsp;</a></div><pre>Richard,

I spoke with the developer again to clarify the CGROUP_MEMORY_POLICY and this is what I came away with&#58;

- MemoryUsage is populated solely by the resident set size of the cgroup
- cgroups set their memory limit based on the resident set size + the kernel cache
- A &#39;hard&#39; policy will attempt to request swap and if none is available, the job will be killed. A &#39;soft&#39; policy will attempt to request swap and if none is available, request physical memory and if none is available, the job will be killed.

So it turns out that &#34;CGROUP_MEMORY_LIMIT_POLICY = soft&#34; may be what you&#39;re looking for. I&#39;m not sure you&#39;d be able to ban specific users, we&#39;d have to get in touch with the VO in question to track down the offending user, but you could stop accepting jobs from the entire VO.

And sorry for the runaround, sorting out memory policy can be fairly difficult.

Thanks,
Brian</pre></div><div class='update_description'><i onclick="document.location='35526#1520637793'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2018-03-09T23:23:13+00:00">Mar 9, 2018 11:23 PM UTC</time> by <b>Sarah Schmiechen</b><a class="anchor" name="1520637793">&nbsp;</a></div><pre>I am out of the office, returning July 18th</pre></div><div class='update_description'><i onclick="document.location='35526#1520637613'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2018-03-09T23:20:13+00:00">Mar 9, 2018 11:20 PM UTC</time> by <b>GLUEX</b><a class="anchor" name="1520637613">&nbsp;</a></div><pre>Hello Brian,

Just to understand, will jobs start to thrash (rapidly page-fault) as soon
as they hit the maximum limit allocated to their cgroup under this policy,

&#34;CGROUP_MEMORY_LIMIT_POLICY = hard&#34;

or will they be killed? I hesitate to change anything that might jeopardize
efficient running of gluex jobs. It would be much easier for me just to
block one or two bad-behaving users, wouldn&#39;t it? By far, most users who
send jobs our way behave well under our present policy. Setting any
resource limit policy to &#34;hard&#34; will inevitably cost me in terms of Gluex
job success rate, I fear.

-Richard Jones

On Tue, Mar 6, 2018 at 8&#58;19 AM, Open Science Grid FootPrints &#60;
osg@....&#62; wrote&#58;

<font color='#7F7E6F'>&#62; [Duplicate message snipped]</font></pre></div><div class='update_description'><i onclick="document.location='35526#1520342375'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2018-03-06T13:19:35+00:00">Mar 6, 2018 01:19 PM UTC</time> by <b>Sarah Schmiechen</b><a class="anchor" name="1520342375">&nbsp;</a></div><pre>Richard,

Any updates on this?

Thanks,
Sarah</pre></div><div class='update_description'><i onclick="document.location='35526#1519743314'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2018-02-27T14:55:14+00:00">Feb 27, 2018 02:55 PM UTC</time> by <b>Brian Lin</b><a class="anchor" name="1519743314">&nbsp;</a></div><pre>Richard,

Did you see my previous message? Have you tried setting &#34;CGROUP_MEMORY_LIMIT_POLICY = hard&#34; in your condor config?

Thanks,
Brian</pre></div><div class='update_description'><i onclick="document.location='35526#1519147533'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2018-02-20T17:25:33+00:00">Feb 20, 2018 05:25 PM UTC</time> by <b>Brian Lin</b><a class="anchor" name="1519147533">&nbsp;</a></div><pre>Richard,

After speaking with the developer, it looks like Condor is arguably doing the right thing and just tracking the resident set size of the cgroup, which turns out to be 34MB&#58;

[root@stat12 ~]# cat
/cgroup/memory/htcondor/condor_local_execute_slot1_1&#92;@stat12.phys.uconn.edu/memory.stat
cache 7222345728
rss 34127872

At this point, I&#39;d recommend setting &#96;CGROUP_MEMORY_LIMIT_POLICY = hard&#96; and giving up on our other configuration. A hard cgroup memory limit will put a limit on physical memory (RSS + cache) but allow for allocation of more virtual memory, which should still give you enough flexibility for your gluex jobs.

- Brian</pre></div><div class='update_description'><i onclick="document.location='35526#1518711857'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2018-02-15T16:24:17+00:00">Feb 15, 2018 04:24 PM UTC</time><a class="anchor" name="1518711857">&nbsp;</a></div><pre>Brian,

This particular job is running on stat12.phys.uconn.edu in slot1_1.

[root@stat12]# top

PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND
2831998 prod01    20   0 24.9g 6.4g 6.4g R 97.9 81.7  31&#58;03.90
blastpgp

[root@stat12 ~]# ls
/cgroup/memory/htcondor/condor_local_execute_slot1_1&#92;@stat12.phys.uconn.edu/
cgroup.event_control  memory.force_empty         memory.memsw.failcnt
memory.memsw.usage_in_bytes      memory.soft_limit_in_bytes
memory.usage_in_bytes  tasks
cgroup.procs          memory.limit_in_bytes
memory.memsw.limit_in_bytes      memory.move_charge_at_immigrate
memory.stat                 memory.use_hierarchy
memory.failcnt        memory.max_usage_in_bytes
memory.memsw.max_usage_in_bytes  memory.oom_control
memory.swappiness           notify_on_release

[root@stat12 ~]# cat
/cgroup/memory/htcondor/condor_local_execute_slot1_1&#92;@stat12.phys.uconn.edu/memory.usage_in_bytes
7253958656

[root@stat12 ~]# cat
/cgroup/memory/htcondor/condor_local_execute_slot1_1&#92;@stat12.phys.uconn.edu/memory.stat
cache 7222345728
rss 34127872
mapped_file 1935163392
pgpgin 668986154
pgpgout 667216598
swap 0
inactive_anon 19226624
<div id='show_1580476602' class=''><button class="btn">Show More</button></div><div class='detail hidden' id='detail_1580476602'>active_anon 14901248
inactive_file 6930923520
active_file 291405824
unevictable 0
hierarchical_memory_limit 9223372036854775807
hierarchical_memsw_limit 9223372036854775807
total_cache 7222345728
total_rss 34127872
total_mapped_file 1935163392
total_pgpgin 668986154
total_pgpgout 667216598
total_swap 0
total_inactive_anon 19226624
total_active_anon 14901248
total_inactive_file 6930923520
total_active_file 291405824
total_unevictable 0

[now back on the CE]

[root@gluskap ~]# condor_q -run -nobatch | grep slot1_1@stat12
7493045.0   hcc             2/14 04&#58;20   1+04&#58;43&#58;15
slot1_1@....

[root@gluskap ~]# condor_q -l 7493045.0
Arguments = &#34;-v std -name gfactory_instance -entry
OSG_US_UConn_gluskap -clientname glidein-unl-edu_OSG_gWMSFrontend.main
-schedd schedd_glideins9@.... -proxy OSG -factory
OSGGOC -web <a href='http&#58;//glidein.grid.iu.edu/factory/stage' target='_blank' rel='nofollow'>http&#58;//glidein.grid.iu.edu/factory/stage</a> -sign
34fbbb0c73a71842496e3a1ccb9116bb27664ae2 -signentry
189b29926504d2aeb97f788426f1424e0e82bfcd -signtype sha1 -descript
description.i29ajF.cfg -descriptentry description.i29ajF.cfg -dir
Condor -param_GLIDEIN_Client glidein-unl-edu_OSG_gWMSFrontend.main
-submitcredid 295461 -slotslayout fixed -clientweb
<a href='http&#58;//glidein.unl.edu/vofrontend/stage' target='_blank' rel='nofollow'>http&#58;//glidein.unl.edu/vofrontend/stage</a> -clientsign
df13e6d977f6b5f9df7e8ca1656fc2160f5e81e4 -clientsigntype sha1
-clientdescript description.i2cfak.cfg -clientgroup main
-clientwebgroup <a href='http&#58;//glidein.unl.edu/vofrontend/stage/group_main' target='_blank' rel='nofollow'>http&#58;//glidein.unl.edu/vofrontend/stage/group_main</a>
-clientsigngroup 52d4564e38291790f9527ae55844009987ae462d
-clientdescriptgroup description.i2cfak.cfg -param_CONDOR_VERSION
default -param_GLIDEIN_Glexec_Use NEVER -param_GLIDEIN_Job_Max_Time
34800 -param_GLIDECLIENT_ReqNode glidein.dot,grid.dot,iu.dot,edu
-param_CONDOR_OS default -param_MIN_DISK_GBS 1
-param_GLIDEIN_Monitoring_Enabled False -param_CONDOR_ARCH default
-param_UPDATE_COLLECTOR_WITH_TCP True -param_PREEMPT_GRACE_TIME 34800
-param_USE_MATCH_AUTH True -param_GLIDEIN_Report_Failed NEVER
-param_GLIDEIN_Entry_Rank
.open,.open,Owner.eq,.not,.eq,.quot,boinc.quot,.close,.star,10000.close,
-param_GLIDEIN_Collector glidein.dot,unl.dot,edu.colon,9620.minus,9660
-cluster 5165664 -subcluster 3&#34;
AutoClusterAttrs =
&#34;JobUniverse,LastCheckpointPlatform,NumCkpts,MachineLastMatchTime,MemoryUsage,OWNER,_condor_RequestCpus,_condor_RequestDisk,_condor_RequestMemory,RequestCpus,RequestDisk,RequestMemory,Scheduler,User,MY_Memory,ParentSlotID,GlobalJobId,JobType,RemoteGroupQuota,SubmitterGroupQuota,AccountingGroup,CumulativeSlotTime,ConcurrencyLimits,NiceUser,Rank,Requirements,DiskUsage,GlideinCpusIsGood,JobCpus,JobIsRunning,JobMemory,JobStatus,MATCH_EXP_JOB_GLIDEIN_Memory,OriginalCpus,OriginalMemory,ResidentSetSize&#34;
AutoClusterId = 394
BlockReadKbytes = 1571295968
BlockReads = 12754270
BlockWriteKbytes = 21620
BlockWrites = 5405
BytesRecvd = 74482.0
BytesSent = 0.0
ClusterId = 7493045
Cmd = &#34;glidein_startup.sh&#34;
CmdHash = &#34;CmdMD5-5d4e1efb079b3e58d718b8391474c803&#34;
CommittedSlotTime = 0
CommittedSuspensionTime = 0
CommittedTime = 0
CompletionDate = 0
CondorCECollectorHost =
ifThenElse(regexp(&#34;&#58;&#34;,&#34;gluskap.phys.uconn.edu&#58;9619&#34;),&#34;gluskap.phys.uconn.edu&#58;9619&#34;,strcat(&#34;gluskap.phys.uconn.edu&#58;9619&#34;,&#34;&#58;&#34;,9619))
CumulativeRemoteSysCpu = 5954.0
CumulativeRemoteUserCpu = 155681.0
CumulativeSlotTime = 41770.0
CumulativeSuspensionTime = 0
CurrentHosts = 1
DelegatedProxyExpiration = 1518779401
DiskUsage = 27500000
DiskUsage_RAW = 25642644
EncryptExecuteDirectory = false
EnteredCurrentStatus = 1518600069
Environment = &#34;HOME=/home/hcc
CONDORCE_COLLECTOR_HOST=gluskap.phys.uconn.edu&#58;9619
OSG_GRID=&#39;/etc/osg/wn-client/&#39; LD_PRELOAD=&#39;/lib64/libkeepalive.so&#39;
OSG_SQUID_LOCATION=&#39;gryphn.phys.uconn.edu&#58;3128&#39; KEEPCNT=&#39;9&#39;
KEEPINTVL=&#39;60&#39; OSG_SITE_READ=&#39;dcap&#58;//nod25.phys.uconn.edu&#58;22125/pnfs/phys.uconn.edu/data&#39;
OSG_APP=&#39;/nfs/direct/app&#39; OSG_HOSTNAME=&#39;gluskap.phys.uconn.edu&#39;
OSG_DATA=&#39;/nfs/direct/annex/osg-data&#39; LOCAL_ROOTSYS=&#39;/usr/local/root&#39;
CERN_ROOT=&#39;/usr/local/cern/pro&#39; OSG_WN_TMP=&#39;worker_node_temp&#39;
CERN=&#39;/usr/local/cern&#39; OSG_STORAGE_ELEMENT=&#39;True&#39; KEEPIDLE=&#39;500&#39;
XERCESCROOT=&#39;/usr/local/xerces&#39; OSG_SITE_NAME=&#39;UConn-OSG_CE&#39;
GLOBUS_LOCATION=&#39;/usr&#39; PATH=&#39;/bin&#58;/usr/bin&#58;/sbin&#58;/usr/sbin&#39;
OSG_DEFAULT_SE=&#39;grinch.phys.uconn.edu&#39;
OSG_SITE_WRITE=&#39;<a href='srm&#58;//grinch.phys.uconn.edu&#58;8443/&#39' target='_blank' rel='nofollow'>srm&#58;//grinch.phys.uconn.edu&#58;8443/&#39</a>; &#34;
Err = &#34;_condor_stderr&#34;
ExecutableSize = 75
ExecutableSize_RAW = 75
ExitBySignal = false
ExitStatus = 0
GlideinClient = &#34;glidein-unl-edu_OSG_gWMSFrontend.main&#34;
GlideinCpusIsGood =  !isUndefined(MATCH_EXP_JOB_GLIDEIN_Cpus) && (
int(MATCH_EXP_JOB_GLIDEIN_Cpus) =!= error )
GlideinCredentialIdentifier = &#34;295461&#34;
GlideinEntryName = &#34;OSG_US_UConn_gluskap&#34;
GlideinFactory = &#34;OSGGOC&#34;
GlideinFrontendName = &#34;hcc-glidein&#58;frontend&#34;
GlideinLogNr = &#34;20180213&#34;
GlideinName = &#34;gfactory_instance&#34;
GlideinProxyURL = &#34;OSG&#34;
GlideinSecurityClass = &#34;frontend&#34;
GlideinSlotsLayout = &#34;fixed&#34;
GlideinWebBase = &#34;<a href='http&#58;//glidein.grid.iu.edu/factory/stage&#34' target='_blank' rel='nofollow'>http&#58;//glidein.grid.iu.edu/factory/stage&#34</a>;
GlideinWorkDir = &#34;Condor&#34;
GlobalJobId = &#34;gluskap.phys.uconn.edu#7493045.0#1518600047&#34;
ImageSize = 7500000
ImageSize_RAW = 6440700
In = &#34;/dev/null&#34;
Iwd = &#34;/home/condor/4856/0/cluster1214856.proc0.subproc0&#34;
JOB_GLIDEIN_Cpus = &#34;$$(ifThenElse(WantWholeNode is true,
!isUndefined(TotalCpus) ? TotalCpus &#58; JobCpus, OriginalCpus))&#34;
JOB_GLIDEIN_Memory = &#34;$$(TotalMemory&#58;0)&#34;
JobCpus = JobIsRunning ? int(MATCH_EXP_JOB_GLIDEIN_Cpus) &#58; OriginalCpus
JobCurrentStartDate = 1518600069
JobCurrentStartExecutingDate = 1518600069
JobIsRunning = ( JobStatus =!= 1 ) && ( JobStatus =!= 5 ) && GlideinCpusIsGood
JobLeaseDuration = 2400
JobMemory = JobIsRunning ? int(MATCH_EXP_JOB_GLIDEIN_Memory) * 95 /
100 &#58; OriginalMemory
JobNotification = 0
JobPrio = 0
JobRunCount = 3
JobStartDate = 1518600069
JobStatus = 2
JobUniverse = 5
KillSig = &#34;SIGTERM&#34;
LastHoldReason = &#34;HTCondor-CE held job due to no matching routes,
route job limit, or route failure threshold; see &#39;HTCondor-CE
Troubleshooting Guide&#39;&#34;
LastHoldReasonCode = 26
LastHoldReasonSubCode = 0
LastJobLeaseRenewal = 1518707626
LastJobStatus = 1
LastMatchTime = 1518600069
LastReleaseReason = &#34;by gridmanager (by user hcc)&#34;
LastSuspensionTime = 0
LeaveJobInQueue = JobStatus == 4
LocalSysCpu = 0.0
LocalUserCpu = 0.0
MachineAttrCpus0 = 1
MachineAttrSlotWeight0 = 1
MATCH_EXP_JOB_GLIDEIN_Memory = &#34;8060&#34;
MATCH_TotalMemory = 8060
MaxHosts = 1
maxMemory = 2048
maxWallTime = 4320
MemoryUsage = ( ( ResidentSetSize + 1023 ) / 1024 )
MinHosts = 1
MyType = &#34;Job&#34;
NiceUser = false
NumCkpts = 0
NumCkpts_RAW = 0
NumJobCompletions = 0
NumJobMatches = 1
NumJobReconnects = 2
NumJobStarts = 1
NumRestarts = 0
NumShadowStarts = 3
NumSystemHolds = 0
OnExitHold = ifThenElse(orig_OnExitHold =!=
undefined,orig_OnExitHold,false) || ifThenElse(minWalltime =!=
undefined && RemoteWallClockTime =!= undefined,RemoteWallClockTime &#60;
60 * minWallTime,false)
OnExitHoldReason = ifThenElse(( orig_OnExitHold =!= undefined ) &&
orig_OnExitHold,ifThenElse(orig_OnExitHoldReason =!=
undefined,orig_OnExitHoldReason,strcat(&#34;The on_exit_hold expression
(&#34;,unparse(orig_OnExitHold),&#34;) evaluated to
TRUE.&#34;)),ifThenElse(minWalltime =!= undefined && RemoteWallClockTime
=!= undefined && ( RemoteWallClockTime &#60; 60 * minWallTime
),strcat(&#34;The job&#39;s wall clock time, &#34;,int(RemoteWallClockTime /
60),&#34;min, is less than the minimum specified by the job
(&#34;,minWalltime,&#34;)&#34;),&#34;Job held for unknown reason.&#34;))
OnExitHoldSubCode = ifThenElse(( orig_OnExitHold =!= undefined ) &&
orig_OnExitHold,ifThenElse(orig_OnExitHoldSubCode =!=
undefined,orig_OnExitHoldSubCode,1),42)
orig_environment = &#34;&#34;
orig_OnExitHold = undefined
orig_OnExitHoldReason = undefined
orig_OnExitHoldSubCode = undefined
orig_RequestCpus = 1
OriginalCpus = 1
OriginalMemory = 2048
OrigMaxHosts = 1
osg_environment = &#34;OSG_GRID=&#39;/etc/osg/wn-client/&#39;
LD_PRELOAD=&#39;/lib64/libkeepalive.so&#39;
OSG_SQUID_LOCATION=&#39;gryphn.phys.uconn.edu&#58;3128&#39; KEEPCNT=&#39;9&#39;
KEEPINTVL=&#39;60&#39; OSG_SITE_READ=&#39;dcap&#58;//nod25.phys.uconn.edu&#58;22125/pnfs/phys.uconn.edu/data&#39;
OSG_APP=&#39;/nfs/direct/app&#39; OSG_HOSTNAME=&#39;gluskap.phys.uconn.edu&#39;
OSG_DATA=&#39;/nfs/direct/annex/osg-data&#39; LOCAL_ROOTSYS=&#39;/usr/local/root&#39;
CERN_ROOT=&#39;/usr/local/cern/pro&#39; OSG_WN_TMP=&#39;worker_node_temp&#39;
CERN=&#39;/usr/local/cern&#39; OSG_STORAGE_ELEMENT=&#39;True&#39; KEEPIDLE=&#39;500&#39;
XERCESCROOT=&#39;/usr/local/xerces&#39; OSG_SITE_NAME=&#39;UConn-OSG_CE&#39;
GLOBUS_LOCATION=&#39;/usr&#39; PATH=&#39;/bin&#58;/usr/bin&#58;/sbin&#58;/usr/sbin&#39;
OSG_DEFAULT_SE=&#39;grinch.phys.uconn.edu&#39;
OSG_SITE_WRITE=&#39;<a href='srm&#58;//grinch.phys.uconn.edu&#58;8443/&#39' target='_blank' rel='nofollow'>srm&#58;//grinch.phys.uconn.edu&#58;8443/&#39</a>;&#34;
Out = &#34;_condor_stdout&#34;
Owner = &#34;hcc&#34;
ProcId = 0
PublicClaimId = &#34;&#60;137.99.79.232&#58;9618&#62;#1517616506#12343#...&#34;
QDate = 1518600047
Rank = 0.0
RecentBlockReadKbytes = 17318804
RecentBlockReads = 140889
RecentBlockWriteKbytes = 492
RecentBlockWrites = 123
RecentStatsLifetimeStarter = 1200
ReleaseReason = &#34;by gridmanager (by user hcc)&#34;
remote_cerequirements = &#34;Walltime == 259200 && CondorCE == 1&#34;
Remote_JobUniverse = 5
remote_NodeNumber = 1
remote_OriginalMemory = 2048
remote_queue = &#34;&#34;
remote_SMPGranularity = 1
RemoteHost = &#34;slot1_1@....&#34;
RemoteSlotID = 1
RemoteSysCpu = 3389.0
RemoteUserCpu = 84257.0
RemoteWallClockTime = 41770.0
RequestCpus = ifThenElse(WantWholeNode =?= true,
!isUndefined(TotalCpus) ? TotalCpus &#58; JobCpus,OriginalCpus)
RequestDisk = DiskUsage
RequestMemory = ifThenElse(WantWholeNode =?= true,
!isUndefined(TotalMemory) ? TotalMemory * 95 / 100 &#58;
JobMemory,OriginalMemory)
Requirements = true
ResidentSetSize = 75000
ResidentSetSize_RAW = 50748
RootDir = &#34;/&#34;
RoutedBy = &#34;htcondor-ce&#34;
RoutedFromJobId = &#34;1214856.0&#34;
RoutedJob = true
RouteName = &#34;Local_Condor&#34;
ScheddBday = 1518646110
ServerTime = 1518707772
ShadowBday = 1518646121
ShouldTransferFiles = &#34;IF_NEEDED&#34;
StartdIpAddr = &#34;&#60;137.99.79.232&#58;9618?addrs=137.99.79.232-9618+[--1]-9618&noUDP&sock=3437_76f4_4&#62;&#34;
StartdPrincipal = &#34;execute-side@matchsession/137.99.79.232&#34;
StatsLifetimeStarter = 106990
StreamErr = false
StreamOut = false
SUBMIT_Cmd = &#34;/var/lib/gwms-factory/work-dir/glidein_startup.sh&#34;
SUBMIT_TransferOutputRemaps =
&#34;_condor_stdout=/var/log/gwms-factory/client/user_fehcc/glidein_gfactory_instance/entry_OSG_US_UConn_gluskap/job.5165664.3.out;_condor_stderr=/var/log/gwms-factory/client/user_fehcc/glidein_gfactory_instance/entry_OSG_US_UConn_gluskap/job.5165664.3.err&#34;
SUBMIT_x509userproxy =
&#34;/var/lib/gwms-factory/client-proxies/user_fehcc/glidein_gfactory_instance/credential_glidein-unl-edu_OSG_gWMSFrontend.main_295461&#34;
SubmitterGlobalJobId =
&#34;schedd_glideins9@....#5165664.3#1518564818&#34;
SubmitterId = &#34;schedd_glideins9@....&#34;
TargetType = &#34;Machine&#34;
TotalSubmitProcs = 1
TotalSuspensions = 0
TransferIn = false
TransferInputSizeMB = 0
TransferOutput = &#34;&#34;
TransferOutputRemaps = undefined
TransferQueued = false
TransferringInput = false
User = &#34;hcc@....&#34;
WallClockCheckpoint = 61190
WantCheckpoint = false
WantRemoteIO = true
WantRemoteSyscalls = false
WhenToTransferOutput = &#34;ON_EXIT&#34;
x509userproxy = &#34;credential_glidein-unl-edu_OSG_gWMSFrontend.main_295461&#34;
x509UserProxyEmail = &#34;garhan.attebury@....&#34;
x509UserProxyExpiration = 1518883802
x509UserProxyFirstFQAN = &#34;/hcc/Role=NULL/Capability=NULL&#34;
x509UserProxyFQAN = &#34;/DC=org/DC=opensciencegrid/O=Open Science
Grid/OU=Services/CN=glidein.unl.edu,/hcc/Role=NULL/Capability=NULL&#34;
x509userproxysubject = &#34;/DC=org/DC=opensciencegrid/O=Open Science
Grid/OU=Services/CN=glidein.unl.edu&#34;
x509UserProxyVOName = &#34;hcc&#34;
xcount = 1

[root@gluskap ~]# condor_status -l slot1_1@....
Activity = &#34;Busy&#34;
AddressV1 = &#34;{[ p=&#92;&#34;primary&#92;&#34;; a=&#92;&#34;137.99.79.232&#92;&#34;; port=9618;
n=&#92;&#34;Internet&#92;&#34;; spid=&#92;&#34;3437_76f4_4&#92;&#34;; noUDP=true; ], [ p=&#92;&#34;IPv4&#92;&#34;;
a=&#92;&#34;137.99.79.232&#92;&#34;; port=9618; n=&#92;&#34;Internet&#92;&#34;; spid=&#92;&#34;3437_76f4_4&#92;&#34;;
noUDP=true; ], [ p=&#92;&#34;IPv6&#92;&#34;; a=&#92;&#34;&#58;&#58;1&#92;&#34;; port=9618; n=&#92;&#34;Internet&#92;&#34;;
spid=&#92;&#34;3437_76f4_4&#92;&#34;; noUDP=true; ]}&#34;
Arch = &#34;X86_64&#34;
AuthenticatedIdentity = &#34;condor@....&#34;
AuthenticationMethod = &#34;FS_REMOTE&#34;
CanHibernate = true
CheckpointPlatform = &#34;LINUX X86_64 2.6.x normal N/A none&#34;
ClientMachine = &#34;gluskap.phys.uconn.edu&#34;
ClockDay = 4
ClockMin = 615
COLLECTOR_HOST_STRING = &#34;gluskap.phys.uconn.edu&#34;
CondorLoadAvg = 0.99
CondorPlatform = &#34;$CondorPlatform&#58; X86_64-CentOS_6.9 $&#34;
CondorVersion = &#34;$CondorVersion&#58; 8.6.9 Jan 26 2018 $&#34;
ConsoleIdle = 1087828
CpuBusy = ( ( LoadAvg - CondorLoadAvg ) &#62;= 0.5 )
CpuBusyTime = 0
CpuCacheSize = 512
CpuFamily = 16
CpuIsBusy = false
CpuModelNumber = 2
Cpus = 1
CurrentRank = 1.0
DaemonCoreDutyCycle = 0.004923340333958359
DaemonStartTime = 1517616506
DedicatedScheduler = &#34;DedicatedScheduler@....&#34;
DetectedCpus = 8
DetectedMemory = 8060
Disk = 82451
DynamicSlot = true
EnteredCurrentActivity = 1518600069
EnteredCurrentState = 1518600069
ExecutableSize = 75
ExpectedMachineGracefulDrainingBadput = 186978
ExpectedMachineGracefulDrainingCompletion = 1518708344
ExpectedMachineQuickDrainingBadput = 186978
ExpectedMachineQuickDrainingCompletion = 1518708344
FileSystemDomain = &#34;phys.uconn.edu&#34;
GlobalJobId = &#34;gluskap.phys.uconn.edu#7493045.0#1518600047&#34;
HardwareAddress = &#34;00&#58;22&#58;19&#58;12&#58;19&#58;88&#34;
has_none = true
HasEncryptExecuteDirectory = true
HasFileTransfer = true
HasFileTransferPluginMethods = &#34;file,ftp,http,data,https&#34;
HasIOProxy = true
HasJava = true
HasJICLocalConfig = true
HasJICLocalStdin = true
HasJobDeferral = true
HasMPI = true
HasPerFileEncryption = true
HasReconnect = true
HasSingularity = true
HasTDP = true
HasVM = false
HibernationLevel = 0
HibernationState = &#34;NONE&#34;
HibernationSupportedStates = &#34;S4&#34;
ImageSize = 2964016
IsLocalStartd = false
IsValidCheckpointPlatform = ( TARGET.JobUniverse =!= 1 || ( (
MY.CheckpointPlatform =!= undefined ) && ( (
TARGET.LastCheckpointPlatform =?= MY.CheckpointPlatform ) || (
TARGET.NumCkpts == 0 ) ) ) )
IsWakeAble = false
IsWakeOnLanEnabled = false
IsWakeOnLanSupported = true
JavaMFlops = 613.4951170000001
JavaSpecificationVersion = &#34;1.8&#34;
JavaVendor = &#34;Oracle Corporation&#34;
JavaVersion = &#34;1.8.0_161&#34;
JobId = &#34;7493045.0&#34;
JobPreemptions = 26
JobRankPreemptions = 14
JobStart = 1518600069
JobStarts = 3242
JobUniverse = 5
JobUserPrioPreemptions = 12
KeyboardIdle = 80
KFlops = 1246486
LastBenchmark = 1517616582
LastFetchWorkCompleted = 0
LastFetchWorkSpawned = 0
LastHeardFrom = 1518707746
LastUpdate = 1517616582
LoadAvg = 0.99
Machine = &#34;stat12.phys.uconn.edu&#34;
MachineMaxVacateTime = 10 * 60
MachineResources = &#34;Cpus Memory Disk Swap&#34;
MaxJobRetirementTime = ifthenelse(( ( OWNER =!= undefined && OWNER =!=
&#34;gluexuser&#34; && OWNER =!= &#34;gluexcat&#34; && OWNER =!= &#34;gluexprod&#34; && OWNER
=!= &#34;gluexsoft&#34; && OWNER =!= &#34;gluexsim&#34; ) && ( MemoryUsage =!=
undefined && MemoryUsage &#62; Memory * 1.3 ) ),0,ifthenelse(( ( OWNER =!=
undefined && OWNER =!= &#34;gluexuser&#34; && OWNER =!= &#34;gluexcat&#34; && OWNER
=!= &#34;gluexprod&#34; && OWNER =!= &#34;gluexsoft&#34; && OWNER =!= &#34;gluexsim&#34; ) &&
( MemoryUsage =!= undefined && MemoryUsage &#62; Memory * 1.3 ) ),0,0))
Memory = 2048
MemoryUsage = ( ( ResidentSetSize + 1023 ) / 1024 )
Mips = 11741
MonitorSelfAge = 1091112
MonitorSelfCPUUsage = 0.1747619421328118
MonitorSelfImageSize = 121668
MonitorSelfRegisteredSocketCount = 3
MonitorSelfResidentSetSize = 4872
MonitorSelfSecuritySessions = 29
MonitorSelfTime = 1518707618
MyAddress = &#34;&#60;137.99.79.232&#58;9618?addrs=137.99.79.232-9618+[--1]-9618&noUDP&sock=3437_76f4_4&#62;&#34;
MyCurrentTime = 1518707746
MyType = &#34;Machine&#34;
Name = &#34;slot1_1@....&#34;
NextFetchWorkDelay = -1
NiceUser = false
NumPids = 11
OpSys = &#34;LINUX&#34;
OpSysAndVer = &#34;CentOS6&#34;
OpSysLegacy = &#34;LINUX&#34;
OpSysLongName = &#34;CentOS release 6.9 (Final)&#34;
OpSysMajorVer = 6
OpSysName = &#34;CentOS&#34;
OpSysShortName = &#34;CentOS&#34;
OpSysVer = 609
OSG_APP = &#34;/nfs/direct/app&#34;
OSG_DATA = &#34;/nfs/direct/annex/osg-data&#34;
OWNER = &#34;hcc&#34;
ParallelSchedulingGroup = &#34;stats group&#34;
ParentSlotId = 1
PslotRollupInformation = true
PublicClaimId = &#34;&#60;137.99.79.232&#58;9618&#62;#1517616506#12343#...&#34;
QUEUE = &#34;Opteron1&#34;
Rank = ( Scheduler =?= &#34;DedicatedScheduler@....&#34; ) *
1000 + regexp(&#34;stat31.phys.uconn.edu&#34;,TARGET.GlobalJobId) * 100 +
SlotID
RecentDaemonCoreDutyCycle = 0.002841837345299636
RecentJobPreemptions = 0
RecentJobRankPreemptions = 0
RecentJobStarts = 0
RecentJobUserPrioPreemptions = 0
RemoteAutoregroup = true
RemoteGroup = &#34;&#60;none&#62;&#34;
RemoteNegotiatingGroup = &#34;&#60;none&#62;&#34;
RemoteOwner = &#34;hcc@....&#34;
RemoteUser = &#34;hcc@....&#34;
Requirements = ( START ) && ( IsValidCheckpointPlatform ) && (
WithinResourceLimits )
ResidentSetSize = 51116
RetirementTimeRemaining = 0
SlotID = 1
SlotType = &#34;Dynamic&#34;
SlotTypeID = -1
SlotWeight = Cpus
Start = ( true && ifThenElse(MY.ParentSlotID =!= undefined,( (
ifThenElse(TARGET._condor_RequestCpus =!= undefined,MY.Cpus &#62; 0 &&
TARGET._condor_RequestCpus == MY.Cpus,ifThenElse(TARGET.RequestCpus
=!= undefined,MY.Cpus &#62; 0 && TARGET.RequestCpus == MY.Cpus,1 &#60;=
MY.Cpus)) ) && ( ifThenElse(TARGET._condor_RequestMemory =!=
undefined,( MY_Memory &#62; 1200 && TARGET._condor_RequestMemory &#60;=
MY.Memory && TARGET._condor_RequestMemory &#62; MY.Memory - 1024 ) || (
MY.Memory &#60;= 1200 && TARGET._condor_RequestMemory &#60;= 1200
),ifThenElse(TARGET.RequestMemory =!= undefined,( MY.Memory &#62; 1200 &&
TARGET.RequestMemory &#60;= MY.Memory && TARGET.RequestMemory &#62; MY.Memory
- 1024 ) || ( MY.Memory &#60;= 1200 && TARGET.RequestMemory &#60;= 1200
),false)) ) && ( ifThenElse(TARGET._condor_RequestDisk =!= undefined,(
MY.Disk &#62; 10000000 && TARGET._condor_RequestDisk &#60;= MY.Disk &&
TARGET._condor_RequestDisk &#62; MY.Disk - 10000000 ) || ( MY.Disk &#60;=
10000000 && TARGET._condor_RequestDisk &#60;= 10000000
),ifThenElse(TARGET.RequestDisk =!= undefined,( MY.Disk &#62; 10000000 &&
TARGET.RequestDisk &#60;= MY.Disk && TARGET.RequestDisk &#62; MY.RequestDisk -
10000000 ) || ( MY.Disk &#60;= 10000000 && TARGET.RequestDisk &#60;= 10000000
),false)) ) ),true) )
StartdIpAddr = &#34;&#60;137.99.79.232&#58;9618?addrs=137.99.79.232-9618+[--1]-9618&noUDP&sock=3437_76f4_4&#62;&#34;
StarterAbilityList =
&#34;HasTDP,HasEncryptExecuteDirectory,HasFileTransferPluginMethods,HasJobDeferral,HasJICLocalConfig,HasJICLocalStdin,HasSingularity,HasPerFileEncryption,HasFileTransfer,HasVM,HasReconnect,HasMPI,HasJava&#34;
State = &#34;Claimed&#34;
SubnetMask = &#34;255.255.255.128&#34;
TargetType = &#34;Job&#34;
TimeToLive = 2147483647
TotalClaimRunTime = 107677
TotalCondorLoadAvg = 2.99
TotalCpus = 8.0
TotalDisk = 82450812
TotalJobRunTime = 107677
TotalLoadAvg = 3.18
TotalMemory = 8060
TotalSlotCpus = 1
TotalSlotDisk = 82451.0
TotalSlotMemory = 2048
TotalSlots = 4
TotalTimeClaimedBusy = 107677
TotalVirtualMemory = 39313076
UidDomain = &#34;phys.uconn.edu&#34;
Unhibernate = MY.MachineLastMatchTime =!= undefined
UpdateSequenceNumber = 4253
UpdatesHistory = &#34;00000000000000000000000000000000&#34;
UpdatesLost = 0
UpdatesSequenced = 205
UpdatesTotal = 206
User = &#34;hcc@....&#34;
UtsnameMachine = &#34;x86_64&#34;
UtsnameNodename = &#34;stat12.phys.uconn.edu&#34;
UtsnameRelease = &#34;2.6.32-696.10.3.el6.x86_64-nfsroot&#34;
UtsnameSysname = &#34;Linux&#34;
UtsnameVersion = &#34;#3 SMP Wed Oct 4 09&#58;51&#58;08 EDT 2017&#34;
VirtualMemory = 39313076
WakeOnLanEnabledFlags = &#34;NONE&#34;
WakeOnLanSupportedFlags = &#34;Magic Packet&#34;
WithinResourceLimits = ( ifThenElse(TARGET._condor_RequestCpus =!=
undefined,MY.Cpus &#62; 0 && TARGET._condor_RequestCpus &#60;=
MY.Cpus,ifThenElse(TARGET.RequestCpus =!= undefined,MY.Cpus &#62; 0 &&
TARGET.RequestCpus &#60;= MY.Cpus,1 &#60;= MY.Cpus)) &&
ifThenElse(TARGET._condor_RequestMemory =!= undefined,MY.Memory &#62; 0 &&
TARGET._condor_RequestMemory &#60;=
MY.Memory,ifThenElse(TARGET.RequestMemory =!= undefined,MY.Memory &#62; 0
&& TARGET.RequestMemory &#60;= MY.Memory,false)) &&
ifThenElse(TARGET._condor_RequestDisk =!= undefined,MY.Disk &#62; 0 &&
TARGET._condor_RequestDisk &#60;= MY.Disk,ifThenElse(TARGET.RequestDisk
=!= undefined,MY.Disk &#62; 0 && TARGET.RequestDisk &#60;= MY.Disk,false)) )

Now for the log files, first the StarterLog_slot1_1&#58; [parts related to
previous completed jobs have been clipped]

02/14/18 04&#58;21&#58;09 (pid&#58;2239323)
******************************************************
02/14/18 04&#58;21&#58;09 (pid&#58;2239323) ** condor_starter (CONDOR_STARTER) STARTING UP
02/14/18 04&#58;21&#58;09 (pid&#58;2239323) ** /usr/sbin/condor_starter
02/14/18 04&#58;21&#58;09 (pid&#58;2239323) ** SubsystemInfo&#58; name=STARTER
type=STARTER(8) class=DAEMON(1)
02/14/18 04&#58;21&#58;09 (pid&#58;2239323) ** Configuration&#58; subsystem&#58;STARTER
local&#58;&#60;NONE&#62; class&#58;DAEMON
02/14/18 04&#58;21&#58;09 (pid&#58;2239323) ** $CondorVersion&#58; 8.6.9 Jan 26 2018 $
02/14/18 04&#58;21&#58;09 (pid&#58;2239323) ** $CondorPlatform&#58; X86_64-CentOS_6.9 $
02/14/18 04&#58;21&#58;09 (pid&#58;2239323) ** PID = 2239323
02/14/18 04&#58;21&#58;09 (pid&#58;2239323) ** Log last touched 2/14 04&#58;12&#58;15
02/14/18 04&#58;21&#58;09 (pid&#58;2239323)
******************************************************
02/14/18 04&#58;21&#58;09 (pid&#58;2239323) Using config source&#58; /etc/condor/condor_config
02/14/18 04&#58;21&#58;09 (pid&#58;2239323) Using local config sources&#58;
02/14/18 04&#58;21&#58;09 (pid&#58;2239323)
/etc/condor/config.d/00-restart_peaceful.config
02/14/18 04&#58;21&#58;09 (pid&#58;2239323)
/etc/condor/config.d/00_UConn-OSG_condor.config
02/14/18 04&#58;21&#58;09 (pid&#58;2239323)    /etc/condor/config.d/01_condor_config.local
02/14/18 04&#58;21&#58;09 (pid&#58;2239323)
/etc/condor/config.d/10-batch_gahp_blahp.config
02/14/18 04&#58;21&#58;09 (pid&#58;2239323)    /etc/condor/config.d/99_gratia.conf
02/14/18 04&#58;21&#58;09 (pid&#58;2239323)    /etc/condor/config.d/01_condor_config.local
02/14/18 04&#58;21&#58;09 (pid&#58;2239323) config Macros = 339, Sorted = 338,
StringBytes = 10692, TablesBytes = 12292
02/14/18 04&#58;21&#58;09 (pid&#58;2239323) CLASSAD_CACHING is OFF
02/14/18 04&#58;21&#58;09 (pid&#58;2239323) Daemon Log is logging&#58; D_ALWAYS D_ERROR
02/14/18 04&#58;21&#58;09 (pid&#58;2239323) SharedPortEndpoint&#58; waiting for
connections to named socket 3595_c0ab_3223
02/14/18 04&#58;21&#58;09 (pid&#58;2239323) DaemonCore&#58; command socket at
&#60;137.99.79.232&#58;9618?addrs=137.99.79.232-9618+[--1]-9618&noUDP&sock=3595_c0ab_3223&#62;
02/14/18 04&#58;21&#58;09 (pid&#58;2239323) DaemonCore&#58; private command socket at
&#60;137.99.79.232&#58;9618?addrs=137.99.79.232-9618+[--1]-9618&noUDP&sock=3595_c0ab_3223&#62;
02/14/18 04&#58;21&#58;09 (pid&#58;2239323) Communicating with shadow
&#60;137.99.79.130&#58;9618?addrs=137.99.79.130-9618+[--1]-9618&noUDP&sock=17992_bdd5_1808046&#62;
02/14/18 04&#58;21&#58;09 (pid&#58;2239323) Submitting machine is &#34;gluskap.phys.uconn.edu&#34;
02/14/18 04&#58;21&#58;09 (pid&#58;2239323) setting the orig job name in starter
02/14/18 04&#58;21&#58;09 (pid&#58;2239323) setting the orig job iwd in starter
02/14/18 04&#58;21&#58;09 (pid&#58;2239323) SLOT1_1_USER set, so running job as prod01
02/14/18 04&#58;21&#58;09 (pid&#58;2239323) Chirp config summary&#58; IO false,
Updates false, Delayed updates true.
02/14/18 04&#58;21&#58;09 (pid&#58;2239323) Initialized IO Proxy.
02/14/18 04&#58;21&#58;09 (pid&#58;2239323) Done setting resource limits
02/14/18 04&#58;21&#58;09 (pid&#58;2239323) File transfer completed successfully.
02/14/18 04&#58;21&#58;09 (pid&#58;2239323) Job 7493045.0 set to execute immediately
02/14/18 04&#58;21&#58;09 (pid&#58;2239323) Starting a VANILLA universe job with
ID&#58; 7493045.0
02/14/18 04&#58;21&#58;09 (pid&#58;2239323) IWD&#58; /local/execute/dir_2239323
02/14/18 04&#58;21&#58;09 (pid&#58;2239323) Output file&#58;
/local/execute/dir_2239323/_condor_stdout
02/14/18 04&#58;21&#58;09 (pid&#58;2239323) Error file&#58;
/local/execute/dir_2239323/_condor_stderr
02/14/18 04&#58;21&#58;09 (pid&#58;2239323) Renice expr &#34;0&#34; evaluated to 0
02/14/18 04&#58;21&#58;09 (pid&#58;2239323) Using wrapper
/etc/condor/jobwrapper.pl to exec
/local/execute/dir_2239323/condor_exec.exe -v std -name
gfactory_instance -entry OSG_US_UConn_gluskap -clientname
glidein-unl-edu_OSG_gWMSFrontend.main -schedd
schedd_glideins9@.... -proxy OSG -factory OSGGOC -web
<a href='http&#58;//glidein.grid.iu.edu/factory/stage' target='_blank' rel='nofollow'>http&#58;//glidein.grid.iu.edu/factory/stage</a> -sign
34fbbb0c73a71842496e3a1ccb9116bb27664ae2 -signentry
189b29926504d2aeb97f788426f1424e0e82bfcd -signtype sha1 -descript
description.i29ajF.cfg -descriptentry description.i29ajF.cfg -dir
Condor -param_GLIDEIN_Client glidein-unl-edu_OSG_gWMSFrontend.main
-submitcredid 295461 -slotslayout fixed -clientweb
<a href='http&#58;//glidein.unl.edu/vofrontend/stage' target='_blank' rel='nofollow'>http&#58;//glidein.unl.edu/vofrontend/stage</a> -clientsign
df13e6d977f6b5f9df7e8ca1656fc2160f5e81e4 -clientsigntype sha1
-clientdescript description.i2cfak.cfg -clientgroup main
-clientwebgroup <a href='http&#58;//glidein.unl.edu/vofrontend/stage/group_main' target='_blank' rel='nofollow'>http&#58;//glidein.unl.edu/vofrontend/stage/group_main</a>
-clientsigngroup 52d4564e38291790f9527ae55844009987ae462d
-clientdescriptgroup description.i2cfak.cfg -param_CONDOR_VERSION
default -param_GLIDEIN_Glexec_Use NEVER -param_GLIDEIN_Job_Max_Time
34800 -param_GLIDECLIENT_ReqNode glidein.dot,grid.dot,iu.dot,edu
-param_CONDOR_OS default -param_MIN_DISK_GBS 1
-param_GLIDEIN_Monitoring_Enabled False -param_CONDOR_ARCH default
-param_UPDATE_COLLECTOR_WITH_TCP True -param_PREEMPT_GRACE_TIME 34800
-param_USE_MATCH_AUTH True -param_GLIDEIN_Report_Failed NEVER
-param_GLIDEIN_Entry_Rank
.open,.open,Owner.eq,.not,.eq,.quot,boinc.quot,.close,.star,10000.close,
-param_GLIDEIN_Collector glidein.dot,unl.dot,edu.colon,9620.minus,9660
-cluster 5165664 -subcluster 3
02/14/18 04&#58;21&#58;09 (pid&#58;2239323) Running job as user prod01
02/14/18 04&#58;21&#58;09 (pid&#58;2239323) Create_Process succeeded, pid=2239327
02/14/18 16&#58;26&#58;59 (pid&#58;2239323) Connection to shadow may be lost, will
test by sending whoami request.
02/14/18 16&#58;26&#58;59 (pid&#58;2239323) condor_write()&#58; Socket closed when
trying to write 37 bytes to &#60;137.99.79.130&#58;41926&#62;, fd is 11
02/14/18 16&#58;26&#58;59 (pid&#58;2239323) Buf&#58;&#58;write()&#58; condor_write() failed
02/14/18 16&#58;26&#58;59 (pid&#58;2239323) i/o error result is 0, errno is 0
02/14/18 16&#58;26&#58;59 (pid&#58;2239323) Lost connection to shadow, waiting
2400 secs for reconnect
02/14/18 16&#58;32&#58;11 (pid&#58;2239323) Accepted request to reconnect from &#60;&#58;0&#62;
02/14/18 16&#58;32&#58;11 (pid&#58;2239323) Ignoring old shadow
&#60;137.99.79.130&#58;9618?addrs=137.99.79.130-9618+[--1]-9618&noUDP&sock=17992_bdd5_1808046&#62;
02/14/18 16&#58;32&#58;11 (pid&#58;2239323) Communicating with shadow
&#60;137.99.79.130&#58;9618?addrs=137.99.79.130-9618+[--1]-9618&noUDP&sock=927142_297f_198&#62;
02/14/18 16&#58;32&#58;11 (pid&#58;2239323) Recovered connection to shadow after 312 seconds
02/14/18 17&#58;07&#58;37 (pid&#58;2239323) Connection to shadow may be lost, will
test by sending whoami request.
02/14/18 17&#58;07&#58;37 (pid&#58;2239323) condor_write()&#58; Socket closed when
trying to write 37 bytes to &#60;137.99.79.130&#58;43432&#62;, fd is 15
02/14/18 17&#58;07&#58;37 (pid&#58;2239323) Buf&#58;&#58;write()&#58; condor_write() failed
02/14/18 17&#58;07&#58;37 (pid&#58;2239323) i/o error result is 0, errno is 0
02/14/18 17&#58;07&#58;37 (pid&#58;2239323) Lost connection to shadow, waiting
2400 secs for reconnect
02/14/18 17&#58;08&#58;41 (pid&#58;2239323) Accepted request to reconnect from &#60;&#58;0&#62;
02/14/18 17&#58;08&#58;41 (pid&#58;2239323) Ignoring old shadow
&#60;137.99.79.130&#58;9618?addrs=137.99.79.130-9618+[--1]-9618&noUDP&sock=927142_297f_198&#62;
02/14/18 17&#58;08&#58;41 (pid&#58;2239323) Communicating with shadow
&#60;137.99.79.130&#58;9618?addrs=137.99.79.130-9618+[--1]-9618&noUDP&sock=931545_7952_196&#62;
02/14/18 17&#58;08&#58;41 (pid&#58;2239323) Recovered connection to shadow after 64 seconds
[root@stat12 log]#

The event at 4&#58;30pm on 2/14 was me restarting the condor service on the
master host. As expected, it eventually reconnects to the new shadow and
continues execution.

I do not have the shadow logs because my shadow log recycles every two
hours, and the history of the last event related to this job is more
than 12 hours old. I can expand my value for MAX_SHADOW_LOG and set
SHADOW_DEBUG to D_FULLDEBUG, then reload. It would take a day or two
for the jobs to get underway again and show this behavior.

-Richard Jones

by rjones30@....
</div><script type='text/javascript'>
        $('#show_1580476602').click(function() {
            $('#detail_1580476602').slideDown("normal");
            $('#show_1580476602').hide();
            $('#hide_1580476602').show();
        });
        $('#hide_1580476602').click(function() {
            $('#detail_1580476602').slideUp();
            $('#hide_1580476602').hide();
            $('#show_1580476602').show();
        });
        </script></pre></div><div class='update_description'><i onclick="document.location='35526#1518649756'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2018-02-14T23:09:16+00:00">Feb 14, 2018 11:09 PM UTC</time> by <b>Brian Lin</b><a class="anchor" name="1518649756">&nbsp;</a></div><pre>Richard,

The condor developer would like to see the following&#58;

- contents of the cgroup memory.stat and memory.usage_in_bytes
- condor_q -l of the job in question
- condor_status -l of the slot in question
- StarterLog on the worker node (&#34;STARTER_DEBUG = D_FULLDEBUG&#34; would be
helpful)
- /var/log/condor/ShadowLog on the CE (&#34;SHADOW_DEBUG = D_FULLDEBUG&#34;
would be helpful)

Thanks,
Brian</pre></div><div class='update_description'><i onclick="document.location='35526#1518621041'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2018-02-14T15:10:41+00:00">Feb 14, 2018 03:10 PM UTC</time> by <b>Brian Lin</b><a class="anchor" name="1518621041">&nbsp;</a></div><pre>Richard,

Thanks for the update, I&#39;ll forward this onto the condor team and let you know what they say.

- Brian</pre></div><div class='update_description'><i onclick="document.location='35526#1518620234'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2018-02-14T14:57:14+00:00">Feb 14, 2018 02:57 PM UTC</time> by <b>GLUEX</b><a class="anchor" name="1518620234">&nbsp;</a></div><pre>Hello Brian and all,

Yes, I was able to do the update, now running condor version 8.6.9. The
problem is still there, with blastpgp still regularly taking &#62;5GB of
resident set while condor_status -format &#34;%d&#92;n&#34; MemoryUsage shows only a
small fraction of that amount.

-Richard Jones

On Wed, Feb 14, 2018 at 9&#58;14 AM, Open Science Grid FootPrints &#60;
osg@....&#62; wrote&#58;

<font color='#7F7E6F'>&#62; [Duplicate message snipped]</font></pre></div><div class='update_description'><i onclick="document.location='35526#1518617651'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2018-02-14T14:14:11+00:00">Feb 14, 2018 02:14 PM UTC</time> by <b>Sarah Schmiechen</b><a class="anchor" name="1518617651">&nbsp;</a></div><pre>Hello,

Any updates on this?

Thanks,
Sarah</pre></div><div class='update_description'><i onclick="document.location='35526#1518009426'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2018-02-07T13:17:06+00:00">Feb 7, 2018 01:17 PM UTC</time> by <b>Sarah Schmiechen</b><a class="anchor" name="1518009426">&nbsp;</a></div><pre>Richard,

Were you able to do this update?

Thanks,
Sarah</pre></div><div class='update_description'><i onclick="document.location='35526#1517426293'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2018-01-31T19:18:13+00:00">Jan 31, 2018 07:18 PM UTC</time> by <b>GLUEX</b><a class="anchor" name="1517426293">&nbsp;</a></div><pre>Brian,

Ok, I will look for it tomorrow.

-Richard

On Wed, Jan 31, 2018 at 2&#58;11 PM, Open Science Grid FootPrints &#60;
osg@....&#62; wrote&#58;

<font color='#7F7E6F'>&#62; [Duplicate message snipped]</font></pre></div><div class='update_description'><i onclick="document.location='35526#1517425875'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2018-01-31T19:11:15+00:00">Jan 31, 2018 07:11 PM UTC</time> by <b>Brian Lin</b><a class="anchor" name="1517425875">&nbsp;</a></div><pre>Richard,

Would it be possible to update your version of HTCondor (8.6.9 should be released in the OSG tomorrow) to rule out any previously addressed cgroup issues?

Thanks,
Brian</pre></div><div class='update_description'><i onclick="document.location='35526#1517425243'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2018-01-31T19:00:43+00:00">Jan 31, 2018 07:00 PM UTC</time> by <b>GLUEX</b><a class="anchor" name="1517425243">&nbsp;</a></div><pre>Brian,

By contrast, the information in /cgroups/.../memory.usage_in_bytes is
correct and current. The job I referred to before has finished, but I found
a new instance of blastpgp running that was showing 10GB of memory
consumption in top, and the same number was showing in
/cgroup/.../memory.usage_in_bytes. So it appears that the cgroup
information is correct and current, even though the htcondor values are
not. Concurrent with these observations, I checked what condor_status
-format &#39;%d&#92;n&#39; MemoryUsage is reporting for that job slot, and it reports
1127. From what I am seeing, cgroup gets it right and htcondor for some
reason does not agree with cgroup statistics.

-Richard Jones

On Wed, Jan 31, 2018 at 12&#58;29 PM, Open Science Grid FootPrints &#60;
osg@....&#62; wrote&#58;

<font color='#7F7E6F'>&#62; [Duplicate message snipped]</font></pre></div><div class='update_description'><i onclick="document.location='35526#1517424688'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2018-01-31T18:51:28+00:00">Jan 31, 2018 06:51 PM UTC</time> by <b>GLUEX</b><a class="anchor" name="1517424688">&nbsp;</a></div><pre>Brian,

Yes all of the worker nodes have a common config, except for a couple of
things that get customized on subclusters, like the ParallelSchedulingGroup
and a local attribute named QUEUE that is used by local users to grab high
priority on certain nodes.

-Richard

On Wed, Jan 31, 2018 at 12&#58;30 PM, Open Science Grid FootPrints &#60;
osg@....&#62; wrote&#58;

<font color='#7F7E6F'>&#62; [Duplicate message snipped]</font></pre></div><div class='update_description'><i onclick="document.location='35526#1517419839'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2018-01-31T17:30:39+00:00">Jan 31, 2018 05:30 PM UTC</time> by <b>Brian Lin</b><a class="anchor" name="1517419839">&nbsp;</a></div><pre>And just to verify, have you applied the condor config that you&#39;ve attached to this ticket to all of your worker nodes?</pre></div><div class='update_description'><i onclick="document.location='35526#1517419790'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2018-01-31T17:29:50+00:00">Jan 31, 2018 05:29 PM UTC</time> by <b>Brian Lin</b><a class="anchor" name="1517419790">&nbsp;</a></div><pre>Richard,

And how do they compare to the cgroup memory stats?

Thanks,
Brian</pre></div><div class='update_description'><i onclick="document.location='35526#1517419095'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2018-01-31T17:18:15+00:00">Jan 31, 2018 05:18 PM UTC</time> by <b>GLUEX</b><a class="anchor" name="1517419095">&nbsp;</a></div><pre>Brian,

Here is a retake using condor_status instead of condor_q to probe the
MemoryUsage. This time the job is running on worker nod6.phys.uconn.edu in
slot1_4. While I am at it, I might as well look at slot1_6 as well, since
that one is also well over quota.

top - 12&#58;03&#58;23 up 39 days, 16&#58;14,  1 user,  load average&#58; 5.32, 5.31, 5.10
Tasks&#58; 394 total,   7 running, 385 sleeping,   0 stopped,   2 zombie
Cpu(s)&#58; 58.3%us,  0.6%sy,  0.0%ni, 38.1%id,  2.9%wa,  0.0%hi,  0.0%si,  0.0%st
Mem&#58;  16409024k total, 16236752k used,   172272k free,    41480k buffers
Swap&#58; 62499952k total,   412060k used, 62087892k free, 14976348k cached

PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND
744392 prod04    20   0 20.9g  10g  10g R 80.6 66.5   8&#58;46.55
blastpgp
735104 prod06    20   0 15.0g 3.6g 3.6g R 88.5 23.1  19&#58;19.26
blastpgp
748843 prod08    20   0 3335m 333m  45m R 100.1  2.1   3&#58;52.62 python

[root@gluskap ~]# condor_status -format &#39;%d&#92;n&#39; MemoryUsage
slot1_4@....
1203
[root@gluskap ~]# condor_status -format &#39;%d&#92;n&#39; MemoryUsage
slot1_6@....
1312

You are correct, jobs that only need the standard slot memory allocation do
not need to have a RequestMemory clause. If they have it, they should of
course respect their own self-imposed limits, but I do not check it.

-Richard Jones

On Wed, Jan 31, 2018 at 11&#58;55 AM, Open Science Grid FootPrints &#60;
osg@....&#62; wrote&#58;
<div id='show_620357510' class=''><button class="btn">Show More</button></div><div class='detail hidden' id='detail_620357510'>
<font color='#7F7E6F'>&#62; [Duplicate message snipped]</font>
</div><script type='text/javascript'>
        $('#show_620357510').click(function() {
            $('#detail_620357510').slideDown("normal");
            $('#show_620357510').hide();
            $('#hide_620357510').show();
        });
        $('#hide_620357510').click(function() {
            $('#detail_620357510').slideUp();
            $('#hide_620357510').hide();
            $('#show_620357510').show();
        });
        </script></pre></div><div class='update_description'><i onclick="document.location='35526#1517417706'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2018-01-31T16:55:06+00:00">Jan 31, 2018 04:55 PM UTC</time> by <b>Brian Lin</b><a class="anchor" name="1517417706">&nbsp;</a></div><pre>Richard,

When you run condor_q, you&#39;re querying the schedd&#39;s point of view of memory usage, which updates on the order of minutes. We&#39;re interested in the startd&#39;s perspective of memory usage (updated on the order of seconds), which should be available in the startd ad (condor_status -af name owner memoryusage memory | grep -i &#60;slot&#62;@&#60;host&#62;) should give you an idea.

And just to clarify, you&#39;re ok with jobs using more memory than they request as long as they don&#39;t exceed the available memory of the slot, right?

Thanks,
Brian</pre></div><div class='update_description'><i onclick="document.location='35526#1517222174'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2018-01-29T10:36:14+00:00">Jan 29, 2018 10:36 AM UTC</time> by <b>GLUEX</b><a class="anchor" name="1517222174">&nbsp;</a></div><pre>Brian,

My cluster is running Centos 6.9 (Final) and on the workers the kernel
version is

2.6.32-696.10.3.el6

Here is the cgroups memory information you asked about for a currently
running blastpgp job on one of my worker nodes.

Snippet from &#34;top&#34; identifies user prod06 (slot1_6@....)
as a big over-eater.

top - 05&#58;17&#58;03 up 48 days, 23&#58;24,  1 user,  load average&#58; 35.04, 35.54, 35.33
Tasks&#58; 1613 total,  38 running, 1575 sleeping,   0 stopped,   0 zombie
Cpu(s)&#58; 69.6%us,  1.2%sy,  0.0%ni, 29.1%id,  0.0%wa,  0.0%hi,  0.0%si,  0.0%st
Mem&#58;  65862536k total, 65372404k used,   490132k free,   227844k buffers
Swap&#58; 195312456k total,  1167884k used, 194144572k free, 55812752k cached

PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND
2772065 prod06    20   0 26.0g  14g  14g R 100.1 23.3  48&#58;12.01
blastpgp
2721141 prod08    20   0 13.5g 7.5g 7.5g R 100.1 12.0  51&#58;56.04
blastpgp
1044656 prod02    20   0 22.2g 7.2g 7.2g R 100.1 11.5 248&#58;13.19
blastpgp
2930138 prod07    20   0 10.5g 5.1g 5.1g R 98.5  8.1  33&#58;02.41
blastpgp
2717477 prod04    20   0 5217m 1.8g 1.8g R 145.3  2.9  68&#58;23.08
blastpgp

Now back on the CE gluskap.phys.uconn.edu&#58;

[root@gluskap ~]# condor_q -nobatch -run | grep slot1_6@stat118
7486877.0   hcc             1/26 23&#58;04   1+12&#58;48&#58;07
<div id='show_513679884' class=''><button class="btn">Show More</button></div><div class='detail hidden' id='detail_513679884'>slot1_6@....
[root@gluskap ~]# condor_q -nobatch 7486877.0

-- Schedd&#58; gluskap.phys.uconn.edu &#58; &#60;137.99.79.130&#58;44690&#62; @ 01/29/18 05&#58;23&#58;01
ID         OWNER            SUBMITTED     RUN_TIME ST PRI SIZE CMD
7486877.0   hcc             1/26 23&#58;04   1+12&#58;48&#58;33 R  0   74.0
glidein_startup.sh -v std -name gfactory_instance -entry
OSG_US_UConn_gluskap -clientname glidein-unl-edu_OSG_gWMSFrontend.main
-schedd schedd_glid

1 jobs; 0 completed, 0 removed, 0 idle, 1 running, 0 held, 0 suspended

[root@gluskap ~]# condor_q -nobatch -l 7486877.0 | grep ^Memory
MemoryUsage = ( ( ResidentSetSize + 1023 ) / 1024 )
[root@gluskap ~]# condor_q -nobatch -format &#34;%d&#92;n&#34; MemoryUsage 7486877.0
74
[root@gluskap ~]#

So MemoryUsage for this job is currently 74, in agreement with the SIZE
shown on the condor_q line. Now back on the worker, looking at cgroups
memory info.

[root@stat118 ~]# ls /cgroup/memory/htcondor/*slot1_6*
cgroup.event_control  memory.force_empty         memory.memsw.failcnt
memory.memsw.usage_in_bytes      memory.soft_limit_in_bytes
memory.usage_in_bytes  tasks
cgroup.procs          memory.limit_in_bytes
memory.memsw.limit_in_bytes      memory.move_charge_at_immigrate
memory.stat                 memory.use_hierarchy
memory.failcnt        memory.max_usage_in_bytes
memory.memsw.max_usage_in_bytes  memory.oom_control
memory.swappiness           notify_on_release

[root@stat118 ~]# cat /cgroup/memory/htcondor/*slot1_6*/memory.stat
cache 7908524032
rss 49840128
mapped_file 7835361280
pgpgin 458563712
pgpgout 456622796
swap 0
inactive_anon 27938816
active_anon 21901312
inactive_file 5269790720
active_file 2638700544
unevictable 0
hierarchical_memory_limit 9223372036854775807
hierarchical_memsw_limit 9223372036854775807
total_cache 7908524032
total_rss 49840128
total_mapped_file 7835361280
total_pgpgin 458563712
total_pgpgout 456622796
total_swap 0
total_inactive_anon 27938816
total_active_anon 21901312
total_inactive_file 5269790720
total_active_file 2638700544
total_unevictable 0
[root@stat118 ~]#

Nothing is static during the time I took these data. You can see that the
14G used by this process when I started looking had dropped to around 8GB
by the time I took the cgroups snapshot. Either way, the process seems to
be way over the limit.

On Fri, Jan 26, 2018 at 5&#58;47 PM, Open Science Grid FootPrints &#60;
osg@....&#62; wrote&#58;

<font color='#7F7E6F'>&#62; [Duplicate message snipped]</font>
</div><script type='text/javascript'>
        $('#show_513679884').click(function() {
            $('#detail_513679884').slideDown("normal");
            $('#show_513679884').hide();
            $('#hide_513679884').show();
        });
        $('#hide_513679884').click(function() {
            $('#detail_513679884').slideUp();
            $('#hide_513679884').hide();
            $('#show_513679884').show();
        });
        </script></pre></div><div class='update_description'><i onclick="document.location='35526#1517006832'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2018-01-26T22:47:12+00:00">Jan 26, 2018 10:47 PM UTC</time> by <b>Brian Lin</b><a class="anchor" name="1517006832">&nbsp;</a></div><pre>Richard,

Ok so I spoke with the condor memory expert and my big takeaway is that they do not track the total virtual memory of a job, since that would result in double counting of shared pages. When using cgroups (which you are with &#34;CGROUP_LIMIT_POLICY = none&#34;), condor just consults the cgroup (I believe the information should be in /sys/fs/cgroup/memory/htcondor/&#60;slot dir&#62;/memory.stat on EL7 or /cgroup/memory/htcondor/&#60;slot dir&#62;/memory.stat on EL6), which tracks the number of pages in memory for all procs in the cgroup. This can exclude shared memory if it&#39;s accessing a shared lib that had already been opened, for example.

What OS and kernel version are you running? Could you check the cgroup memory file for one of these troublesome jobs? I&#39;d be interested in comparing that value and the job&#39;s MemoryUsage.

- Brian

P.S. Going off of the SIZE column in condor_q will always give you the wrong information (I think the schedd updates that number about every 20min).</pre></div><div class='update_description'><i onclick="document.location='35526#1517006821'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2018-01-26T22:47:01+00:00">Jan 26, 2018 10:47 PM UTC</time> by <b>Brian Lin</b><a class="anchor" name="1517006821">&nbsp;</a></div><pre>Richard,

Ok so I spoke with the condor memory expert and my big takeaway is that they do not track the total virtual memory of a job, since that would result in double counting of shared pages. When using cgroups (which you are with &#34;CGROUP_LIMIT_POLICY = none&#34;), condor just consults the cgroup (I believe the information should be in /sys/fs/cgroup/memory/htcondor/&#60;slot dir&#62;/memory.stat on EL7 or /cgroup/memory/htcondor/&#60;slot dir&#62;/memory.stat on EL6), which tracks the number of pages in memory for all procs in the cgroup. This can exclude shared memory if it&#39;s accessing a shared lib that had already been opened, for example.

What OS and kernel version are you running? Could you check the cgroup memory file for one of these troublesome jobs? I&#39;d be interested in comparing that value and the job&#39;s MemoryUsage.

- Brian

P.S. Going off of the SIZE column in condor_q will always give you the wrong information (I think the schedd updates that number about every 20min).</pre></div><div class='update_description'><i onclick="document.location='35526#1516971981'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2018-01-26T13:06:21+00:00">Jan 26, 2018 01:06 PM UTC</time> by <b>Sarah Schmiechen</b><a class="anchor" name="1516971981">&nbsp;</a></div><pre>Hello,

Any further updates on this?

Thanks,
Sarah</pre></div><div class='update_description'><i onclick="document.location='35526#1516380314'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2018-01-19T16:45:14+00:00">Jan 19, 2018 04:45 PM UTC</time> by <b>GLUEX</b><a class="anchor" name="1516380314">&nbsp;</a></div><pre>Brian, my answers follow below.

1) Please find the output from condor_config_val -dump attached below.
2) and 3) Here is a log of a sequence of commands that you can follow to
see answers to these questions regarding a large blastpgp process that is
running presently on slot1_3@....

First, on stat108 a snapshot from &#34;top&#34; with option &#34;H&#34; enabled (show
threads). The processes have been ordered by decreasing SHR so this
blastpgp process would float to the top of the listing.

PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND
1967806 prod03    20   0 19.4g  10g  10g R 94.0 16.9  15&#58;25.54
blastpgp
2075969 prod03    20   0 19.4g  10g  10g S  0.0 16.9   0&#58;00.00
blastpgp
2097107 prod05    20   0 1836m 639m 638m R 77.2  1.0   4&#58;37.35
blastpgp
2097367 prod05    20   0 1836m 639m 638m R 85.3  1.0   1&#58;49.70
blastpgp
2046078 prod02    20   0 1829m 172m 169m R 72.1  0.3   9&#58;46.71
blastpgp
2046082 prod02    20   0 1829m 172m 169m R 86.9  0.3   6&#58;01.10 blastpgp

[root@stat108 ~]# pstree
prod03condor_exec.execondor_startup.condor_mastercondor_credmon
condor_procd
condor_shared_p

condor_startdcondor_startercondor_exec.exeblastpgp

Now back on the CE gluskap.phys.uconn.edu, I look at the condor properties
of this running job in a sequence of commands that will be easy for you to
follow. I know I am looking at the right process because my uid on the
worker corresponds with the condor slot number on that worker&#58; prod03 user
<div id='show_1170085154' class=''><button class="btn">Show More</button></div><div class='detail hidden' id='detail_1170085154'>runs jobs in slot1_3 on the workers.

[root@gluskap ~]# condor_q -run -nobatch | grep slot1_3@stat108
7483752.0   hcc             1/16 01&#58;16   0+20&#58;39&#58;37 slot1_3@....n

[root@gluskap ~]# condor_q -nobatch 7483752.0
-- Schedd&#58; gluskap.phys.uconn.edu &#58; &#60;137.99.79.130&#58;44690&#62; @ 01/19/18 11&#58;25&#58;41
ID         OWNER            SUBMITTED     RUN_TIME ST PRI SIZE CMD
7483752.0   hcc             1/16 01&#58;16   0+20&#58;40&#58;18 R  0   47.0 glidein_startup

[ note above how this job with 10GB of resident set is only listed with
size 47MB. ]

[root@gluskap ~]# condor_q -l 7483752.0 | grep ^Image
ImageSize = 17500000
ImageSize_RAW = 16587332

[ ImageSize is larger, but still nothing like the actual running footprint. ]

[root@gluskap ~]# condor_q -l 7483752.0 | grep ^Prop
[root@gluskap ~]# condor_q -l slot1_3@.... | grep ^Prop
[root@gluskap ~]# condor_q -l slot1_3@.... | grep -i ^prop
[root@gluskap ~]# condor_q -l 7483752.0 | grep -i ^Prop
[root@gluskap ~]#

[ There does not seem to be anything in the condor job properties that
starts with &#34;Prop&#34;. ]

-Richard Jones

On Fri, Jan 19, 2018 at 11&#58;06 AM, Open Science Grid FootPrints &#60;
osg@....&#62; wrote&#58;

<font color='#7F7E6F'>&#62; [Duplicate message snipped]</font>
</div><script type='text/javascript'>
        $('#show_1170085154').click(function() {
            $('#detail_1170085154').slideDown("normal");
            $('#show_1170085154').hide();
            $('#hide_1170085154').show();
        });
        $('#hide_1170085154').click(function() {
            $('#detail_1170085154').slideUp();
            $('#hide_1170085154').hide();
            $('#show_1170085154').show();
        });
        </script></pre></div><div class='update_description'><i onclick="document.location='35526#1516377984'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2018-01-19T16:06:24+00:00">Jan 19, 2018 04:06 PM UTC</time> by <b>Brian Lin</b><a class="anchor" name="1516377984">&nbsp;</a></div><pre>Rich,

Sorry for the delay, I had to go hunt down the developer. A few questions&#58;

1) What does your STARTD_JOB_ATTRS look like now? Actually, could you just attach the output of &#96;condor_config_val -dump&#96; on the worker node?
2) Is the job&#39;s &#34;ImageSize&#34; (it&#39;s in KB) any closer? The manual also mentions &#34;ProportionalSetSizeKb&#34; which should be the same except it accounts for shared memory better (<a href='http&#58;//research.cs.wisc.edu/htcondor/manual/v8.6/12_Appendix_A.html#103950' target='_blank' rel='nofollow'>http&#58;//research.cs.wisc.edu/htcondor/manual/v8.6/12_Appendix_A.html#103950</a>)
3) Do you know if blast has multiple processes running?

It&#39;s currently a mystery to me how MemoryUsage is so far off. It won&#39;t match the high water mark of the virtual memory you&#39;re seeing (that&#39;s what ImageSize should be measuring) but it should at least be close to the resident memory numbers. And with cgroups enabled, it should be much closer, if not over-reporting.

- Brian</pre></div><div class='update_description'><i onclick="document.location='35526#1516198594'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2018-01-17T14:16:34+00:00">Jan 17, 2018 02:16 PM UTC</time> by <b>Sarah Schmiechen</b><a class="anchor" name="1516198594">&nbsp;</a></div><pre>Hi Brian,

Have you heard anything back?

Thanks,
Sarah</pre></div><div class='update_description'><i onclick="document.location='35526#1515607701'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2018-01-10T18:08:21+00:00">Jan 10, 2018 06:08 PM UTC</time> by <b>Brian Lin</b><a class="anchor" name="1515607701">&nbsp;</a></div><pre>I&#39;ve asked the condor developer about this and I&#39;m awaiting his response.

- Brian</pre></div><div class='update_description'><i onclick="document.location='35526#1515417386'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2018-01-08T13:16:26+00:00">Jan 8, 2018 01:16 PM UTC</time> by <b>Sarah Schmiechen</b><a class="anchor" name="1515417386">&nbsp;</a></div><pre>Hi Brian,

Have you been able to look at this ticket?

Thanks,
Sarah</pre></div><div class='update_description'><i onclick="document.location='35526#1514984737'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2018-01-03T13:05:37+00:00">Jan 3, 2018 01:05 PM UTC</time> by <b>Sarah Schmiechen</b><a class="anchor" name="1514984737">&nbsp;</a></div><pre>Pushing out a few days per David&#39;s Out of Office message.</pre></div><div class='update_description'><i onclick="document.location='35526#1514293693'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-12-26T13:08:13+00:00">Dec 26, 2017 01:08 PM UTC</time> by <b>dswanson@....</b><a class="anchor" name="1514293693">&nbsp;</a></div><pre>I am away from the office until Jan 3. In my absence, please refer all hcc business to hcc-support@.....  I will answer messages
as I am able, but will be out of email contact most of this time.

-- David Swanson</pre></div><div class='update_description'><i onclick="document.location='35526#1514293606'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-12-26T13:06:46+00:00">Dec 26, 2017 01:06 PM UTC</time> by <b>Sarah Schmiechen</b><a class="anchor" name="1514293606">&nbsp;</a></div><pre>Hi Brian - any updates on this?
Thanks,
Sarah</pre></div><div class='update_description'><i onclick="document.location='35526#1513437251'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-12-16T15:14:11+00:00">Dec 16, 2017 03:14 PM UTC</time> by <b>GLUEX</b><a class="anchor" name="1513437251">&nbsp;</a></div><pre>Brian,

I added MemoryUsage to STARTD_JOB_ATTRS and restarted all condor daemons
from a cold restart. Now I see MemoryUsage reported by condor_status, but
it is a formula not a value. And the inputs to the formula are not defined.
So I guess this is still not defined in the machine context??

[root@gluskap ~]# condor_status -l slot1_4@.... | grep ^Memory
Memory = 2048
MemoryUsage = ( ( ResidentSetSize + 1023 ) / 1024 )
[root@gluskap ~]# condor_status -l slot1_4@.... | grep
^ResidentSetSize
[root@gluskap ~]# condor_status -l slot1_4@.... | grep
ResidentSetSize
MemoryUsage = ( ( ResidentSetSize + 1023 ) / 1024 )
MonitorSelfResidentSetSize = 7096

This MonitorSelfResidentSetSize does not seem to be useful for our purposes
because it is the same value for all jobs on a node, regardless of actual
memory occupation. I guess it is a node-wide measurement? I can and should
continue to expand the STARTD_JOB_ATTRS list until MemoryUsage is properly
computed in the machine context. But once that is complete, it still does
not solve my problem because MemoryUsage is not being computed correctly.

[root@gluskap ~]# condor_q -nobatch -run | grep slot1_4@nod20
7474559.0   hcc            12/14 16&#58;01   1+03&#58;26&#58;19 slot1_4@....
[root@gluskap ~]# condor_q -format &#34;%f&#92;n&#34; MemoryUsage 7474559
196.000000
[root@gluskap ~]#

How does this blastpgp job hide its memory consumption from the condor
watchers?

I think there is a hint to this in what &#34;top&#34; is showing on nod20 slot 4&#58;
notice that the column &#34;SHR&#34; is the same as &#34;RES&#34;
<div id='show_1402186548' class=''><button class="btn">Show More</button></div><div class='detail hidden' id='detail_1402186548'>
PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND
3547054 prod04    20   0 25.0g 9.9g 9.9g R 94.4 63.3 161&#58;45.09 blastpgp

My hypothesis is that by allocating its memory in the shared pool
instead of the process-owned pool, it is able to hide its usage from
the condor monitors. I do not see this large SHR allocation for other
jobs. What do you think?

-Richard Jones

On Fri, Dec 15, 2017 at 2&#58;39 PM, Open Science Grid FootPrints &#60;
osg@....&#62; wrote&#58;

<font color='#7F7E6F'>&#62; [Duplicate message snipped]</font>
</div><script type='text/javascript'>
        $('#show_1402186548').click(function() {
            $('#detail_1402186548').slideDown("normal");
            $('#show_1402186548').hide();
            $('#hide_1402186548').show();
        });
        $('#hide_1402186548').click(function() {
            $('#detail_1402186548').slideUp();
            $('#hide_1402186548').hide();
            $('#show_1402186548').show();
        });
        </script></pre></div><div class='update_description'><i onclick="document.location='35526#1513366750'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-12-15T19:39:10+00:00">Dec 15, 2017 07:39 PM UTC</time> by <b>Brian Lin</b><a class="anchor" name="1513366750">&nbsp;</a></div><pre>I&#39;m not sure about that but let&#39;s see if the restart with the cgroup config knob set fixes it, that should give a more accurate reading of the peak memory load. I&#39;ll mention it to the condor team.

- Brian</pre></div><div class='update_description'><i onclick="document.location='35526#1513365972'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-12-15T19:26:12+00:00">Dec 15, 2017 07:26 PM UTC</time> by <b>GLUEX</b><a class="anchor" name="1513365972">&nbsp;</a></div><pre>Brian,

Good suggestion, I have done this and verified that the change worked. But
there is still the fundamental issue that the value that condor assigns to
MemoryUsage for this job in the example I showed (MemoryUsage = 74 MB) is a
tiny fraction of the actual VM used by the job (24GB of VM, 5.5GB of
resident set as shown by top). How can this job evade the condor daemon and
fool it about its true memory usage?

-Richard Jones

On Fri, Dec 15, 2017 at 10&#58;37 AM, Open Science Grid FootPrints &#60;
osg@....&#62; wrote&#58;

<font color='#7F7E6F'>&#62; [Duplicate message snipped]</font></pre></div><div class='update_description'><i onclick="document.location='35526#1513352277'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-12-15T15:37:57+00:00">Dec 15, 2017 03:37 PM UTC</time> by <b>Brian Lin</b><a class="anchor" name="1513352277">&nbsp;</a></div><pre>Richard,

The STARTD_JOB_ATTRS config that we specified (<a href='http&#58;//research.cs.wisc.edu/htcondor/manual/v8.6/3_5Configuration_Macros.html#24668' target='_blank' rel='nofollow'>http&#58;//research.cs.wisc.edu/htcondor/manual/v8.6/3_5Configuration_Macros.html#24668</a>) should be putting the OWNER attribute into the startd ad. I thought MemoryUsage was part of the startd as well but I guess not, go ahead and change that config to&#58;

STARTD_JOB_ATTRS = $(STARTD_JOB_ATTRS) Owner MemoryUsage

Also, I think these changes require a restart of the startd for the cgroup configuration at the very least. That could explain why the MemoryUsage is off/delayed.

- Brian</pre></div><div class='update_description'><i onclick="document.location='35526#1513284792'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-12-14T20:53:12+00:00">Dec 14, 2017 08:53 PM UTC</time> by <b>GLUEX</b><a class="anchor" name="1513284792">&nbsp;</a></div><pre>Brian,

Another question&#58; After I installed the new lines in my
/etc/condor/config.d/01_condor_config.local on my workers, I did a service
condor reload command to trigger a reload. I did not stop and restart
condor from scratch on the workers. Is the reload sufficient?

-Richard Jones

On Thu, Dec 14, 2017 at 3&#58;48 PM, Richard Jones &#60;richard.t.jones@....&#62;
wrote&#58;

<font color='#7F7E6F'>&#62; Brian,</font>
<font color='#7F7E6F'>&#62;</font>
<font color='#7F7E6F'>&#62; Here is another instance, where one node is pretty heavily loaded with</font>
<font color='#7F7E6F'>&#62; these ill-behaved jobs. I answer your questions with regard to slot 9</font>
<font color='#7F7E6F'>&#62; (local user prod09, line 2 below) because it has been running for a long</font>
<font color='#7F7E6F'>&#62; time over the job memory allocation. All of these jobs run with a standard</font>
<font color='#7F7E6F'>&#62; RequestMemory of 2048.</font>
<font color='#7F7E6F'>&#62;</font>
<font color='#7F7E6F'>&#62;    PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND</font>
<font color='#7F7E6F'>&#62;</font>
<font color='#7F7E6F'>&#62; 3060387 prod11    20   0 25.9g 5.5g 5.5g D  5.6  8.7   5&#58;13.42 blastpgp</font>
<font color='#7F7E6F'>&#62;</font>
<font color='#7F7E6F'>&#62;</font>
<font color='#7F7E6F'>&#62; 2401560 prod09    20   0 23.6g 5.1g 5.1g R 95.7  8.2 170&#58;06.47 blastpgp</font>
<font color='#7F7E6F'>&#62;</font>
<font color='#7F7E6F'>&#62;</font>
<font color='#7F7E6F'>&#62; 3219120 prod12    20   0 14.8g 4.5g 4.5g D  8.5  7.1   5&#58;48.21 blastpgp</font>
<font color='#7F7E6F'>&#62;</font>
<font color='#7F7E6F'>&#62;</font>
<font color='#7F7E6F'>&#62; 2977597 prod26    20   0 11.8g 4.4g 4.4g D 14.1  6.9  13&#58;58.28 blastpgp</font>
<font color='#7F7E6F'>&#62;</font>
<font color='#7F7E6F'>&#62;</font>
<font color='#7F7E6F'>&#62; 2977593 prod08    20   0 10.4g 4.2g 4.2g D 19.7  6.7  13&#58;51.25 blastpgp</font>
<div id='show_405691620' class=''><button class="btn">Show More</button></div><div class='detail hidden' id='detail_405691620'><font color='#7F7E6F'>&#62;</font>
<font color='#7F7E6F'>&#62;</font>
<font color='#7F7E6F'>&#62; 2977595 prod30    20   0 9063m 3.8g 3.8g D 13.8  6.0  13&#58;20.72 blastpgp</font>
<font color='#7F7E6F'>&#62;</font>
<font color='#7F7E6F'>&#62;</font>
<font color='#7F7E6F'>&#62; 2792918 prod31    20   0 7788m 2.6g 2.6g D  7.2  4.2  10&#58;12.68 blastpgp</font>
<font color='#7F7E6F'>&#62;</font>
<font color='#7F7E6F'>&#62;</font>
<font color='#7F7E6F'>&#62; 2694182 prod15    20   0 1744m 1.6g  312 R 99.3  2.5 645&#58;02.35 osu-hydro</font>
<font color='#7F7E6F'>&#62;</font>
<font color='#7F7E6F'>&#62;</font>
<font color='#7F7E6F'>&#62; 2607770 prod04    20   0 1122m 987m  536 R 98.7  1.5 131&#58;38.20 osu-hydro</font>
<font color='#7F7E6F'>&#62;</font>
<font color='#7F7E6F'>&#62;</font>
<font color='#7F7E6F'>&#62; 3011848 prod23    20   0 5122m 821m 819m D  5.9  1.3  12&#58;01.06 blastpgp</font>
<font color='#7F7E6F'>&#62;</font>
<font color='#7F7E6F'>&#62;</font>
<font color='#7F7E6F'>&#62; 3485905 prod29    20   0 1926m 558m 557m R 16.7  0.9   2&#58;38.60 blastpgp</font>
<font color='#7F7E6F'>&#62;</font>
<font color='#7F7E6F'>&#62;</font>
<font color='#7F7E6F'>&#62;  457267 prod05    20   0 1066m 551m 5020 R 100.0  0.9 428&#58;19.29 Herwig</font>
<font color='#7F7E6F'>&#62;</font>
<font color='#7F7E6F'>&#62; [root@gluskap ~]# condor_q -nobatch -run | grep slot1_9@stat112</font>
<font color='#7F7E6F'>&#62; 7473555.0   hcc            12/13 00&#58;43   1+14&#58;53&#58;37 slot1_9@....</font>
<font color='#7F7E6F'>&#62;</font>
<font color='#7F7E6F'>&#62; root@gluskap ~]# condor_q -format &#34;%f &#34; MemoryUsage -format &#34; %s&#92;n&#34; Owner 7473555</font>
<font color='#7F7E6F'>&#62; 74.000000  2048.000000 hcc</font>
<font color='#7F7E6F'>&#62;</font>
<font color='#7F7E6F'>&#62; [root@gluskap ~]# condor_status -l slot1_9@.... | grep &#34;^Owner&#34;</font>
<font color='#7F7E6F'>&#62; [root@gluskap ~]#</font>
<font color='#7F7E6F'>&#62;</font>
<font color='#7F7E6F'>&#62; [root@gluskap ~]# condor_status -l slot1_9@.... | grep &#34;^Memory&#34;</font>
<font color='#7F7E6F'>&#62; Memory = 2048</font>
<font color='#7F7E6F'>&#62;</font>
<font color='#7F7E6F'>&#62; From this you can see that Owner and MemoryUsage are part of the ClassAd</font>
<font color='#7F7E6F'>&#62; of the job (condor_q to see them) whereas Memory is in the machine ClassAd</font>
<font color='#7F7E6F'>&#62; (condor_status to see it) not the job.</font>
<font color='#7F7E6F'>&#62;</font>
<font color='#7F7E6F'>&#62; I am not sure, but I wonder if the Owner attribute needs to be exported somehow from the job to the machine in order for the machine to see it?</font>
<font color='#7F7E6F'>&#62;</font>
<font color='#7F7E6F'>&#62; -Richard Jones</font>
<font color='#7F7E6F'>&#62;</font>
<font color='#7F7E6F'>&#62;</font>
<font color='#7F7E6F'>&#62;</font>
<font color='#7F7E6F'>&#62;</font>
<font color='#7F7E6F'>&#62; On Wed, Dec 13, 2017 at 10&#58;47 AM, Open Science Grid FootPrints &#60;</font>
<font color='#7F7E6F'>&#62; osg@....&#62; wrote&#58;</font>
<font color='#7F7E6F'>&#62;</font>
<font color='#7F7E6F'>&#62;&#62; [Duplicate message snipped]</font>
</div><script type='text/javascript'>
        $('#show_405691620').click(function() {
            $('#detail_405691620').slideDown("normal");
            $('#show_405691620').hide();
            $('#hide_405691620').show();
        });
        $('#hide_405691620').click(function() {
            $('#detail_405691620').slideUp();
            $('#hide_405691620').hide();
            $('#show_405691620').show();
        });
        </script></pre></div><div class='update_description'><i onclick="document.location='35526#1513284612'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-12-14T20:50:12+00:00">Dec 14, 2017 08:50 PM UTC</time> by <b>GLUEX</b><a class="anchor" name="1513284612">&nbsp;</a></div><pre>Brian,

Here is another instance, where one node is pretty heavily loaded with
these ill-behaved jobs. I answer your questions with regard to slot 9
(local user prod09, line 2 below) because it has been running for a long
time over the job memory allocation. All of these jobs run with a standard
RequestMemory of 2048.

PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND
3060387 prod11    20   0 25.9g 5.5g 5.5g D  5.6  8.7   5&#58;13.42 blastpgp

2401560 prod09    20   0 23.6g 5.1g 5.1g R 95.7  8.2 170&#58;06.47 blastpgp

3219120 prod12    20   0 14.8g 4.5g 4.5g D  8.5  7.1   5&#58;48.21 blastpgp

2977597 prod26    20   0 11.8g 4.4g 4.4g D 14.1  6.9  13&#58;58.28 blastpgp

2977593 prod08    20   0 10.4g 4.2g 4.2g D 19.7  6.7  13&#58;51.25 blastpgp

2977595 prod30    20   0 9063m 3.8g 3.8g D 13.8  6.0  13&#58;20.72 blastpgp

2792918 prod31    20   0 7788m 2.6g 2.6g D  7.2  4.2  10&#58;12.68 blastpgp

2694182 prod15    20   0 1744m 1.6g  312 R 99.3  2.5 645&#58;02.35 osu-hydro

2607770 prod04    20   0 1122m 987m  536 R 98.7  1.5 131&#58;38.20 osu-hydro

3011848 prod23    20   0 5122m 821m 819m D  5.9  1.3  12&#58;01.06 blastpgp

3485905 prod29    20   0 1926m 558m 557m R 16.7  0.9   2&#58;38.60 blastpgp

457267 prod05    20   0 1066m 551m 5020 R 100.0  0.9 428&#58;19.29 Herwig

[root@gluskap ~]# condor_q -nobatch -run | grep slot1_9@stat112
7473555.0   hcc            12/13 00&#58;43   1+14&#58;53&#58;37
<div id='show_507983982' class=''><button class="btn">Show More</button></div><div class='detail hidden' id='detail_507983982'>slot1_9@....

root@gluskap ~]# condor_q -format &#34;%f &#34; MemoryUsage -format &#34; %s&#92;n&#34;
Owner 7473555
74.000000  2048.000000 hcc

[root@gluskap ~]# condor_status -l slot1_9@.... |
grep &#34;^Owner&#34;
[root@gluskap ~]#

[root@gluskap ~]# condor_status -l slot1_9@.... |
grep &#34;^Memory&#34;
Memory = 2048

From this you can see that Owner and MemoryUsage are part of the ClassAd of
the job (condor_q to see them) whereas Memory is in the machine ClassAd
(condor_status to see it) not the job.

I am not sure, but I wonder if the Owner attribute needs to be
exported somehow from the job to the machine in order for the machine
to see it?

-Richard Jones

On Wed, Dec 13, 2017 at 10&#58;47 AM, Open Science Grid FootPrints &#60;
osg@....&#62; wrote&#58;

<font color='#7F7E6F'>&#62; [Duplicate message snipped]</font>
</div><script type='text/javascript'>
        $('#show_507983982').click(function() {
            $('#detail_507983982').slideDown("normal");
            $('#show_507983982').hide();
            $('#hide_507983982').show();
        });
        $('#hide_507983982').click(function() {
            $('#detail_507983982').slideUp();
            $('#hide_507983982').hide();
            $('#show_507983982').show();
        });
        </script></pre></div><div class='update_description'><i onclick="document.location='35526#1513180031'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-12-13T15:47:11+00:00">Dec 13, 2017 03:47 PM UTC</time> by <b>Brian Lin</b><a class="anchor" name="1513180031">&nbsp;</a></div><pre>Richard,

That&#39;s strange, setting CGROUP_MEMORY_LIMIT_POLICY to &#34;none&#34; should give
you near instantaneous updates of the peak memory usage
(<a href='http&#58;//research.cs.wisc.edu/htcondor/manual/v8.6/3_5Configuration_Macros.html#27114' target='_blank' rel='nofollow'>http&#58;//research.cs.wisc.edu/htcondor/manual/v8.6/3_5Configuration_Macros.html#27114</a>).
Could you check out the ads of the worker nodes you&#39;ve updated with the
config to see if they have an &#34;Owner&#34;. Could you also compare the
MemoryUsage and Memory attributes?

Thanks,
Brian</pre></div><div class='update_description'><i onclick="document.location='35526#1513163832'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-12-13T11:17:12+00:00">Dec 13, 2017 11:17 AM UTC</time> by <b>GLUEX</b><a class="anchor" name="1513163832">&nbsp;</a></div><pre>Hello Brian,

I applied the rules you proposed to all of the nodes, workers + CE on my
cluster. However, I still see some of these jobs running well outside the
RequestMemory bounds of 2048 MB. Here is a snapshot of &#34;top&#34; on one of my
workers, where you see user prod01 running an application blastpgp that has
grown to more than 4GB of physical memory.

517881 prod04    20   0  368m 105m 4128 R 100.0  1.3 654&#58;24.95 R

517905 prod05    20   0  381m 247m 4176 R 99.9  3.1 654&#58;03.78 R

522343 prod02    20   0 1156m  67m 1996 R 99.9  0.8 646&#58;54.19
xenon1t_G4p10
795134 prod03    20   0 23.7g 1.6g 1.6g R 98.5 20.7 333&#58;47.68 blastpgp

920397 prod01    20   0 19.4g 4.4g 4.4g R 97.9 56.4  60&#58;32.18 blastpgp

159 root      20   0     0    0    0 S  0.7  0.0   7&#58;11.00 kswapd0

However, the job still only reports 35MB for the MemoryUsage, as shown
below.

[root@gluskap ~]# condor_q -nobatch -run | grep slot1_1@stat0
7472706.0   hcc            12/12 17&#58;55   0+12&#58;10&#58;22
slot1_1@....
[root@gluskap ~]# condor_q -nobatch -format &#39;%f&#92;n&#39; MemoryUsage 7472706
35.000000

Do you know how the true memory usage by this job has escaped detection by
condor? It has been running at this size for more than 30 minutes, and
normally the latency on updates to this variable is only a couple of
minutes.

-Richard Jones
<div id='show_1006544135' class=''><button class="btn">Show More</button></div><div class='detail hidden' id='detail_1006544135'>
On Tue, Dec 12, 2017 at 6&#58;05 PM, Open Science Grid FootPrints &#60;
osg@....&#62; wrote&#58;

<font color='#7F7E6F'>&#62; [Duplicate message snipped]</font>
</div><script type='text/javascript'>
        $('#show_1006544135').click(function() {
            $('#detail_1006544135').slideDown("normal");
            $('#show_1006544135').hide();
            $('#hide_1006544135').show();
        });
        $('#hide_1006544135').click(function() {
            $('#detail_1006544135').slideUp();
            $('#hide_1006544135').hide();
            $('#show_1006544135').show();
        });
        </script></pre></div><div class='update_description'><i onclick="document.location='35526#1513119912'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-12-12T23:05:12+00:00">Dec 12, 2017 11:05 PM UTC</time> by <b>GLUEX</b><a class="anchor" name="1513119912">&nbsp;</a></div><pre>Brian,

Ok, this is better that what I have done in the past, which is a cron job
that scans condor_q for jobs with a VM above the slot Memory * 1.2 and does
condor_hold on them. I will try this. Thanks.

-Richard Jones

On Tue, Dec 12, 2017 at 5&#58;53 PM, Open Science Grid FootPrints &#60;
osg@....&#62; wrote&#58;

<font color='#7F7E6F'>&#62; [Duplicate message snipped]</font></pre></div><div class='update_description'><i onclick="document.location='35526#1513119202'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-12-12T22:53:22+00:00">Dec 12, 2017 10:53 PM UTC</time> by <b>Brian Lin</b><a class="anchor" name="1513119202">&nbsp;</a></div><pre>Richard,

Unfortunately I was wrong and it doesn&#39;t sound like we can implement that policy currently (but the condor team has been talking about work to allow it!) because the recorded MemoryUsage for a job is the peak memory. However, we can implement strict memory limits for non-gluex jobs with the following configuration&#58;

CGROUP_MEMORY_LIMIT_POLICY = none
STARTD_JOB_ATTRS = $(STARTD_JOB_ATTRS) OWNER
MEMORY_EXCEEDED = ((OWNER =!= UNDEFINED && OWNER =!= &#34;gluex&#34;) && (MemoryUsage =!= UNDEFINED && MemoryUsage &#62; Memory))
use POLICY &#58; PREEMPTY_IF(MEMORY_EXCEEDED)

Assuming that all your gluex jobs are mapped to the &#39;gluex&#39; user.

- Brian</pre></div><div class='update_description'><i onclick="document.location='35526#1513117392'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-12-12T22:23:12+00:00">Dec 12, 2017 10:23 PM UTC</time> by <b>GLUEX</b><a class="anchor" name="1513117392">&nbsp;</a></div><pre>Brian,

The policy I would like to employ is a kind of smoothed memory limit for
all jobs. From my observation of running this cluster with plenty of
opportunistic jobs, many users take advantage of how our cluster tolerates
memory consumption outside the strict bounds of RequestMemory for the job,
not just Gluex. As long as they can do so without bringing the worker to
its knees with thrashing when every slot on the node is filled with that
user&#39;s jobs then I consider it well tuned. Typically this means brief
excursions outside the requestmemory limits, followed by a smooth decay
back to within the limits. When jobs do that stochastically, they tend not
to pile on all at once, but again I consider it the user&#39;s responsibility
to tune their jobs to run efficiently on the resource that they describe in
their job request. Even if that means 30 seconds of heavy paging when
paired on a node with other jobs that steady-state at the max RS, 30
seconds is ok. An integration time of 30 seconds of cpu clock or 300
seconds of wall clock would be fine.

-Richard Jones

[root@stat0 ~]# condor_version
$CondorVersion&#58; 8.6.5 Aug 07 2017 $
$CondorPlatform&#58; X86_64-CentOS_6.9 $

On Tue, Dec 12, 2017 at 5&#58;08 PM, Open Science Grid FootPrints &#60;
osg@....&#62; wrote&#58;

<font color='#7F7E6F'>&#62; [Duplicate message snipped]</font></pre></div><div class='update_description'><i onclick="document.location='35526#1513116485'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-12-12T22:08:05+00:00">Dec 12, 2017 10:08 PM UTC</time> by <b>Brian Lin</b><a class="anchor" name="1513116485">&nbsp;</a></div><pre>Richard,

Sorry, I missed your last message before I posted mine. It&#39;s not clear to me from that message what memory policy you&#39;re looking to employ -- is it what Derek said, to enforce strict memory limits on non-GlueX jobs? Or perhaps allowing jobs to go over memory limits for only short periods of time? I just spoke with a condor developer and I think we would be able to accommodate either

What version of condor are you running?

Thanks,
Brian</pre></div><div class='update_description'><i onclick="document.location='35526#1513114337'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-12-12T21:32:17+00:00">Dec 12, 2017 09:32 PM UTC</time> by <b>Derek Weitzel</b><a class="anchor" name="1513114337">&nbsp;</a></div><pre>I wonder if there is a way to enforce memory limits for opportunistic jobs, but not for GlueX jobs?

-Derek

<font color='#7F7E6F'>&#62; On Dec 12, 2017, at 3&#58;29 PM, Open Science Grid FootPrints &#60;osg@....&#62; wrote&#58;</font>
<font color='#7F7E6F'>&#62;</font>
<font color='#7F7E6F'>&#62; [Duplicate message snipped]</font></pre></div><div class='update_description'><i onclick="document.location='35526#1513114151'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-12-12T21:29:11+00:00">Dec 12, 2017 09:29 PM UTC</time> by <b>GLUEX</b><a class="anchor" name="1513114151">&nbsp;</a></div><pre>Brian,

Thanks for this link, I am aware of cgroups and their capabilities. Do you
agree with me, that they do not allow me to enforce the kind of memory
usage policy I want (described in my last message)? The only way I know of
is to assume users who know what they are doing, and if not then ban them.
This is supposed to be a production cluster, and by far most users on the
osg respect that.

-Richard J.

On Tue, Dec 12, 2017 at 4&#58;03 PM, Open Science Grid FootPrints &#60;
osg@....&#62; wrote&#58;

<font color='#7F7E6F'>&#62; [Duplicate message snipped]</font></pre></div><div class='update_description'><i onclick="document.location='35526#1513112630'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-12-12T21:03:50+00:00">Dec 12, 2017 09:03 PM UTC</time> by <b>Brian Lin</b><a class="anchor" name="1513112630">&nbsp;</a></div><pre>Richard,

You&#39;ll want to read this section on cgroups&#58; <a href='http&#58;//research.cs.wisc.edu/htcondor/manual/v8.6/3_14Setting_Up.html#47677.' target='_blank' rel='nofollow'>http&#58;//research.cs.wisc.edu/htcondor/manual/v8.6/3_14Setting_Up.html#47677.</a> Let me know if you have any questions.

- Brian</pre></div><div class='update_description'><i onclick="document.location='35526#1513112351'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-12-12T20:59:11+00:00">Dec 12, 2017 08:59 PM UTC</time> by <b>GLUEX</b><a class="anchor" name="1513112351">&nbsp;</a></div><pre>Hello,

Yes, the /etc/security option is always there if I need it. I would rather
not go that route because it is not compatible with our planned GlueX
production, which is the primary purpose for this cluster. Our Geant4-based
simulation has a few rare events (few dozen per hour in production) that
have a lot of secondaries and can grow to occupy a lot of memory for a
short amount of time (few GB for a few ms) . On the local JLab compute
resources we have these kinds of hard limits on VM/RS resources per job
that ends up killing some fraction of our sim jobs, cutting into our
production efficiency. Either we have to request RAM equal to the maximum
spike (and leave CPUs idle on that node) or we have to suffer this loss in
production efficiency when jobs crash. One advantage of moving these jobs
to the OSG was that our institution clusters can control these limits, and
be configured so that a job will not be killed for a brief spike of RAM
demand in excess of its normal usage.

Effficiency will really be hurt if I need to allocate 8GB of ram to a job
that normally uses 1GB but spikes occasionally to 7GB and goes right back
down again. Does this make sense?

-Richard J.

On Tue, Dec 12, 2017 at 3&#58;43 PM, Open Science Grid FootPrints &#60;
osg@....&#62; wrote&#58;

<font color='#7F7E6F'>&#62; [Duplicate message snipped]</font></pre></div><div class='update_description'><i onclick="document.location='35526#1513111401'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-12-12T20:43:21+00:00">Dec 12, 2017 08:43 PM UTC</time> by <b>Derek Weitzel</b><a class="anchor" name="1513111401">&nbsp;</a></div><pre>Hi richard,

I think the best case scenario would see HTCondor killing the jobs that grow to large RAM usage, rather than hurting your nodes or you having to ban anyone.

This would require re-configuring HTCondor to include memory limits for jobs.  I am bringing Brian Lin who may know the right configuration knobs to do this.

In the mean time, here is the HTCondor wiki page on how to limit Memory usage of jobs&#58;
<a href='https&#58;//htcondor-wiki.cs.wisc.edu/index.cgi/wiki?p=HowToLimitMemoryUsage' target='_blank' rel='nofollow'>https&#58;//htcondor-wiki.cs.wisc.edu/index.cgi/wiki?p=HowToLimitMemoryUsage</a></pre></div><div class='update_description'><i onclick="document.location='35526#1513036030'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-12-11T23:47:10+00:00">Dec 11, 2017 11:47 PM UTC</time> by <b>GLUEX</b><a class="anchor" name="1513036030">&nbsp;</a></div><pre>Derek,

A couple of corrections to the previous message&#58;

- the destructive app that brought my cluster to its knees over the
weekend was named &#34;action-suid&#34; not access-suid as I claimed previously.
- the number of action-suid instances was equal to the number of
cpu-slots allocated, not greater than.
- the problem is that the action-suid app grabs more and more memory
until the node collapses and starts killing of kernel threads.

-Richard Jones

On Mon, Dec 11, 2017 at 6&#58;26 PM, Open Science Grid FootPrints &#60;
osg@....&#62; wrote&#58;

<font color='#7F7E6F'>&#62; [Duplicate message snipped]</font></pre></div><div class='update_description'><i onclick="document.location='35526#1513034772'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-12-11T23:26:12+00:00">Dec 11, 2017 11:26 PM UTC</time> by <b>GLUEX</b><a class="anchor" name="1513034772">&nbsp;</a></div><pre>Derek,

Yes, as you suspected I have switched away from GUMS and use the
recommended lcmaps voms plugin authentication mechanism. As suggested in
the link you posted, I added the following lines to
/etc/grid-security/ban-voms-mapfile, at least until I can get things better
understood. My initial investigations indicate that the problem over the
weekend was caused by an osg vo user, not hcc. The application was
access-suid or something like that, which was spawning dozens of copies of
itself within a one-cpu process and swamping the machine. I could not keep
up with the reboots, so I am banning everyone not on a strict production
regime (Atlas, ILC, LIGO) until I have a better idea how to tame it.

-Richard Jones

On Mon, Dec 11, 2017 at 11&#58;43 AM, Open Science Grid FootPrints &#60;
osg@....&#62; wrote&#58;

<font color='#7F7E6F'>&#62; [Duplicate message snipped]</font></pre></div><div class='update_description'><i onclick="document.location='35526#1513010610'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-12-11T16:43:30+00:00">Dec 11, 2017 04:43 PM UTC</time> by <b>Derek Weitzel</b><a class="anchor" name="1513010610">&nbsp;</a></div><pre>That probably has more to do with &#34;advertisement&#34; than enforcement.

Are you running gums, or a host map file?  The method of banning a user is different depending on the technology you are using.

Here is lcmaps banning procedure&#58;
<a href='https&#58;//opensciencegrid.github.io/docs/security/lcmaps-voms-authentication/#banning-vos' target='_blank' rel='nofollow'>https&#58;//opensciencegrid.github.io/docs/security/lcmaps-voms-authentication/#banning-vos</a>

You will probably want a line like&#58;
&#34;/hcc/*&#34;</pre></div><div class='update_description'><i onclick="document.location='35526#1513008673'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-12-11T16:11:13+00:00">Dec 11, 2017 04:11 PM UTC</time> by <b>GLUEX</b><a class="anchor" name="1513008673">&nbsp;</a></div><pre>Hello Derek,

Yes, it seems to still be a problem. It was over the weekend, and this
morning I woke up to find 80% of my cluster nodes non-responsive. As a
stop-gap measure, I have removed support for hcc from the list of VO&#39;s our
site supports. Will that prevent these jobs from coming our way? I noticed
that 24 hours after this change (osg-configure -c; service condor-ce
restart; service condor restart) I was still getting these jobs starting on
my site.

-Richard Jones

On Mon, Dec 11, 2017 at 11&#58;06 AM, Open Science Grid FootPrints &#60;
osg@....&#62; wrote&#58;

<font color='#7F7E6F'>&#62; [Duplicate message snipped]</font></pre></div><div class='update_description'><i onclick="document.location='35526#1513008406'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-12-11T16:06:46+00:00">Dec 11, 2017 04:06 PM UTC</time> by <b>Derek Weitzel</b><a class="anchor" name="1513008406">&nbsp;</a></div><pre>Hi,

I know this user&#39;s workflow can cause the RAM to increase.  I think it&#39;s a function of the input data, so the user is unable to verify all of the input data will be within the RAM size.

Is this continuing to happen?  I may have just been a burst of input data that drove the RAM usage high.

Also, do you have any memory protections on your cluster?  If the memory usage gets high, I would expect the cluster to kill the job.</pre></div><div class='update_description'><i onclick="document.location='35526#1512999314'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-12-11T13:35:14+00:00">Dec 11, 2017 01:35 PM UTC</time> by <b>Sarah Schmiechen</b><a class="anchor" name="1512999314">&nbsp;</a></div><pre>Derek,

Do you have any updates on this?

Thanks,
Sarah</pre></div><div class='update_description'><i onclick="document.location='35526#1512656457'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-12-07T14:20:57+00:00">Dec 7, 2017 02:20 PM UTC</time> by <b>Sarah Schmiechen</b><a class="anchor" name="1512656457">&nbsp;</a></div><pre>Hi Derek,

This issue has come up again. Would you rather work in this ticket, or reopen <a href='34516.html' target='_blank' rel='nofollow'>https&#58;//ticket.opensciencegrid.org/34516?</a>

Thanks,
Sarah</pre></div><div class='update_description'><i onclick="document.location='35526#1512226554'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-12-02T14:55:54+00:00">Dec 2, 2017 02:55 PM UTC</time> by <b>OSG-GOC</b><a class="anchor" name="1512226554">&nbsp;</a></div><pre>Hello Derek and all,
I would like to reopen ticket 34516 (hcc jobs thrashing on my site) as this continues to be a problem. The executable name is blastpgp. Here is a &#34;top&#34; snapshot on one of my worker nodes. These blastpgp nodes are taking over the available virtual memory and occupying far more resident set than they ask for in RequestMemory, which is just the default. Notice the state &#34;D&#34; which means that the node is thrashing in continual page swaps rather than running the jobs. Not all instances of blastpgp do this, but a large fraction do, enough to really impact the stability of my cluster.
PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND
670357 prod19    20   0 19.3g 5.4g 5.4g D 35.5  8.5  84&#58;10.10 blastpgp
670358 prod02    20   0 19.3g 5.2g 5.2g D 17.2  8.3  82&#58;47.60 blastpgp
690950 prod28    20   0 19.2g 4.7g 4.7g D 17.2  7.5  25&#58;18.99 blastpgp
680607 prod10    20   0 19.4g 4.6g 4.6g R 59.6  7.3  62&#58;19.36 blastpgp
1011972 prod04    20   0 14.8g 4.6g 4.5g D 29.9  7.2  40&#58;09.23 blastpgp
487027 prod09    20   0 25.0g 3.8g 3.7g R 199.2  6.0 139&#58;53.19 blastpgp
691016 prod30    20   0 7777m 1.7g 1.6g R 56.6  2.6 126&#58;34.94 blastpgp
2118242 prod07    20   0 1901m 541m 539m D 23.1  0.8   1&#58;36.09 blastpgp
1229785 prod05    20   0 6450m 517m 503m R 200.0  0.8  96&#58;43.76 blastpgp
1524010 prod20    20   0 4596m  68m  66m D  0.3  0.1   6&#58;05.90 blastpgp
406792 prod11    20   0 4976m  61m  53m D  0.3  0.1  28&#58;37.90 blastpgp
1011971 prod31    20   0 6628m  55m  50m D 99.9  0.1  48&#58;15.84 blastpgp
393517 prod21    20   0 9098m  53m  31m D 100.6  0.1 226&#58;09.87 blastpgp
2133723 prod18    20   0  184m  33m  29m D  0.0  0.1   0&#58;00.08 orca_cleanup
1373846 prod27    20   0 6422m  14m  10m D  0.0  0.0   4&#58;14.74 blastpgp

by /DC=org/DC=opensciencegrid/O=Open Science Grid/OU=People/CN=Richard T. Jones 594</pre></div>
</div>
<script type="text/javascript">
function reset_anchor() {
    $("#updates .selected").removeClass("selected");
    var urls = document.location.toString().split('#'); 
    var anchor = urls[1];
    if(anchor) {
        $("a[name='"+anchor+"']").parents(".update_description").addClass("selected");
    }
}
function submitspam(ticket_id) {
    myret = confirm("Would you like to close this ticket as a security ticket, and submit the ticket content to akismet?");
    if(myret == true) {
        $.ajax("viewer/processspam?id="+ticket_id).done(function() {
            window.location.reload();
        });
    }
}

$(function() {
    reset_anchor();
    var ADDITIONAL_COOKIE_NAME = 'gocticket';
    var options = { path: '/', expires: 365};

    if(window.opener && window.opener.name == "gocticket_list") {
        v = $.cookie("closewindow");
        if(!v) {
            $("#closewindow").attr("checked", "checked"); //on by default
        } else {
            if(v == "checked") {
                $("#closewindow").attr("checked", "checked");
            }
        }
        $("#closewindow").click(function() {
            $.cookie("closewindow", $(this).attr('checked'), options);
        });
    } else {
        $("#closewindow_area").hide();
    }
    function updateTimeago() {
        $("time").timeago();
        setTimeout(updateTimeago, 30*1000);
    }
    updateTimeago();
    $(".description").focus(expand_description);
});
</script>
<hr/>
<footer>
<p>GOC Ticket Version 2.2 | <a href="https://ticket.opensciencegrid.org/goc/submit?app_issue_check=on&amp;app_issue_type=goc&amp;app_goc_url=https%3A%2F%2Fticket.opensciencegrid.org%3A443%2F35526">Report Bugs</a>
 | <a href="https://github.com/opensciencegrid/operations/blob/master/docs/privacy.md">Privacy Policy</a>
</p>

<p> <img align="top" src="images/tag_orange.png"/> Copyright 2018 The Trustees of Indiana University - Developed for Open Science Grid</p>
</footer>


</div><!--container-fluid-->
<script>
//used by searchbox
function parseValue(value) {
    var obj = new Object();
    var tokens = value.split("\t");
    obj.str = tokens[0];
    obj.count = tokens[1];
    return obj;
}

$(function() {
    //bootstrap-2.0.4 stuff
    $(".alert-message").alert();
    $(".dropdown-toggle").dropdown();
    $("span[rel='tooltip']").tooltip();
    $("a[rel=popover]").popover();

    //activate menu that user is currently on
    $("#menu_navigator").addClass("active"); 
    $("#submenu_").addClass("active"); 

    //translate zend validation error message to bootstrap
    $(".errors").addClass("alert").addClass("alert-error");

});
</script>


</body>
